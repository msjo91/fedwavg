{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35825f4-b26d-4ee5-8914-d28fdc8bc1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "dataset = 'svhn'\n",
    "\n",
    "filename = f'{dataset}_diri9a001_42_fedprox'\n",
    "\n",
    "gpu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8447bc5c-393a-4f8d-80fd-50a652a7a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, Subset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "148a5cb1-99a9-4d54-8239-3ae733854155",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    torch.use_deterministic_algorithms(False)\n",
    "except AttributeError:\n",
    "    torch.set_deterministic(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5090116-fe54-40fe-8459-e9ce9aea270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = os.path.dirname(os.getcwd())\n",
    "path_data = os.path.join(path_root, 'data')\n",
    "path_logs = os.path.join(path_root, 'logs')\n",
    "path_models = os.path.join(path_root, 'models', filename)\n",
    "path_results = os.path.join(path_root, 'results', filename)\n",
    "\n",
    "for p in [path_data, path_logs, path_models, path_results]:\n",
    "    os.makedirs(p, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e4f24c-2bab-456f-9897-fb7cf4a72efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(filename)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "streamformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "streamhandler = logging.StreamHandler()\n",
    "streamhandler.setFormatter(streamformatter)\n",
    "logger.addHandler(streamhandler)\n",
    "\n",
    "fileformatter = logging.Formatter('%(message)s')\n",
    "filehandler = logging.FileHandler(os.path.join(path_logs, filename + '.log'), mode='w')\n",
    "filehandler.setFormatter(fileformatter)\n",
    "logger.addHandler(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4543504-728a-4bbf-8ebd-a70f223288d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, target = self.dataset[idx]\n",
    "        return data, target, idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0859e906-dbe3-4c6f-a44d-3ce50555d228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/hgato/projects/benchmark/data/train_32x32.mat\n",
      "Using downloaded and verified file: /home/hgato/projects/benchmark/data/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'cifar10':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([x / 255 for x in [125.3, 123, 113.9]], [x / 255 for x in [63, 62.1, 66.7]])\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([x / 255 for x in [125.3, 123, 113.9]], [x / 255 for x in [63, 62.1, 66.7]])\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.CIFAR10(path_data, train=False, transform=test_transform)\n",
    "    \n",
    "elif dataset == 'svhn':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.SVHN(path_data, split='train', transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.SVHN(path_data, split='test', transform=test_transform, download=True)\n",
    "    \n",
    "elif dataset == 'fmnist':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.FashionMNIST(path_data, train=False, transform=test_transform)\n",
    "\n",
    "elif dataset == 'mnist':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.MNIST(path_data, train=False, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6182a3f4-472d-4246-9ae0-e184be956dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_data, f'{dataset}_diri9a001_42.json')) as f:\n",
    "    indices = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02409155-7c8a-4217-8258-ccb1a2c7e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_workers = 128, 0\n",
    "\n",
    "inferloaders, subset_indices = {}, []\n",
    "for k, v in indices.items():\n",
    "    infersubset = Subset(custom_dataset, v['index'])\n",
    "    inferloaders[k] = DataLoader(infersubset, batch_size=batch_size, num_workers=num_workers)\n",
    "    subset_indices.extend(v['index'])\n",
    "\n",
    "train_subset = Subset(train_dataset, indices=subset_indices)\n",
    "fed_trainloader = DataLoader(train_subset, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "try:\n",
    "    train_labels = np.asarray(custom_dataset.dataset.targets)\n",
    "    test_labels = test_dataset.targets\n",
    "except AttributeError:\n",
    "    train_labels = np.asarray(custom_dataset.dataset.labels)\n",
    "    test_labels = test_dataset.labels\n",
    "subset_classes = np.unique(train_labels[subset_indices])\n",
    "boolarr = [True if y in subset_classes else False for y in test_labels]\n",
    "subset_indices = np.arange(len(test_dataset))[boolarr]\n",
    "test_subset = Subset(test_dataset, indices=subset_indices)\n",
    "testloader = DataLoader(test_subset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37423e2e-709a-4a3c-b0de-99acc3e83e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, padding=1, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=kernel_size, stride=1, padding=padding, bias=bias)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if (stride != 1) or (in_channel != self.expansion * out_channel):\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, self.expansion * out_channel, kernel_size=1, stride=stride, bias=bias),\n",
    "                nn.BatchNorm2d(self.expansion * out_channel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dd80881-9cee-4d1c-8348-aaf4eacc8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, padding=1, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=1, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv3 = nn.Conv2d(out_channel, self.expansion * out_channel, kerenel_size=1, bias=bias)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * out_channel)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if (stride != 1) or (in_channel != self.expansion * out_channel):\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, self.expansion * out_channel, kernel_size=1, stride=stride, bias=bias),\n",
    "                nn.BatchNorm2d(self.expansion * out_channel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eede0030-1cdc-4185-a7b3-825076d49102",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, in_channel=3, out_channels=[64, 128, 256, 512], num_blocks=[2, 2, 2, 2], strides=[1, 2, 2, 2], num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_channel = out_channels[0]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channels[0], kernel_size=3, stride=strides[0], padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels[0])\n",
    "        \n",
    "        self.block1 = self._make_layer(block, out_channels[0], num_blocks[0], strides[0])\n",
    "        self.block2 = self._make_layer(block, out_channels[1], num_blocks[1], strides[1])\n",
    "        self.block3 = self._make_layer(block, out_channels[2], num_blocks[2], strides[2])\n",
    "        self.block4 = self._make_layer(block, out_channels[3], num_blocks[3], strides[3])\n",
    "        \n",
    "        self.linear = nn.Linear(out_channels[-1] * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channel, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channel, out_channel, stride=stride))\n",
    "            self.in_channel = out_channel * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae0fe2f3-5ec9-46fe-b29c-8db8b81089ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(w):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "        \n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], float(len(w)))\n",
    "        \n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "611e3c0f-9f0e-419a-aa1f-a497b671aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, epochs, lr, weight_decay, criterion, device, global_model, mu=0.01):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    global_params = list(global_model.parameters())\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        batch_losses = []\n",
    "        \n",
    "        for inputs, labels, _ in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss.mean()\n",
    "            \n",
    "            prox_reg = 0.0\n",
    "            for i, param in enumerate(model.parameters()):\n",
    "                prox_reg += ((mu / 2) * torch.norm((param - global_params[i])) ** 2)\n",
    "            loss += prox_reg\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "        epoch_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "        \n",
    "    local_weights = model.state_dict()\n",
    "\n",
    "    return local_weights, epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d5c54db-1ba4-4f65-817d-fb8b2b28fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, loader, criterion, device):\n",
    "    avg_loss, correct, num_samples = 0, 0, 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss.mean()\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += torch.sum(torch.eq(predicted, labels)).item()\n",
    "            num_samples += len(labels)\n",
    "\n",
    "    acc = correct / num_samples\n",
    "    avg_loss /= len(loader)\n",
    "    \n",
    "    return acc, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "629ff8ea-9394-4a48-a5e0-1a90d329a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_train(model, loader, device, match_history, flag=False):\n",
    "    forgettables = []\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, indices in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            match = predicted.eq(labels)\n",
    "            match = match.type(torch.IntTensor)\n",
    "            total += len(indices)\n",
    "            \n",
    "            for j, idx in enumerate(indices):\n",
    "                sample_history = match_history.get(idx.item(), [])\n",
    "                sample_history.append(match[j].item())\n",
    "                match_history[idx.item()] = sample_history\n",
    "                correct += match[j].item()\n",
    "                if flag is True:\n",
    "                    try:\n",
    "                        if match[j].item() - sample_history[-2] == -1:\n",
    "                            forgettables.append(idx.item())\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "            \n",
    "    acc = correct / total\n",
    "            \n",
    "    return match_history, forgettables, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7562c7c7-4368-442b-8bd7-fa1674a6842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatfloats(li):\n",
    "    new = [float(f'{e:>8.4f}') for e in li]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc4a3691-7135-496c-9153-23e7fc4f1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet9 = {'block': ResidualBlock, 'num_blocks': [1, 1, 1, 1]}\n",
    "resnet18 = {'block': ResidualBlock, 'num_blocks': [2, 2, 2, 2]}\n",
    "resnet34 = {'block': ResidualBlock, 'num_blocks': [3, 4, 6, 3]}\n",
    "resnet50 = {'block': Bottleneck, 'num_blocks': [3, 4, 6, 3]}\n",
    "resnet101 = {'block': Bottleneck, 'num_blocks': [3, 4, 23, 3]}\n",
    "resnet152 = {'block': Bottleneck, 'num_blocks': [3, 8, 36, 3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1261d399-3f2a-4f98-b585-81e5037a36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f'cuda:{gpu}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20a73dfb-ec2c-40fd-9893-3eac24fbca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = resnet9.copy()\n",
    "\n",
    "out_channels = [64, 128, 256, 512]\n",
    "strides = [1, 2, 2, 2]\n",
    "in_channel = 3\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "fed_model = ResNet(resnet['block'], in_channel=in_channel, out_channels=out_channels, num_blocks=resnet['num_blocks'], strides=strides, num_classes=num_classes)\n",
    "fed_weights = fed_model.state_dict()\n",
    "\n",
    "fed_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d0203-4dd4-4c5b-9f57-5693890ec7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-02 18:52:45,129 - INFO - \n",
      "Algorithm: FedProx\n",
      "Client Split: 9\n",
      "Dataset: svhn\n",
      "Model: ResNet9 | Rounds: 200 | Epochs: 10 | LR: 0.001\n",
      "\n",
      "2021-09-02 18:52:45,131 - INFO -  | Global Training Round : 1 / 200 |\n",
      "2021-09-02 18:53:10,112 - INFO -   |-- [Party 1] Average Train Loss:   2.1949 Train Accuracy: global   7.43% local  41.55% ...    0 forgettables out of 2828 ( 0.00%) ... total data used 2828\n",
      "2021-09-02 18:53:10,116 - INFO -   |--    Epoch Losses (10): [2.6833, 2.6171, 2.4277, 2.3146, 2.2543, 2.1085, 2.0449, 1.895, 1.7891, 1.8143]\n",
      "2021-09-02 18:53:31,220 - INFO -   |-- [Party 2] Average Train Loss:   2.0906 Train Accuracy: global   6.61% local  56.48% ...    0 forgettables out of 2376 ( 0.00%) ... total data used 2376\n",
      "2021-09-02 18:53:31,222 - INFO -   |--    Epoch Losses (10): [2.6242, 2.6222, 2.4361, 2.271, 2.1132, 1.9693, 1.8609, 1.7624, 1.6789, 1.5676]\n",
      "2021-09-02 18:54:08,925 - INFO -   |-- [Party 3] Average Train Loss:   1.7897 Train Accuracy: global   6.51% local  69.57% ...    0 forgettables out of 4252 ( 0.00%) ... total data used 4252\n",
      "2021-09-02 18:54:08,927 - INFO -   |--    Epoch Losses (10): [2.672, 2.4593, 2.1542, 1.8795, 1.6996, 1.6006, 1.4278, 1.3374, 1.3532, 1.3132]\n",
      "2021-09-02 18:54:21,258 - INFO -   |-- [Party 4] Average Train Loss:   2.2484 Train Accuracy: global   6.70% local  46.27% ...    0 forgettables out of 1567 ( 0.00%) ... total data used 1567\n",
      "2021-09-02 18:54:21,259 - INFO -   |--    Epoch Losses (10): [2.6344, 2.7192, 2.6127, 2.4501, 2.2889, 2.1512, 2.0867, 1.9643, 1.849, 1.7273]\n",
      "2021-09-02 18:54:43,251 - INFO -   |-- [Party 5] Average Train Loss:   2.0524 Train Accuracy: global   6.52% local  58.39% ...    0 forgettables out of 2516 ( 0.00%) ... total data used 2516\n",
      "2021-09-02 18:54:43,255 - INFO -   |--    Epoch Losses (10): [2.6254, 2.6023, 2.3483, 2.2112, 2.0598, 1.8916, 1.818, 1.7213, 1.6591, 1.5866]\n",
      "2021-09-02 18:55:08,018 - INFO -   |-- [Party 6] Average Train Loss:   2.0642 Train Accuracy: global   6.98% local  51.10% ...    0 forgettables out of 2967 ( 0.00%) ... total data used 2967\n",
      "2021-09-02 18:55:08,019 - INFO -   |--    Epoch Losses (10): [2.651, 2.5673, 2.3382, 2.2314, 2.0725, 1.9251, 1.8316, 1.7406, 1.6961, 1.5878]\n",
      "2021-09-02 18:55:25,906 - INFO -   |-- [Party 7] Average Train Loss:   2.1157 Train Accuracy: global   6.81% local  40.79% ...    0 forgettables out of 1998 ( 0.00%) ... total data used 1998\n",
      "2021-09-02 18:55:25,909 - INFO -   |--    Epoch Losses (10): [2.6522, 2.7011, 2.5145, 2.2858, 2.1253, 1.9545, 1.8517, 1.7506, 1.6771, 1.6441]\n",
      "2021-09-02 18:55:38,936 - INFO -   |-- [Party 8] Average Train Loss:   2.3259 Train Accuracy: global   6.01% local  31.49% ...    0 forgettables out of 1664 ( 0.00%) ... total data used 1664\n",
      "2021-09-02 18:55:38,938 - INFO -   |--    Epoch Losses (10): [2.5978, 2.6693, 2.5756, 2.43, 2.3132, 2.2511, 2.2145, 2.2044, 2.0662, 1.9367]\n",
      "2021-09-02 18:55:52,522 - INFO -   |-- [Party 9] Average Train Loss:   2.1466 Train Accuracy: global   6.15% local  37.81% ...    0 forgettables out of 1756 ( 0.00%) ... total data used 1756\n",
      "2021-09-02 18:55:52,524 - INFO -   |--    Epoch Losses (10): [2.6337, 2.6583, 2.5373, 2.3496, 2.1735, 2.0224, 1.9174, 1.8002, 1.7247, 1.6494]\n",
      "2021-09-02 18:56:08,786 - INFO -     |---- Number of Forgettables: 0 (0.00%)\n",
      "2021-09-02 18:56:08,790 - INFO -     |---- Train Accuracy: 11.66%\n",
      "2021-09-02 18:56:08,791 - INFO -     |---- Test Accuracy: 11.07%\n",
      "2021-09-02 18:56:08,792 - INFO -     |---- Test Loss: 2.4697\n",
      "2021-09-02 18:56:08,795 - INFO -     |---- Elapsed time: 0:03:23.664110\n",
      "2021-09-02 18:56:08,796 - INFO - \n",
      "Test Acc: Max 11.0710% (1 round) | Last 11.0710% | Avg 11.0710% (1 round)\n",
      "\n",
      "2021-09-02 18:56:08,799 - INFO -  | Global Training Round : 2 / 200 |\n",
      "2021-09-02 18:56:30,808 - INFO -   |-- [Party 1] Average Train Loss:   1.4137 Train Accuracy: global  11.70% local  55.13% ... 1045 forgettables out of 2828 (36.95%) ... total data used 2828\n",
      "2021-09-02 18:56:30,809 - INFO -   |--    Epoch Losses (10): [2.0724, 1.7515, 1.5292, 1.3481, 1.2808, 1.3113, 1.238, 1.1502, 1.2609, 1.1944]\n",
      "2021-09-02 18:56:51,141 - INFO -   |-- [Party 2] Average Train Loss:   1.3390 Train Accuracy: global  11.83% local  57.58% ... 1218 forgettables out of 2376 (51.26%) ... total data used 2376\n",
      "2021-09-02 18:56:51,143 - INFO -   |--    Epoch Losses (10): [2.0135, 1.7382, 1.4391, 1.3058, 1.2486, 1.1891, 1.1622, 1.1109, 1.0883, 1.094]\n",
      "2021-09-02 18:57:24,492 - INFO -   |-- [Party 3] Average Train Loss:   1.2591 Train Accuracy: global  11.19% local  75.54% ... 2623 forgettables out of 4252 (61.69%) ... total data used 4252\n",
      "2021-09-02 18:57:24,495 - INFO -   |--    Epoch Losses (10): [1.8677, 1.4246, 1.2634, 1.1908, 1.1547, 1.1586, 1.1501, 1.1077, 1.1246, 1.1489]\n",
      "2021-09-02 18:57:36,346 - INFO -   |-- [Party 4] Average Train Loss:   1.5113 Train Accuracy: global  11.68% local  67.84% ...  618 forgettables out of 1567 (39.44%) ... total data used 1567\n",
      "2021-09-02 18:57:36,348 - INFO -   |--    Epoch Losses (10): [2.1469, 1.9585, 1.7069, 1.4799, 1.4112, 1.378, 1.3194, 1.3035, 1.228, 1.1809]\n",
      "2021-09-02 18:57:55,167 - INFO -   |-- [Party 5] Average Train Loss:   1.3812 Train Accuracy: global  11.92% local  66.14% ... 1259 forgettables out of 2516 (50.04%) ... total data used 2516\n",
      "2021-09-02 18:57:55,168 - INFO -   |--    Epoch Losses (10): [2.1623, 1.832, 1.5275, 1.3897, 1.2278, 1.1584, 1.1282, 1.1491, 1.1188, 1.1181]\n",
      "2021-09-02 18:58:17,391 - INFO -   |-- [Party 6] Average Train Loss:   1.3381 Train Accuracy: global  11.49% local  64.51% ... 1369 forgettables out of 2967 (46.14%) ... total data used 2967\n",
      "2021-09-02 18:58:17,393 - INFO -   |--    Epoch Losses (10): [1.9755, 1.6573, 1.418, 1.2669, 1.2183, 1.191, 1.2166, 1.1991, 1.1187, 1.1193]\n",
      "2021-09-02 18:58:32,929 - INFO -   |-- [Party 7] Average Train Loss:   1.3520 Train Accuracy: global   9.81% local  61.66% ...  814 forgettables out of 1998 (40.74%) ... total data used 1998\n",
      "2021-09-02 18:58:32,930 - INFO -   |--    Epoch Losses (10): [2.038, 1.7883, 1.5089, 1.364, 1.2387, 1.1668, 1.1212, 1.1043, 1.0877, 1.1022]\n",
      "2021-09-02 18:58:47,303 - INFO -   |-- [Party 8] Average Train Loss:   1.3932 Train Accuracy: global  14.30% local  73.80% ...  498 forgettables out of 1664 (29.93%) ... total data used 1664\n",
      "2021-09-02 18:58:47,304 - INFO -   |--    Epoch Losses (10): [2.0882, 1.8389, 1.6086, 1.4147, 1.2894, 1.1836, 1.1597, 1.1431, 1.1336, 1.0722]\n",
      "2021-09-02 18:59:01,479 - INFO -   |-- [Party 9] Average Train Loss:   1.4200 Train Accuracy: global  11.96% local  65.95% ...  558 forgettables out of 1756 (31.78%) ... total data used 1756\n",
      "2021-09-02 18:59:01,481 - INFO -   |--    Epoch Losses (10): [2.1622, 1.8974, 1.596, 1.4105, 1.3237, 1.2423, 1.1923, 1.1459, 1.1039, 1.1261]\n",
      "2021-09-02 18:59:17,023 - INFO -     |---- Number of Forgettables: 10002 (45.62%)\n",
      "2021-09-02 18:59:17,025 - INFO -     |---- Train Accuracy: 71.46%\n",
      "2021-09-02 18:59:17,025 - INFO -     |---- Test Accuracy: 65.98%\n",
      "2021-09-02 18:59:17,026 - INFO -     |---- Test Loss: 0.9644\n",
      "2021-09-02 18:59:17,027 - INFO -     |---- Elapsed time: 0:06:31.896523\n",
      "2021-09-02 18:59:17,028 - INFO - \n",
      "Test Acc: Max 65.9842% (2 round) | Last 65.9842% | Avg 38.5276% (2 round)\n",
      "\n",
      "2021-09-02 18:59:17,030 - INFO -  | Global Training Round : 3 / 200 |\n",
      "2021-09-02 18:59:38,630 - INFO -   |-- [Party 1] Average Train Loss:   0.9161 Train Accuracy: global  70.72% local  72.10% ...  254 forgettables out of 2828 ( 8.98%) ... total data used 2828\n",
      "2021-09-02 18:59:38,634 - INFO -   |--    Epoch Losses (10): [1.0353, 0.9842, 0.9388, 0.9303, 0.9163, 0.8996, 0.8705, 0.8765, 0.8432, 0.8663]\n",
      "2021-09-02 18:59:56,659 - INFO -   |-- [Party 2] Average Train Loss:   0.8483 Train Accuracy: global  72.22% local  73.61% ...  276 forgettables out of 2376 (11.62%) ... total data used 2376\n",
      "2021-09-02 18:59:56,661 - INFO -   |--    Epoch Losses (10): [1.0876, 0.9684, 0.8598, 0.7785, 0.8244, 0.789, 0.7845, 0.8214, 0.7781, 0.7917]\n",
      "2021-09-02 19:00:32,429 - INFO -   |-- [Party 3] Average Train Loss:   0.8774 Train Accuracy: global  72.22% local  80.95% ...  542 forgettables out of 4252 (12.75%) ... total data used 4252\n",
      "2021-09-02 19:00:32,433 - INFO -   |--    Epoch Losses (10): [0.9952, 0.9038, 0.8725, 0.8635, 0.8564, 0.8257, 0.8481, 0.8648, 0.8936, 0.8509]\n",
      "2021-09-02 19:00:45,789 - INFO -   |-- [Party 4] Average Train Loss:   0.8817 Train Accuracy: global  71.60% local  74.60% ...  183 forgettables out of 1567 (11.68%) ... total data used 1567\n",
      "2021-09-02 19:00:45,791 - INFO -   |--    Epoch Losses (10): [1.0322, 1.0345, 0.8733, 0.8667, 0.8644, 0.7891, 0.8148, 0.8333, 0.8767, 0.8323]\n",
      "2021-09-02 19:01:04,506 - INFO -   |-- [Party 5] Average Train Loss:   0.8595 Train Accuracy: global  72.06% local  81.28% ...  293 forgettables out of 2516 (11.65%) ... total data used 2516\n",
      "2021-09-02 19:01:04,508 - INFO -   |--    Epoch Losses (10): [1.0411, 0.9321, 0.8261, 0.8316, 0.8344, 0.8314, 0.8296, 0.8187, 0.8244, 0.8256]\n",
      "2021-09-02 19:01:29,622 - INFO -   |-- [Party 6] Average Train Loss:   0.8952 Train Accuracy: global  71.86% local  74.22% ...  252 forgettables out of 2967 ( 8.49%) ... total data used 2967\n",
      "2021-09-02 19:01:29,624 - INFO -   |--    Epoch Losses (10): [1.0589, 0.9206, 0.8552, 0.8572, 0.8953, 0.9449, 0.9169, 0.8276, 0.8293, 0.8463]\n",
      "2021-09-02 19:01:46,411 - INFO -   |-- [Party 7] Average Train Loss:   0.8157 Train Accuracy: global  70.87% local  79.88% ...  171 forgettables out of 1998 ( 8.56%) ... total data used 1998\n",
      "2021-09-02 19:01:46,413 - INFO -   |--    Epoch Losses (10): [1.0545, 0.9405, 0.8294, 0.7789, 0.7462, 0.7723, 0.7668, 0.728, 0.7611, 0.7794]\n"
     ]
    }
   ],
   "source": [
    "rounds = 200\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "wdecay = 0\n",
    "\n",
    "logger.info(f'\\nAlgorithm: FedProx\\nClient Split: 9\\nDataset: {dataset}\\nModel: ResNet9 | Rounds: {rounds} | Epochs: {epochs} | LR: {lr}\\n')\n",
    "\n",
    "train_accs, train_losses, test_accs, test_losses = [], [], [], []\n",
    "match_history, round_forget_history = {}, []\n",
    "forget_cnt_per_client = [0] * len(indices.keys())\n",
    "\n",
    "st = time.time()\n",
    "for r in range(rounds):\n",
    "    \n",
    "    round_forget, round_samples = 0, 0\n",
    "    forget_history, forgettables = {}, {}\n",
    "    local_weights, local_losses = [], []\n",
    "    logger.info(f' | Global Training Round : {r + 1} / {rounds} |')\n",
    "    \n",
    "    for i, k in enumerate(indices.keys()):\n",
    "        match_history, forgettables, global_acc = infer_train(fed_model, inferloaders[k], device, match_history, flag=True)\n",
    "\n",
    "        fed_model.train()\n",
    "        \n",
    "        sampler_idx = indices[k]['index'].copy()\n",
    "        sampler = SubsetRandomSampler(sampler_idx)\n",
    "\n",
    "        trainloader = DataLoader(custom_dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
    "        \n",
    "        local_model = copy.deepcopy(fed_model)\n",
    "        w, ls = train(local_model, trainloader, epochs, lr, wdecay, criterion, device, fed_model, mu=0.01)\n",
    "\n",
    "        local_weights.append(copy.deepcopy(w))\n",
    "        train_losses.append(ls)\n",
    "        \n",
    "        match_history, _, local_acc = infer_train(local_model, inferloaders[k], device, match_history, flag=False)\n",
    "        train_accs.append(local_acc)\n",
    "        \n",
    "        forget_cnt = len(forgettables)\n",
    "        round_forget += forget_cnt\n",
    "        forget_cnt_per_client[int(k)] = forget_cnt\n",
    "        round_samples += len(indices[k]['index'])\n",
    "        \n",
    "        logger.info('  |-- [Party {:>1}] Average Train Loss: {:>8.4f} Train Accuracy: global {:>6.2f}% local {:>6.2f}% ... {:>4} forgettables out of {:>4} ({:>5.2f}%) ... total data used {:>4}'.format(\n",
    "            i + 1, sum(ls) / len(ls), 100 * global_acc, 100 * local_acc, forget_cnt, len(indices[k]['index']), 100 * forget_cnt / len(indices[k]['index']), len(sampler_idx)\n",
    "        ))\n",
    "        logger.info('  |--    Epoch Losses ({:>2}): {}'.format(epochs, formatfloats(ls)))\n",
    "        \n",
    "    fed_weights = average_weights(local_weights)\n",
    "    fed_model.load_state_dict(fed_weights)\n",
    "    train_acc, _ = inference(fed_model, fed_trainloader, criterion, device)\n",
    "    \n",
    "    if (r + 1) % 50 == 0:\n",
    "        torch.save(fed_model.state_dict(), os.path.join(path_models, filename + f'_round{r+1}.pth'))\n",
    "    \n",
    "    test_acc, test_ls = inference(fed_model, testloader, criterion, device)\n",
    "    test_accs.append(test_acc)\n",
    "    test_losses.append(test_ls)\n",
    "    round_forget_history.append(round_forget)\n",
    "    logger.info('    |---- Number of Forgettables: {} ({:.2f}%)'.format(round_forget, 100 * round_forget / round_samples))\n",
    "    logger.info('    |---- Train Accuracy: {:>.2f}%'.format(100 * train_acc))\n",
    "    logger.info('    |---- Test Accuracy: {:>.2f}%'.format(100 * test_acc))\n",
    "    logger.info('    |---- Test Loss: {:.4f}'.format(test_ls))\n",
    "    logger.info('    |---- Elapsed time: {}'.format(timedelta(seconds=time.time()-st)))\n",
    "    logger.info(f'\\nTest Acc: Max {np.max(test_acc) * 100:.4f}% ({np.argmax(test_accs)+1} round) | Last {test_accs[-1] * 100:.4f}% | Avg {np.mean(test_accs) * 100:.4f}% ({np.argmax(test_accs > np.mean(test_accs))+1} round)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654dbc03-4dd8-4c75-932e-f9f842c9ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = np.asarray(train_losses)\n",
    "train_accs = np.asarray(train_accs)\n",
    "\n",
    "with open(os.path.join(path_results, f'{filename}_tr_ls.npy'), 'wb') as f:\n",
    "    np.save(f, train_losses)\n",
    "with open(os.path.join(path_results, f'{filename}_tr_acc.npy'), 'wb') as f:\n",
    "    np.save(f, train_accs)\n",
    "with open(os.path.join(path_results, f'{filename}_te_ls.npy'), 'wb') as f:\n",
    "    np.save(f, test_losses)\n",
    "with open(os.path.join(path_results, f'{filename}_te_acc.npy'), 'wb') as f:\n",
    "    np.save(f, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e644234-224e-4353-888c-01affc417a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(50, 30))\n",
    "axs = axs.ravel()\n",
    "\n",
    "axs[0].plot(test_accs, c='orange')\n",
    "axs[0].set_title('Test Accuracies')\n",
    "axs[0].set_xlabel('Rounds')\n",
    "axs[0].set_ylabel('Test Accuracy')\n",
    "axs[1].plot(test_losses, c='blue')\n",
    "axs[1].set_title('Test Losses')\n",
    "axs[1].set_xlabel('Rounds')\n",
    "axs[1].set_ylabel('Test Loss')\n",
    "axs[2].plot(train_accs, c='red')\n",
    "axs[2].set_title('Train Average Accuracies by Epochs')\n",
    "axs[2].set_xlabel('Epochs')\n",
    "axs[2].set_ylabel('Train Average Accuracy')\n",
    "axs[3].plot(train_losses.mean(axis=1), c='turquoise')\n",
    "axs[3].set_title('Train Average Losses by Epochs')\n",
    "axs[3].set_xlabel('Epochs')\n",
    "axs[3].set_ylabel('Train Average Loss')\n",
    "axs[4].plot(np.mean(train_accs.reshape(-1, 10), axis=1), c='green')\n",
    "axs[4].set_title('Train Average Accuracies by Rounds')\n",
    "axs[4].set_xlabel('Rounds')\n",
    "axs[4].set_ylabel('Train Average Accuracy')\n",
    "axs[5].plot(train_losses.mean(axis=1).reshape(-1, 10).mean(axis=1), c='lightpink')\n",
    "axs[5].set_title('Train Average Losses by Rounds')\n",
    "axs[5].set_xlabel('Rounds')\n",
    "axs[5].set_ylabel('Train Average Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9a5cc-41d2-41f1-a4cd-114d63d3b2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

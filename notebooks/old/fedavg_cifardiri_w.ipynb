{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35825f4-b26d-4ee5-8914-d28fdc8bc1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "dataset = 'cifar10'\n",
    "\n",
    "standardize = 0.1\n",
    "\n",
    "filename = '{}_diri9a001_42_fedavg_w{}'.format(dataset, ''.join(e for e in str(standardize) if e.isalnum()))\n",
    "\n",
    "gpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8447bc5c-393a-4f8d-80fd-50a652a7a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, Subset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "148a5cb1-99a9-4d54-8239-3ae733854155",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    torch.use_deterministic_algorithms(False)\n",
    "except AttributeError:\n",
    "    torch.set_deterministic(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5090116-fe54-40fe-8459-e9ce9aea270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = os.path.dirname(os.getcwd())\n",
    "path_data = os.path.join(path_root, 'data')\n",
    "path_logs = os.path.join(path_root, 'logs')\n",
    "path_models = os.path.join(path_root, 'models', filename)\n",
    "path_results = os.path.join(path_root, 'results', filename)\n",
    "\n",
    "for p in [path_data, path_logs, path_models, path_results]:\n",
    "    os.makedirs(p, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e4f24c-2bab-456f-9897-fb7cf4a72efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(filename)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "streamformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "streamhandler = logging.StreamHandler()\n",
    "streamhandler.setFormatter(streamformatter)\n",
    "logger.addHandler(streamhandler)\n",
    "\n",
    "fileformatter = logging.Formatter('%(message)s')\n",
    "filehandler = logging.FileHandler(os.path.join(path_logs, filename + '.log'), mode='w')\n",
    "filehandler.setFormatter(fileformatter)\n",
    "logger.addHandler(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4543504-728a-4bbf-8ebd-a70f223288d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, target = self.dataset[idx]\n",
    "        return data, target, idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0859e906-dbe3-4c6f-a44d-3ce50555d228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'cifar10':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([x / 255 for x in [125.3, 123, 113.9]], [x / 255 for x in [63, 62.1, 66.7]])\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([x / 255 for x in [125.3, 123, 113.9]], [x / 255 for x in [63, 62.1, 66.7]])\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.CIFAR10(path_data, train=False, transform=test_transform)\n",
    "    \n",
    "elif dataset == 'svhn':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.SVHN(path_data, split='train', transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.SVHN(path_data, split='test', transform=test_transform, download=True)\n",
    "    \n",
    "elif dataset == 'fmnist':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.FashionMNIST(path_data, train=False, transform=test_transform)\n",
    "\n",
    "elif dataset == 'mnist':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.MNIST(path_data, train=False, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6182a3f4-472d-4246-9ae0-e184be956dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_data, f'{dataset}_diri9a001_42.json')) as f:\n",
    "    indices = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02409155-7c8a-4217-8258-ccb1a2c7e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_workers = 128, 0\n",
    "\n",
    "inferloaders, subset_indices = {}, []\n",
    "for k, v in indices.items():\n",
    "    infersubset = Subset(custom_dataset, v['index'])\n",
    "    inferloaders[k] = DataLoader(infersubset, batch_size=batch_size, num_workers=num_workers)\n",
    "    subset_indices.extend(v['index'])\n",
    "\n",
    "train_subset = Subset(train_dataset, indices=subset_indices)\n",
    "fed_trainloader = DataLoader(train_subset, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "try:\n",
    "    train_labels = np.asarray(custom_dataset.dataset.targets)\n",
    "    test_labels = test_dataset.targets\n",
    "except AttributeError:\n",
    "    train_labels = np.asarray(custom_dataset.dataset.labels)\n",
    "    test_labels = test_dataset.labels\n",
    "subset_classes = np.unique(train_labels[subset_indices])\n",
    "boolarr = [True if y in subset_classes else False for y in test_labels]\n",
    "subset_indices = np.arange(len(test_dataset))[boolarr]\n",
    "test_subset = Subset(test_dataset, indices=subset_indices)\n",
    "testloader = DataLoader(test_subset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37423e2e-709a-4a3c-b0de-99acc3e83e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, padding=1, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=kernel_size, stride=1, padding=padding, bias=bias)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if (stride != 1) or (in_channel != self.expansion * out_channel):\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, self.expansion * out_channel, kernel_size=1, stride=stride, bias=bias),\n",
    "                nn.BatchNorm2d(self.expansion * out_channel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dd80881-9cee-4d1c-8348-aaf4eacc8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, padding=1, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=1, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv3 = nn.Conv2d(out_channel, self.expansion * out_channel, kerenel_size=1, bias=bias)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * out_channel)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if (stride != 1) or (in_channel != self.expansion * out_channel):\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, self.expansion * out_channel, kernel_size=1, stride=stride, bias=bias),\n",
    "                nn.BatchNorm2d(self.expansion * out_channel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eede0030-1cdc-4185-a7b3-825076d49102",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, in_channel=3, out_channels=[64, 128, 256, 512], num_blocks=[2, 2, 2, 2], strides=[1, 2, 2, 2], num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_channel = out_channels[0]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channels[0], kernel_size=3, stride=strides[0], padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels[0])\n",
    "        \n",
    "        self.block1 = self._make_layer(block, out_channels[0], num_blocks[0], strides[0])\n",
    "        self.block2 = self._make_layer(block, out_channels[1], num_blocks[1], strides[1])\n",
    "        self.block3 = self._make_layer(block, out_channels[2], num_blocks[2], strides[2])\n",
    "        self.block4 = self._make_layer(block, out_channels[3], num_blocks[3], strides[3])\n",
    "        \n",
    "        self.linear = nn.Linear(out_channels[-1] * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channel, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channel, out_channel, stride=stride))\n",
    "            self.in_channel = out_channel * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f232cb6d-451b-4973-a9bd-92afb2ac1451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplenorm(x):\n",
    "    return x / np.sum(x)\n",
    "\n",
    "def weighting(forget_cnt_per_client, standardize=0.1):\n",
    "    if (standardize == 0.0) or (np.sum(forget_cnt_per_client) == 0):\n",
    "        weights = np.ones(len(forget_cnt_per_client))\n",
    "    else:\n",
    "        weights = 1 - standardize + len(forget_cnt_per_client) * standardize * simplenorm(forget_cnt_per_client)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae0fe2f3-5ec9-46fe-b29c-8db8b81089ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(w, standardized_weights):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    \n",
    "    w_avg.update((k, v * standardized_weights[0]) for k, v in w_avg.items())\n",
    "    \n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += (w[i][key] * standardized_weights[i])\n",
    "        w_avg[key] = torch.div(w_avg[key], float(len(w)))\n",
    "        \n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "611e3c0f-9f0e-419a-aa1f-a497b671aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, epochs, lr, weight_decay, criterion, device):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        batch_losses = []\n",
    "        \n",
    "        for inputs, labels, _ in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss.mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "        epoch_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "        \n",
    "    local_weights = model.state_dict()\n",
    "\n",
    "    return local_weights, epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d5c54db-1ba4-4f65-817d-fb8b2b28fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, loader, criterion, device):\n",
    "    avg_loss, correct, num_samples = 0, 0, 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss.mean()\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += torch.sum(torch.eq(predicted, labels)).item()\n",
    "            num_samples += len(labels)\n",
    "\n",
    "    acc = correct / num_samples\n",
    "    avg_loss /= len(loader)\n",
    "    \n",
    "    return acc, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "629ff8ea-9394-4a48-a5e0-1a90d329a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_train(model, loader, device, match_history, flag=False):\n",
    "    forgettables = []\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, indices in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            match = predicted.eq(labels)\n",
    "            match = match.type(torch.IntTensor)\n",
    "            total += len(indices)\n",
    "            \n",
    "            for j, idx in enumerate(indices):\n",
    "                sample_history = match_history.get(idx.item(), [])\n",
    "                sample_history.append(match[j].item())\n",
    "                match_history[idx.item()] = sample_history\n",
    "                correct += match[j].item()\n",
    "                if flag is True:\n",
    "                    try:\n",
    "                        if match[j].item() - sample_history[-2] == -1:\n",
    "                            forgettables.append(idx.item())\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "            \n",
    "    acc = correct / total\n",
    "            \n",
    "    return match_history, forgettables, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7562c7c7-4368-442b-8bd7-fa1674a6842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatfloats(li):\n",
    "    new = [float(f'{e:>8.4f}') for e in li]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc4a3691-7135-496c-9153-23e7fc4f1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet9 = {'block': ResidualBlock, 'num_blocks': [1, 1, 1, 1]}\n",
    "resnet18 = {'block': ResidualBlock, 'num_blocks': [2, 2, 2, 2]}\n",
    "resnet34 = {'block': ResidualBlock, 'num_blocks': [3, 4, 6, 3]}\n",
    "resnet50 = {'block': Bottleneck, 'num_blocks': [3, 4, 6, 3]}\n",
    "resnet101 = {'block': Bottleneck, 'num_blocks': [3, 4, 23, 3]}\n",
    "resnet152 = {'block': Bottleneck, 'num_blocks': [3, 8, 36, 3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1261d399-3f2a-4f98-b585-81e5037a36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f'cuda:{gpu}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20a73dfb-ec2c-40fd-9893-3eac24fbca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = resnet9.copy()\n",
    "\n",
    "out_channels = [64, 128, 256, 512]\n",
    "strides = [1, 2, 2, 2]\n",
    "in_channel = 3\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "fed_model = ResNet(resnet['block'], in_channel=in_channel, out_channels=out_channels, num_blocks=resnet['num_blocks'], strides=strides, num_classes=num_classes)\n",
    "fed_weights = fed_model.state_dict()\n",
    "\n",
    "fed_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d0203-4dd4-4c5b-9f57-5693890ec7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-02 18:39:22,240 - INFO - \n",
      "Algorithm: FedAvg Weighted 0.1\n",
      "Client Split: 9\n",
      "Dataset: cifar10\n",
      "Model: ResNet9 | Rounds: 200 | Epochs: 10 | LR: 0.001\n",
      "\n",
      "2021-09-02 18:39:22,242 - INFO -  | Global Training Round : 1 / 200 |\n",
      "2021-09-02 18:39:37,193 - INFO -   |-- [Party 1] Average Train Loss:   1.5470 Train Accuracy: global  10.39% local  53.66% ...    0 forgettables out of 1819 ( 0.00%) ... total data used 1819\n",
      "2021-09-02 18:39:37,197 - INFO -   |--    Epoch Losses (10): [2.1042, 1.7897, 1.6828, 1.5865, 1.4988, 1.4776, 1.4133, 1.38, 1.2954, 1.2414]\n",
      "2021-09-02 18:39:44,789 - INFO -   |-- [Party 2] Average Train Loss:   1.6083 Train Accuracy: global  10.18% local  47.18% ...    0 forgettables out of 1081 ( 0.00%) ... total data used 1081\n",
      "2021-09-02 18:39:44,792 - INFO -   |--    Epoch Losses (10): [2.1244, 1.8739, 1.7418, 1.6242, 1.5761, 1.5332, 1.4729, 1.4597, 1.3744, 1.3021]\n",
      "2021-09-02 18:40:04,238 - INFO -   |-- [Party 3] Average Train Loss:   1.4923 Train Accuracy: global   9.51% local  53.69% ...    0 forgettables out of 2630 ( 0.00%) ... total data used 2630\n",
      "2021-09-02 18:40:04,243 - INFO -   |--    Epoch Losses (10): [2.0141, 1.7462, 1.6199, 1.5422, 1.4727, 1.4182, 1.3647, 1.3003, 1.2486, 1.196]\n",
      "2021-09-02 18:40:10,717 - INFO -   |-- [Party 4] Average Train Loss:   1.6980 Train Accuracy: global  10.49% local  45.90% ...    0 forgettables out of  915 ( 0.00%) ... total data used  915\n",
      "2021-09-02 18:40:10,719 - INFO -   |--    Epoch Losses (10): [2.1564, 1.9063, 1.77, 1.7065, 1.6752, 1.6033, 1.5907, 1.5617, 1.5136, 1.496]\n",
      "2021-09-02 18:40:23,437 - INFO -   |-- [Party 5] Average Train Loss:   1.7143 Train Accuracy: global   9.71% local  45.08% ...    0 forgettables out of 1668 ( 0.00%) ... total data used 1668\n",
      "2021-09-02 18:40:23,441 - INFO -   |--    Epoch Losses (10): [2.1044, 1.8104, 1.7502, 1.7752, 1.7553, 1.6557, 1.6007, 1.6127, 1.5341, 1.5446]\n",
      "2021-09-02 18:40:35,257 - INFO -   |-- [Party 6] Average Train Loss:   1.5981 Train Accuracy: global   9.15% local  54.45% ...    0 forgettables out of 1596 ( 0.00%) ... total data used 1596\n",
      "2021-09-02 18:40:35,259 - INFO -   |--    Epoch Losses (10): [2.1225, 1.8308, 1.7271, 1.6344, 1.5772, 1.5475, 1.4871, 1.4244, 1.3595, 1.2708]\n",
      "2021-09-02 18:40:50,846 - INFO -   |-- [Party 7] Average Train Loss:   1.4965 Train Accuracy: global  10.18% local  58.47% ...    0 forgettables out of 2131 ( 0.00%) ... total data used 2131\n",
      "2021-09-02 18:40:50,850 - INFO -   |--    Epoch Losses (10): [2.0845, 1.752, 1.6463, 1.5614, 1.4585, 1.3969, 1.351, 1.2688, 1.2402, 1.2058]\n",
      "2021-09-02 18:40:58,470 - INFO -   |-- [Party 8] Average Train Loss:   1.6173 Train Accuracy: global   9.41% local  53.32% ...    0 forgettables out of 1084 ( 0.00%) ... total data used 1084\n",
      "2021-09-02 18:40:58,474 - INFO -   |--    Epoch Losses (10): [2.2117, 1.8453, 1.7282, 1.6156, 1.5869, 1.558, 1.4778, 1.4248, 1.3584, 1.3666]\n",
      "2021-09-02 18:41:13,799 - INFO -   |-- [Party 9] Average Train Loss:   1.4419 Train Accuracy: global   9.00% local  57.37% ...    0 forgettables out of 2034 ( 0.00%) ... total data used 2034\n",
      "2021-09-02 18:41:13,803 - INFO -   |--    Epoch Losses (10): [1.9733, 1.6895, 1.6034, 1.5046, 1.4362, 1.3675, 1.2849, 1.2282, 1.1828, 1.1487]\n",
      "2021-09-02 18:41:22,952 - INFO -     |---- Number of Forgettables: 0 (0.00%)\n",
      "2021-09-02 18:41:22,953 - INFO -     |---- Train Accuracy: 25.95%\n",
      "2021-09-02 18:41:22,955 - INFO -     |---- Test Accuracy: 26.69%\n",
      "2021-09-02 18:41:22,955 - INFO -     |---- Test Loss: 2.0055\n",
      "2021-09-02 18:41:22,956 - INFO -     |---- Elapsed time: 0:02:00.714771\n",
      "2021-09-02 18:41:22,958 - INFO - \n",
      "Test Acc: Max 26.6900% (1 round) | Last 26.6900% | Avg 26.6900% (1 round)\n",
      "\n",
      "2021-09-02 18:41:22,962 - INFO -  | Global Training Round : 2 / 200 |\n",
      "2021-09-02 18:41:35,542 - INFO -   |-- [Party 1] Average Train Loss:   1.3175 Train Accuracy: global  27.65% local  60.80% ...  658 forgettables out of 1819 (36.17%) ... total data used 1819\n",
      "2021-09-02 18:41:35,545 - INFO -   |--    Epoch Losses (10): [1.7761, 1.5064, 1.4779, 1.3741, 1.3148, 1.2275, 1.1746, 1.152, 1.1031, 1.0681]\n",
      "2021-09-02 18:41:43,634 - INFO -   |-- [Party 2] Average Train Loss:   1.3574 Train Accuracy: global  27.94% local  57.45% ...  331 forgettables out of 1081 (30.62%) ... total data used 1081\n",
      "2021-09-02 18:41:43,636 - INFO -   |--    Epoch Losses (10): [1.8618, 1.5963, 1.5185, 1.4307, 1.3146, 1.2763, 1.1838, 1.1963, 1.1055, 1.0906]\n",
      "2021-09-02 18:42:03,332 - INFO -   |-- [Party 3] Average Train Loss:   1.2619 Train Accuracy: global  25.44% local  58.86% ... 1010 forgettables out of 2630 (38.40%) ... total data used 2630\n",
      "2021-09-02 18:42:03,336 - INFO -   |--    Epoch Losses (10): [1.7203, 1.5079, 1.4146, 1.2996, 1.2215, 1.1787, 1.1086, 1.1093, 1.0402, 1.0178]\n",
      "2021-09-02 18:42:10,090 - INFO -   |-- [Party 4] Average Train Loss:   1.4413 Train Accuracy: global  25.90% local  47.10% ...  287 forgettables out of  915 (31.37%) ... total data used  915\n",
      "2021-09-02 18:42:10,091 - INFO -   |--    Epoch Losses (10): [1.7808, 1.7085, 1.5124, 1.4988, 1.4389, 1.4093, 1.31, 1.3574, 1.2281, 1.1689]\n",
      "2021-09-02 18:42:22,222 - INFO -   |-- [Party 5] Average Train Loss:   1.4043 Train Accuracy: global  23.20% local  50.06% ...  541 forgettables out of 1668 (32.43%) ... total data used 1668\n",
      "2021-09-02 18:42:22,226 - INFO -   |--    Epoch Losses (10): [1.6603, 1.6903, 1.5013, 1.4245, 1.4633, 1.3528, 1.3037, 1.2448, 1.2315, 1.1702]\n",
      "2021-09-02 18:42:32,585 - INFO -   |-- [Party 6] Average Train Loss:   1.3316 Train Accuracy: global  23.50% local  57.39% ...  643 forgettables out of 1596 (40.29%) ... total data used 1596\n",
      "2021-09-02 18:42:32,589 - INFO -   |--    Epoch Losses (10): [1.8047, 1.5384, 1.4688, 1.3822, 1.3306, 1.2655, 1.173, 1.156, 1.1006, 1.0967]\n",
      "2021-09-02 18:42:46,275 - INFO -   |-- [Party 7] Average Train Loss:   1.2900 Train Accuracy: global  26.23% local  63.12% ...  869 forgettables out of 2131 (40.78%) ... total data used 2131\n",
      "2021-09-02 18:42:46,278 - INFO -   |--    Epoch Losses (10): [1.8075, 1.508, 1.4059, 1.3429, 1.276, 1.228, 1.1632, 1.1025, 1.0584, 1.0073]\n",
      "2021-09-02 18:42:53,368 - INFO -   |-- [Party 8] Average Train Loss:   1.2889 Train Accuracy: global  26.29% local  58.03% ...  391 forgettables out of 1084 (36.07%) ... total data used 1084\n",
      "2021-09-02 18:42:53,372 - INFO -   |--    Epoch Losses (10): [1.732, 1.5345, 1.4241, 1.3642, 1.2866, 1.2001, 1.1521, 1.083, 1.0512, 1.0608]\n",
      "2021-09-02 18:43:07,126 - INFO -   |-- [Party 9] Average Train Loss:   1.2373 Train Accuracy: global  25.86% local  59.29% ...  836 forgettables out of 2034 (41.10%) ... total data used 2034\n",
      "2021-09-02 18:43:07,127 - INFO -   |--    Epoch Losses (10): [1.7235, 1.4846, 1.379, 1.2937, 1.2139, 1.1314, 1.0931, 1.057, 1.0318, 0.9654]\n",
      "2021-09-02 18:43:15,972 - INFO -     |---- Number of Forgettables: 5566 (37.21%)\n",
      "2021-09-02 18:43:15,975 - INFO -     |---- Train Accuracy: 53.32%\n",
      "2021-09-02 18:43:15,976 - INFO -     |---- Test Accuracy: 52.97%\n",
      "2021-09-02 18:43:15,977 - INFO -     |---- Test Loss: 1.2931\n",
      "2021-09-02 18:43:15,977 - INFO -     |---- Elapsed time: 0:03:53.735832\n",
      "2021-09-02 18:43:15,978 - INFO - \n",
      "Test Acc: Max 52.9700% (2 round) | Last 52.9700% | Avg 39.8300% (2 round)\n",
      "\n",
      "2021-09-02 18:43:15,981 - INFO -  | Global Training Round : 3 / 200 |\n",
      "2021-09-02 18:43:29,103 - INFO -   |-- [Party 1] Average Train Loss:   1.0266 Train Accuracy: global  52.61% local  71.41% ...  377 forgettables out of 1819 (20.73%) ... total data used 1819\n",
      "2021-09-02 18:43:29,105 - INFO -   |--    Epoch Losses (10): [1.3759, 1.2362, 1.1456, 1.0742, 1.0182, 0.9358, 0.9009, 0.8854, 0.8571, 0.8365]\n",
      "2021-09-02 18:43:36,586 - INFO -   |-- [Party 2] Average Train Loss:   1.0304 Train Accuracy: global  52.08% local  75.49% ...  203 forgettables out of 1081 (18.78%) ... total data used 1081\n",
      "2021-09-02 18:43:36,588 - INFO -   |--    Epoch Losses (10): [1.3443, 1.3201, 1.1964, 1.1059, 1.0216, 0.9454, 0.8968, 0.8533, 0.8342, 0.7861]\n",
      "2021-09-02 18:43:54,378 - INFO -   |-- [Party 3] Average Train Loss:   1.0082 Train Accuracy: global  53.04% local  72.81% ...  462 forgettables out of 2630 (17.57%) ... total data used 2630\n",
      "2021-09-02 18:43:54,380 - INFO -   |--    Epoch Losses (10): [1.3224, 1.1711, 1.1042, 1.0609, 0.9992, 1.0099, 0.9109, 0.872, 0.819, 0.8121]\n",
      "2021-09-02 18:44:01,336 - INFO -   |-- [Party 4] Average Train Loss:   1.0655 Train Accuracy: global  49.95% local  70.82% ...  170 forgettables out of  915 (18.58%) ... total data used  915\n",
      "2021-09-02 18:44:01,338 - INFO -   |--    Epoch Losses (10): [1.3954, 1.2305, 1.2015, 1.1102, 1.0361, 0.9768, 0.993, 0.9187, 0.9003, 0.892]\n",
      "2021-09-02 18:44:13,537 - INFO -   |-- [Party 5] Average Train Loss:   1.1706 Train Accuracy: global  53.72% local  63.91% ...  201 forgettables out of 1668 (12.05%) ... total data used 1668\n",
      "2021-09-02 18:44:13,538 - INFO -   |--    Epoch Losses (10): [1.3617, 1.2854, 1.1745, 1.1783, 1.1789, 1.2049, 1.0774, 1.1086, 1.068, 1.0679]\n",
      "2021-09-02 18:44:24,940 - INFO -   |-- [Party 6] Average Train Loss:   1.0386 Train Accuracy: global  52.51% local  65.04% ...  280 forgettables out of 1596 (17.54%) ... total data used 1596\n",
      "2021-09-02 18:44:24,941 - INFO -   |--    Epoch Losses (10): [1.3909, 1.2664, 1.1859, 1.1008, 1.0051, 0.974, 0.9121, 0.8587, 0.8281, 0.8637]\n",
      "2021-09-02 18:44:39,391 - INFO -   |-- [Party 7] Average Train Loss:   0.9920 Train Accuracy: global  55.09% local  70.91% ...  404 forgettables out of 2131 (18.96%) ... total data used 2131\n",
      "2021-09-02 18:44:39,393 - INFO -   |--    Epoch Losses (10): [1.3498, 1.166, 1.0765, 1.0415, 1.027, 0.9422, 0.8687, 0.8461, 0.8175, 0.7843]\n",
      "2021-09-02 18:44:46,689 - INFO -   |-- [Party 8] Average Train Loss:   0.9753 Train Accuracy: global  52.77% local  73.71% ...  193 forgettables out of 1084 (17.80%) ... total data used 1084\n",
      "2021-09-02 18:44:46,692 - INFO -   |--    Epoch Losses (10): [1.4136, 1.2047, 1.0809, 1.0098, 0.9687, 0.9007, 0.8629, 0.8297, 0.7855, 0.6969]\n",
      "2021-09-02 18:45:00,538 - INFO -   |-- [Party 9] Average Train Loss:   0.9216 Train Accuracy: global  54.92% local  71.68% ...  407 forgettables out of 2034 (20.01%) ... total data used 2034\n",
      "2021-09-02 18:45:00,540 - INFO -   |--    Epoch Losses (10): [1.2908, 1.0881, 1.0394, 0.9878, 0.9302, 0.8318, 0.816, 0.8115, 0.7191, 0.7014]\n",
      "2021-09-02 18:45:09,407 - INFO -     |---- Number of Forgettables: 2697 (18.03%)\n",
      "2021-09-02 18:45:09,408 - INFO -     |---- Train Accuracy: 66.55%\n",
      "2021-09-02 18:45:09,410 - INFO -     |---- Test Accuracy: 65.04%\n",
      "2021-09-02 18:45:09,410 - INFO -     |---- Test Loss: 1.0127\n",
      "2021-09-02 18:45:09,411 - INFO -     |---- Elapsed time: 0:05:47.169656\n",
      "2021-09-02 18:45:09,412 - INFO - \n",
      "Test Acc: Max 65.0400% (3 round) | Last 65.0400% | Avg 48.2333% (2 round)\n",
      "\n",
      "2021-09-02 18:45:09,414 - INFO -  | Global Training Round : 4 / 200 |\n"
     ]
    }
   ],
   "source": [
    "rounds = 200\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "wdecay = 0\n",
    "\n",
    "logger.info(f'\\nAlgorithm: FedAvg Weighted {standardize}\\nClient Split: 9\\nDataset: {dataset}\\nModel: ResNet9 | Rounds: {rounds} | Epochs: {epochs} | LR: {lr}\\n')\n",
    "\n",
    "train_accs, train_losses, test_accs, test_losses = [], [], [], []\n",
    "match_history, round_forget_history = {}, []\n",
    "forget_cnt_per_client = [0] * len(indices.keys())\n",
    "\n",
    "st = time.time()\n",
    "for r in range(rounds):\n",
    "    \n",
    "    round_forget, round_samples = 0, 0\n",
    "    forget_history, forgettables = {}, {}\n",
    "    local_weights, local_losses = [], []\n",
    "    logger.info(f' | Global Training Round : {r + 1} / {rounds} |')\n",
    "    \n",
    "    for i, k in enumerate(indices.keys()):\n",
    "        match_history, forgettables, global_acc = infer_train(fed_model, inferloaders[k], device, match_history, flag=True)\n",
    "\n",
    "        fed_model.train()\n",
    "        \n",
    "        sampler_idx = indices[k]['index'].copy()\n",
    "        sampler = SubsetRandomSampler(sampler_idx)\n",
    "\n",
    "        trainloader = DataLoader(custom_dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
    "        \n",
    "        local_model = copy.deepcopy(fed_model)\n",
    "        w, ls = train(local_model, trainloader, epochs, lr, wdecay, criterion, device)\n",
    "\n",
    "        local_weights.append(copy.deepcopy(w))\n",
    "        train_losses.append(ls)\n",
    "        \n",
    "        match_history, _, local_acc = infer_train(local_model, inferloaders[k], device, match_history, flag=False)\n",
    "        train_accs.append(local_acc)\n",
    "        \n",
    "        forget_cnt = len(forgettables)\n",
    "        round_forget += forget_cnt\n",
    "        forget_cnt_per_client[int(k)] = forget_cnt\n",
    "        round_samples += len(indices[k]['index'])\n",
    "        \n",
    "        logger.info('  |-- [Party {:>1}] Average Train Loss: {:>8.4f} Train Accuracy: global {:>6.2f}% local {:>6.2f}% ... {:>4} forgettables out of {:>4} ({:>5.2f}%) ... total data used {:>4}'.format(\n",
    "            i + 1, sum(ls) / len(ls), 100 * global_acc, 100 * local_acc, forget_cnt, len(indices[k]['index']), 100 * forget_cnt / len(indices[k]['index']), len(sampler_idx)\n",
    "        ))\n",
    "        logger.info('  |--    Epoch Losses ({:>2}): {}'.format(epochs, formatfloats(ls)))\n",
    "        \n",
    "    standardized_weights = weighting(forget_cnt_per_client, standardize)\n",
    "    fed_weights = average_weights(local_weights, standardized_weights)\n",
    "    fed_model.load_state_dict(fed_weights)\n",
    "    train_acc, _ = inference(fed_model, fed_trainloader, criterion, device)\n",
    "    \n",
    "    if (r + 1) % 50 == 0:\n",
    "        torch.save(fed_model.state_dict(), os.path.join(path_models, filename + f'_round{r+1}.pth'))\n",
    "    \n",
    "    test_acc, test_ls = inference(fed_model, testloader, criterion, device)\n",
    "    test_accs.append(test_acc)\n",
    "    test_losses.append(test_ls)\n",
    "    round_forget_history.append(round_forget)\n",
    "    logger.info('    |---- Number of Forgettables: {} ({:.2f}%)'.format(round_forget, 100 * round_forget / round_samples))\n",
    "    logger.info('    |---- Train Accuracy: {:>.2f}%'.format(100 * train_acc))\n",
    "    logger.info('    |---- Test Accuracy: {:>.2f}%'.format(100 * test_acc))\n",
    "    logger.info('    |---- Test Loss: {:.4f}'.format(test_ls))\n",
    "    logger.info('    |---- Elapsed time: {}'.format(timedelta(seconds=time.time()-st)))\n",
    "    logger.info(f'\\nTest Acc: Max {np.max(test_acc) * 100:.4f}% ({np.argmax(test_accs)+1} round) | Last {test_accs[-1] * 100:.4f}% | Avg {np.mean(test_accs) * 100:.4f}% ({np.argmax(test_accs > np.mean(test_accs))+1} round)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654dbc03-4dd8-4c75-932e-f9f842c9ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = np.asarray(train_losses)\n",
    "train_accs = np.asarray(train_accs)\n",
    "\n",
    "with open(os.path.join(path_results, f'{filename}_tr_ls.npy'), 'wb') as f:\n",
    "    np.save(f, train_losses)\n",
    "with open(os.path.join(path_results, f'{filename}_tr_acc.npy'), 'wb') as f:\n",
    "    np.save(f, train_accs)\n",
    "with open(os.path.join(path_results, f'{filename}_te_ls.npy'), 'wb') as f:\n",
    "    np.save(f, test_losses)\n",
    "with open(os.path.join(path_results, f'{filename}_te_acc.npy'), 'wb') as f:\n",
    "    np.save(f, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e644234-224e-4353-888c-01affc417a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(50, 30))\n",
    "axs = axs.ravel()\n",
    "\n",
    "axs[0].plot(test_accs, c='orange')\n",
    "axs[0].set_title('Test Accuracies')\n",
    "axs[0].set_xlabel('Rounds')\n",
    "axs[0].set_ylabel('Test Accuracy')\n",
    "axs[1].plot(test_losses, c='blue')\n",
    "axs[1].set_title('Test Losses')\n",
    "axs[1].set_xlabel('Rounds')\n",
    "axs[1].set_ylabel('Test Loss')\n",
    "axs[2].plot(train_accs, c='red')\n",
    "axs[2].set_title('Train Average Accuracies by Epochs')\n",
    "axs[2].set_xlabel('Epochs')\n",
    "axs[2].set_ylabel('Train Average Accuracy')\n",
    "axs[3].plot(train_losses.mean(axis=1), c='turquoise')\n",
    "axs[3].set_title('Train Average Losses by Epochs')\n",
    "axs[3].set_xlabel('Epochs')\n",
    "axs[3].set_ylabel('Train Average Loss')\n",
    "axs[4].plot(np.mean(train_accs.reshape(-1, 10), axis=1), c='green')\n",
    "axs[4].set_title('Train Average Accuracies by Rounds')\n",
    "axs[4].set_xlabel('Rounds')\n",
    "axs[4].set_ylabel('Train Average Accuracy')\n",
    "axs[5].plot(train_losses.mean(axis=1).reshape(-1, 10).mean(axis=1), c='lightpink')\n",
    "axs[5].set_title('Train Average Losses by Rounds')\n",
    "axs[5].set_xlabel('Rounds')\n",
    "axs[5].set_ylabel('Train Average Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9a5cc-41d2-41f1-a4cd-114d63d3b2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

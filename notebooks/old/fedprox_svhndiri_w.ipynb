{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35825f4-b26d-4ee5-8914-d28fdc8bc1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "dataset = 'svhn'\n",
    "\n",
    "standardize = 0.1\n",
    "\n",
    "filename = '{}_diri9a001_42_fedprox_w{}'.format(dataset, ''.join(e for e in str(standardize) if e.isalnum()))\n",
    "\n",
    "gpu = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8447bc5c-393a-4f8d-80fd-50a652a7a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, Subset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "148a5cb1-99a9-4d54-8239-3ae733854155",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    torch.use_deterministic_algorithms(False)\n",
    "except AttributeError:\n",
    "    torch.set_deterministic(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5090116-fe54-40fe-8459-e9ce9aea270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = os.path.dirname(os.getcwd())\n",
    "path_data = os.path.join(path_root, 'data')\n",
    "path_logs = os.path.join(path_root, 'logs')\n",
    "path_models = os.path.join(path_root, 'models', filename)\n",
    "path_results = os.path.join(path_root, 'results', filename)\n",
    "\n",
    "for p in [path_data, path_logs, path_models, path_results]:\n",
    "    os.makedirs(p, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e4f24c-2bab-456f-9897-fb7cf4a72efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(filename)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "streamformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "streamhandler = logging.StreamHandler()\n",
    "streamhandler.setFormatter(streamformatter)\n",
    "logger.addHandler(streamhandler)\n",
    "\n",
    "fileformatter = logging.Formatter('%(message)s')\n",
    "filehandler = logging.FileHandler(os.path.join(path_logs, filename + '.log'), mode='w')\n",
    "filehandler.setFormatter(fileformatter)\n",
    "logger.addHandler(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4543504-728a-4bbf-8ebd-a70f223288d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, target = self.dataset[idx]\n",
    "        return data, target, idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0859e906-dbe3-4c6f-a44d-3ce50555d228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/hgato/projects/benchmark/data/train_32x32.mat\n",
      "Using downloaded and verified file: /home/hgato/projects/benchmark/data/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'cifar10':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([x / 255 for x in [125.3, 123, 113.9]], [x / 255 for x in [63, 62.1, 66.7]])\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([x / 255 for x in [125.3, 123, 113.9]], [x / 255 for x in [63, 62.1, 66.7]])\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.CIFAR10(path_data, train=False, transform=test_transform)\n",
    "    \n",
    "elif dataset == 'svhn':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.SVHN(path_data, split='train', transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.SVHN(path_data, split='test', transform=test_transform, download=True)\n",
    "    \n",
    "elif dataset == 'fmnist':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.FashionMNIST(path_data, train=False, transform=test_transform)\n",
    "\n",
    "elif dataset == 'mnist':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.MNIST(path_data, train=False, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6182a3f4-472d-4246-9ae0-e184be956dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_data, f'{dataset}_diri9a001_42.json')) as f:\n",
    "    indices = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02409155-7c8a-4217-8258-ccb1a2c7e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_workers = 128, 0\n",
    "\n",
    "inferloaders, subset_indices = {}, []\n",
    "for k, v in indices.items():\n",
    "    infersubset = Subset(custom_dataset, v['index'])\n",
    "    inferloaders[k] = DataLoader(infersubset, batch_size=batch_size, num_workers=num_workers)\n",
    "    subset_indices.extend(v['index'])\n",
    "\n",
    "train_subset = Subset(train_dataset, indices=subset_indices)\n",
    "fed_trainloader = DataLoader(train_subset, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "try:\n",
    "    train_labels = np.asarray(custom_dataset.dataset.targets)\n",
    "    test_labels = test_dataset.targets\n",
    "except AttributeError:\n",
    "    train_labels = np.asarray(custom_dataset.dataset.labels)\n",
    "    test_labels = test_dataset.labels\n",
    "subset_classes = np.unique(train_labels[subset_indices])\n",
    "boolarr = [True if y in subset_classes else False for y in test_labels]\n",
    "subset_indices = np.arange(len(test_dataset))[boolarr]\n",
    "test_subset = Subset(test_dataset, indices=subset_indices)\n",
    "testloader = DataLoader(test_subset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37423e2e-709a-4a3c-b0de-99acc3e83e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, padding=1, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=kernel_size, stride=1, padding=padding, bias=bias)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if (stride != 1) or (in_channel != self.expansion * out_channel):\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, self.expansion * out_channel, kernel_size=1, stride=stride, bias=bias),\n",
    "                nn.BatchNorm2d(self.expansion * out_channel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dd80881-9cee-4d1c-8348-aaf4eacc8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, padding=1, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=1, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv3 = nn.Conv2d(out_channel, self.expansion * out_channel, kerenel_size=1, bias=bias)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * out_channel)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if (stride != 1) or (in_channel != self.expansion * out_channel):\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, self.expansion * out_channel, kernel_size=1, stride=stride, bias=bias),\n",
    "                nn.BatchNorm2d(self.expansion * out_channel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eede0030-1cdc-4185-a7b3-825076d49102",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, in_channel=3, out_channels=[64, 128, 256, 512], num_blocks=[2, 2, 2, 2], strides=[1, 2, 2, 2], num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_channel = out_channels[0]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channels[0], kernel_size=3, stride=strides[0], padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels[0])\n",
    "        \n",
    "        self.block1 = self._make_layer(block, out_channels[0], num_blocks[0], strides[0])\n",
    "        self.block2 = self._make_layer(block, out_channels[1], num_blocks[1], strides[1])\n",
    "        self.block3 = self._make_layer(block, out_channels[2], num_blocks[2], strides[2])\n",
    "        self.block4 = self._make_layer(block, out_channels[3], num_blocks[3], strides[3])\n",
    "        \n",
    "        self.linear = nn.Linear(out_channels[-1] * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channel, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channel, out_channel, stride=stride))\n",
    "            self.in_channel = out_channel * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9209704a-22fa-4e92-b5e2-b666bda96ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplenorm(x):\n",
    "    return x / np.sum(x)\n",
    "\n",
    "def weighting(forget_cnt_per_client, standardize=0.1):\n",
    "    if (standardize == 0.0) or (np.sum(forget_cnt_per_client) == 0):\n",
    "        weights = np.ones(len(forget_cnt_per_client))\n",
    "    else:\n",
    "        weights = 1 - standardize + len(forget_cnt_per_client) * standardize * simplenorm(forget_cnt_per_client)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae0fe2f3-5ec9-46fe-b29c-8db8b81089ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(w, standardized_weights):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    \n",
    "    w_avg.update((k, v * standardized_weights[0]) for k, v in w_avg.items())\n",
    "    \n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += (w[i][key] * standardized_weights[i])\n",
    "        w_avg[key] = torch.div(w_avg[key], float(len(w)))\n",
    "        \n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "611e3c0f-9f0e-419a-aa1f-a497b671aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, epochs, lr, weight_decay, criterion, device, global_model, mu=0.01):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    global_params = list(global_model.parameters())\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        batch_losses = []\n",
    "        \n",
    "        for inputs, labels, _ in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss.mean()\n",
    "            \n",
    "            prox_reg = 0.0\n",
    "            for i, param in enumerate(model.parameters()):\n",
    "                prox_reg += ((mu / 2) * torch.norm((param - global_params[i])) ** 2)\n",
    "            loss += prox_reg\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "        epoch_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "        \n",
    "    local_weights = model.state_dict()\n",
    "\n",
    "    return local_weights, epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d5c54db-1ba4-4f65-817d-fb8b2b28fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, loader, criterion, device):\n",
    "    avg_loss, correct, num_samples = 0, 0, 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss.mean()\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += torch.sum(torch.eq(predicted, labels)).item()\n",
    "            num_samples += len(labels)\n",
    "\n",
    "    acc = correct / num_samples\n",
    "    avg_loss /= len(loader)\n",
    "    \n",
    "    return acc, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "629ff8ea-9394-4a48-a5e0-1a90d329a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_train(model, loader, device, match_history, flag=False):\n",
    "    forgettables = []\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, indices in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            match = predicted.eq(labels)\n",
    "            match = match.type(torch.IntTensor)\n",
    "            total += len(indices)\n",
    "            \n",
    "            for j, idx in enumerate(indices):\n",
    "                sample_history = match_history.get(idx.item(), [])\n",
    "                sample_history.append(match[j].item())\n",
    "                match_history[idx.item()] = sample_history\n",
    "                correct += match[j].item()\n",
    "                if flag is True:\n",
    "                    try:\n",
    "                        if match[j].item() - sample_history[-2] == -1:\n",
    "                            forgettables.append(idx.item())\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "            \n",
    "    acc = correct / total\n",
    "            \n",
    "    return match_history, forgettables, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7562c7c7-4368-442b-8bd7-fa1674a6842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatfloats(li):\n",
    "    new = [float(f'{e:>8.4f}') for e in li]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc4a3691-7135-496c-9153-23e7fc4f1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet9 = {'block': ResidualBlock, 'num_blocks': [1, 1, 1, 1]}\n",
    "resnet18 = {'block': ResidualBlock, 'num_blocks': [2, 2, 2, 2]}\n",
    "resnet34 = {'block': ResidualBlock, 'num_blocks': [3, 4, 6, 3]}\n",
    "resnet50 = {'block': Bottleneck, 'num_blocks': [3, 4, 6, 3]}\n",
    "resnet101 = {'block': Bottleneck, 'num_blocks': [3, 4, 23, 3]}\n",
    "resnet152 = {'block': Bottleneck, 'num_blocks': [3, 8, 36, 3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1261d399-3f2a-4f98-b585-81e5037a36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f'cuda:{gpu}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20a73dfb-ec2c-40fd-9893-3eac24fbca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = resnet9.copy()\n",
    "\n",
    "out_channels = [64, 128, 256, 512]\n",
    "strides = [1, 2, 2, 2]\n",
    "in_channel = 3\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "fed_model = ResNet(resnet['block'], in_channel=in_channel, out_channels=out_channels, num_blocks=resnet['num_blocks'], strides=strides, num_classes=num_classes)\n",
    "fed_weights = fed_model.state_dict()\n",
    "\n",
    "fed_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d0203-4dd4-4c5b-9f57-5693890ec7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-02 18:52:49,524 - INFO - \n",
      "Algorithm: FedProx Weighted 0.1\n",
      "Client Split: 9\n",
      "Dataset: svhn\n",
      "Model: ResNet9 | Rounds: 200 | Epochs: 10 | LR: 0.001\n",
      "\n",
      "2021-09-02 18:52:49,527 - INFO -  | Global Training Round : 1 / 200 |\n",
      "2021-09-02 18:53:13,047 - INFO -   |-- [Party 1] Average Train Loss:   2.1781 Train Accuracy: global   7.43% local  41.51% ...    0 forgettables out of 2828 ( 0.00%) ... total data used 2828\n",
      "2021-09-02 18:53:13,049 - INFO -   |--    Epoch Losses (10): [2.6825, 2.6166, 2.4235, 2.2991, 2.2331, 2.0726, 2.0194, 1.8804, 1.7916, 1.7621]\n",
      "2021-09-02 18:53:32,297 - INFO -   |-- [Party 2] Average Train Loss:   2.0964 Train Accuracy: global   6.61% local  52.53% ...    0 forgettables out of 2376 ( 0.00%) ... total data used 2376\n",
      "2021-09-02 18:53:32,299 - INFO -   |--    Epoch Losses (10): [2.6242, 2.6229, 2.438, 2.2809, 2.1109, 1.9701, 1.8755, 1.7636, 1.6977, 1.5809]\n",
      "2021-09-02 18:54:05,957 - INFO -   |-- [Party 3] Average Train Loss:   1.8043 Train Accuracy: global   6.51% local  63.31% ...    0 forgettables out of 4252 ( 0.00%) ... total data used 4252\n",
      "2021-09-02 18:54:05,961 - INFO -   |--    Epoch Losses (10): [2.6723, 2.4675, 2.1627, 1.9246, 1.7525, 1.6016, 1.4451, 1.3475, 1.3509, 1.3188]\n",
      "2021-09-02 18:54:17,355 - INFO -   |-- [Party 4] Average Train Loss:   2.2536 Train Accuracy: global   6.70% local  48.56% ...    0 forgettables out of 1567 ( 0.00%) ... total data used 1567\n",
      "2021-09-02 18:54:17,357 - INFO -   |--    Epoch Losses (10): [2.6348, 2.719, 2.6117, 2.4474, 2.2879, 2.169, 2.0945, 1.9728, 1.8452, 1.7542]\n",
      "2021-09-02 18:54:37,430 - INFO -   |-- [Party 5] Average Train Loss:   2.0522 Train Accuracy: global   6.52% local  55.45% ...    0 forgettables out of 2516 ( 0.00%) ... total data used 2516\n",
      "2021-09-02 18:54:37,433 - INFO -   |--    Epoch Losses (10): [2.6258, 2.6039, 2.3506, 2.1932, 2.0631, 1.8932, 1.8211, 1.7473, 1.6653, 1.5586]\n",
      "2021-09-02 18:54:59,042 - INFO -   |-- [Party 6] Average Train Loss:   2.0574 Train Accuracy: global   6.98% local  49.85% ...    0 forgettables out of 2967 ( 0.00%) ... total data used 2967\n",
      "2021-09-02 18:54:59,045 - INFO -   |--    Epoch Losses (10): [2.6512, 2.5676, 2.3397, 2.2222, 2.0775, 1.9064, 1.8294, 1.7328, 1.6941, 1.553]\n",
      "2021-09-02 18:55:15,211 - INFO -   |-- [Party 7] Average Train Loss:   2.1175 Train Accuracy: global   6.81% local  45.40% ...    0 forgettables out of 1998 ( 0.00%) ... total data used 1998\n",
      "2021-09-02 18:55:15,214 - INFO -   |--    Epoch Losses (10): [2.6517, 2.7036, 2.5201, 2.2888, 2.1485, 1.9421, 1.8549, 1.7435, 1.6779, 1.6441]\n",
      "2021-09-02 18:55:27,808 - INFO -   |-- [Party 8] Average Train Loss:   2.3186 Train Accuracy: global   6.01% local  40.44% ...    0 forgettables out of 1664 ( 0.00%) ... total data used 1664\n",
      "2021-09-02 18:55:27,810 - INFO -   |--    Epoch Losses (10): [2.598, 2.6697, 2.5753, 2.4308, 2.3056, 2.2517, 2.2178, 2.172, 2.0274, 1.938]\n",
      "2021-09-02 18:55:41,404 - INFO -   |-- [Party 9] Average Train Loss:   2.1452 Train Accuracy: global   6.15% local  45.05% ...    0 forgettables out of 1756 ( 0.00%) ... total data used 1756\n",
      "2021-09-02 18:55:41,406 - INFO -   |--    Epoch Losses (10): [2.6332, 2.6585, 2.5352, 2.349, 2.162, 2.0296, 1.921, 1.8071, 1.7084, 1.6482]\n",
      "2021-09-02 18:55:57,799 - INFO -     |---- Number of Forgettables: 0 (0.00%)\n",
      "2021-09-02 18:55:57,800 - INFO -     |---- Train Accuracy: 11.66%\n",
      "2021-09-02 18:55:57,801 - INFO -     |---- Test Accuracy: 11.07%\n",
      "2021-09-02 18:55:57,803 - INFO -     |---- Test Loss: 2.5016\n",
      "2021-09-02 18:55:57,803 - INFO -     |---- Elapsed time: 0:03:08.276725\n",
      "2021-09-02 18:55:57,805 - INFO - \n",
      "Test Acc: Max 11.0710% (1 round) | Last 11.0710% | Avg 11.0710% (1 round)\n",
      "\n",
      "2021-09-02 18:55:57,807 - INFO -  | Global Training Round : 2 / 200 |\n",
      "2021-09-02 18:56:16,916 - INFO -   |-- [Party 1] Average Train Loss:   1.4464 Train Accuracy: global  11.70% local  63.26% ... 1083 forgettables out of 2828 (38.30%) ... total data used 2828\n",
      "2021-09-02 18:56:16,917 - INFO -   |--    Epoch Losses (10): [2.1013, 1.7779, 1.5638, 1.3879, 1.3219, 1.2827, 1.3097, 1.1883, 1.2981, 1.2326]\n",
      "2021-09-02 18:56:32,844 - INFO -   |-- [Party 2] Average Train Loss:   1.3550 Train Accuracy: global  11.83% local  45.66% ... 1135 forgettables out of 2376 (47.77%) ... total data used 2376\n",
      "2021-09-02 18:56:32,846 - INFO -   |--    Epoch Losses (10): [2.0577, 1.7784, 1.4648, 1.2953, 1.2354, 1.2167, 1.199, 1.1285, 1.0863, 1.0879]\n",
      "2021-09-02 18:57:01,318 - INFO -   |-- [Party 3] Average Train Loss:   1.2733 Train Accuracy: global  11.19% local  70.53% ... 2358 forgettables out of 4252 (55.46%) ... total data used 4252\n",
      "2021-09-02 18:57:01,319 - INFO -   |--    Epoch Losses (10): [1.8982, 1.4499, 1.2764, 1.1904, 1.1818, 1.1395, 1.1442, 1.1476, 1.1361, 1.1693]\n",
      "2021-09-02 18:57:11,925 - INFO -   |-- [Party 4] Average Train Loss:   1.5271 Train Accuracy: global  11.68% local  60.05% ...  672 forgettables out of 1567 (42.88%) ... total data used 1567\n",
      "2021-09-02 18:57:11,926 - INFO -   |--    Epoch Losses (10): [2.1573, 1.9643, 1.7425, 1.548, 1.4212, 1.3371, 1.2966, 1.3183, 1.2278, 1.2579]\n",
      "2021-09-02 18:57:29,116 - INFO -   |-- [Party 5] Average Train Loss:   1.3811 Train Accuracy: global  11.92% local  70.47% ... 1206 forgettables out of 2516 (47.93%) ... total data used 2516\n",
      "2021-09-02 18:57:29,117 - INFO -   |--    Epoch Losses (10): [2.1562, 1.8266, 1.5215, 1.3439, 1.2315, 1.1898, 1.1529, 1.1232, 1.1233, 1.1426]\n",
      "2021-09-02 18:57:51,291 - INFO -   |-- [Party 6] Average Train Loss:   1.3651 Train Accuracy: global  11.49% local  70.21% ... 1283 forgettables out of 2967 (43.24%) ... total data used 2967\n",
      "2021-09-02 18:57:51,293 - INFO -   |--    Epoch Losses (10): [2.019, 1.6847, 1.4573, 1.3032, 1.2367, 1.2158, 1.2511, 1.2215, 1.1263, 1.1359]\n",
      "2021-09-02 18:58:05,083 - INFO -   |-- [Party 7] Average Train Loss:   1.3731 Train Accuracy: global   9.81% local  61.46% ...  865 forgettables out of 1998 (43.29%) ... total data used 1998\n",
      "2021-09-02 18:58:05,084 - INFO -   |--    Epoch Losses (10): [2.066, 1.8281, 1.5476, 1.3794, 1.2639, 1.1775, 1.1444, 1.137, 1.1001, 1.0868]\n",
      "2021-09-02 18:58:17,471 - INFO -   |-- [Party 8] Average Train Loss:   1.4181 Train Accuracy: global  14.30% local  70.97% ...  645 forgettables out of 1664 (38.76%) ... total data used 1664\n",
      "2021-09-02 18:58:17,473 - INFO -   |--    Epoch Losses (10): [2.1045, 1.8645, 1.6393, 1.4342, 1.3007, 1.2089, 1.1754, 1.1783, 1.1605, 1.1143]\n",
      "2021-09-02 18:58:29,858 - INFO -   |-- [Party 9] Average Train Loss:   1.4161 Train Accuracy: global  11.96% local  51.99% ...  672 forgettables out of 1756 (38.27%) ... total data used 1756\n",
      "2021-09-02 18:58:29,859 - INFO -   |--    Epoch Losses (10): [2.1268, 1.9009, 1.6184, 1.406, 1.3184, 1.2497, 1.1848, 1.1387, 1.1061, 1.1106]\n",
      "2021-09-02 18:58:45,695 - INFO -     |---- Number of Forgettables: 9919 (45.24%)\n",
      "2021-09-02 18:58:45,697 - INFO -     |---- Train Accuracy: 69.42%\n",
      "2021-09-02 18:58:45,698 - INFO -     |---- Test Accuracy: 62.66%\n",
      "2021-09-02 18:58:45,699 - INFO -     |---- Test Loss: 1.0546\n",
      "2021-09-02 18:58:45,701 - INFO -     |---- Elapsed time: 0:05:56.173848\n",
      "2021-09-02 18:58:45,702 - INFO - \n",
      "Test Acc: Max 62.6575% (2 round) | Last 62.6575% | Avg 36.8642% (2 round)\n",
      "\n",
      "2021-09-02 18:58:45,705 - INFO -  | Global Training Round : 3 / 200 |\n",
      "2021-09-02 18:59:07,247 - INFO -   |-- [Party 1] Average Train Loss:   0.9283 Train Accuracy: global  69.38% local  57.71% ...  375 forgettables out of 2828 (13.26%) ... total data used 2828\n",
      "2021-09-02 18:59:07,251 - INFO -   |--    Epoch Losses (10): [1.0658, 1.0081, 0.9388, 0.9341, 0.9453, 0.9774, 0.9022, 0.8781, 0.8219, 0.8114]\n",
      "2021-09-02 18:59:24,000 - INFO -   |-- [Party 2] Average Train Loss:   0.8470 Train Accuracy: global  70.16% local  76.52% ...  236 forgettables out of 2376 ( 9.93%) ... total data used 2376\n",
      "2021-09-02 18:59:24,002 - INFO -   |--    Epoch Losses (10): [1.0566, 0.9478, 0.865, 0.7753, 0.832, 0.8073, 0.7831, 0.8176, 0.786, 0.7987]\n",
      "2021-09-02 18:59:56,148 - INFO -   |-- [Party 3] Average Train Loss:   0.8848 Train Accuracy: global  70.32% local  77.16% ...  611 forgettables out of 4252 (14.37%) ... total data used 4252\n",
      "2021-09-02 18:59:56,149 - INFO -   |--    Epoch Losses (10): [1.0015, 0.8878, 0.8742, 0.8935, 0.862, 0.8394, 0.8662, 0.8787, 0.8938, 0.851]\n",
      "2021-09-02 19:00:07,037 - INFO -   |-- [Party 4] Average Train Loss:   0.8932 Train Accuracy: global  69.37% local  72.11% ...  238 forgettables out of 1567 (15.19%) ... total data used 1567\n",
      "2021-09-02 19:00:07,038 - INFO -   |--    Epoch Losses (10): [1.0488, 1.0368, 0.9059, 0.8801, 0.884, 0.8003, 0.8302, 0.8175, 0.8731, 0.8548]\n",
      "2021-09-02 19:00:25,796 - INFO -   |-- [Party 5] Average Train Loss:   0.8622 Train Accuracy: global  69.08% local  82.00% ...  356 forgettables out of 2516 (14.15%) ... total data used 2516\n",
      "2021-09-02 19:00:25,798 - INFO -   |--    Epoch Losses (10): [1.0382, 0.9222, 0.8192, 0.8343, 0.8602, 0.8358, 0.8388, 0.8286, 0.8153, 0.8297]\n",
      "2021-09-02 19:00:46,613 - INFO -   |-- [Party 6] Average Train Loss:   0.9034 Train Accuracy: global  69.43% local  77.38% ...  322 forgettables out of 2967 (10.85%) ... total data used 2967\n",
      "2021-09-02 19:00:46,614 - INFO -   |--    Epoch Losses (10): [1.073, 0.9392, 0.8746, 0.8798, 0.8987, 0.9265, 0.9052, 0.8484, 0.8299, 0.8586]\n",
      "2021-09-02 19:01:02,324 - INFO -   |-- [Party 7] Average Train Loss:   0.8217 Train Accuracy: global  68.42% local  83.23% ...  175 forgettables out of 1998 ( 8.76%) ... total data used 1998\n",
      "2021-09-02 19:01:02,326 - INFO -   |--    Epoch Losses (10): [1.0878, 0.9362, 0.8374, 0.7823, 0.7456, 0.7773, 0.7707, 0.7524, 0.7512, 0.7757]\n",
      "2021-09-02 19:01:13,946 - INFO -   |-- [Party 8] Average Train Loss:   0.8312 Train Accuracy: global  69.35% local  82.81% ...  234 forgettables out of 1664 (14.06%) ... total data used 1664\n",
      "2021-09-02 19:01:13,947 - INFO -   |--    Epoch Losses (10): [1.0825, 0.9993, 0.8676, 0.8132, 0.7899, 0.774, 0.7666, 0.729, 0.7358, 0.7541]\n",
      "2021-09-02 19:01:26,295 - INFO -   |-- [Party 9] Average Train Loss:   0.8522 Train Accuracy: global  68.74% local  79.50% ...  163 forgettables out of 1756 ( 9.28%) ... total data used 1756\n",
      "2021-09-02 19:01:26,296 - INFO -   |--    Epoch Losses (10): [1.0547, 0.9728, 0.8981, 0.8355, 0.8009, 0.807, 0.7948, 0.7875, 0.7986, 0.7723]\n",
      "2021-09-02 19:01:42,371 - INFO -     |---- Number of Forgettables: 2710 (12.36%)\n",
      "2021-09-02 19:01:42,374 - INFO -     |---- Train Accuracy: 84.98%\n",
      "2021-09-02 19:01:42,374 - INFO -     |---- Test Accuracy: 84.55%\n",
      "2021-09-02 19:01:42,375 - INFO -     |---- Test Loss: 0.4935\n",
      "2021-09-02 19:01:42,376 - INFO -     |---- Elapsed time: 0:08:52.849809\n",
      "2021-09-02 19:01:42,378 - INFO - \n",
      "Test Acc: Max 84.5536% (3 round) | Last 84.5536% | Avg 52.7607% (2 round)\n",
      "\n",
      "2021-09-02 19:01:42,381 - INFO -  | Global Training Round : 4 / 200 |\n",
      "2021-09-02 19:02:03,453 - INFO -   |-- [Party 1] Average Train Loss:   0.8305 Train Accuracy: global  84.90% local  78.57% ...   91 forgettables out of 2828 ( 3.22%) ... total data used 2828\n",
      "2021-09-02 19:02:03,455 - INFO -   |--    Epoch Losses (10): [0.8153, 0.8101, 0.9083, 0.8627, 0.8636, 0.7429, 0.7681, 0.7902, 0.9007, 0.8434]\n",
      "2021-09-02 19:02:21,930 - INFO -   |-- [Party 2] Average Train Loss:   0.6875 Train Accuracy: global  83.92% local  77.19% ...  142 forgettables out of 2376 ( 5.98%) ... total data used 2376\n",
      "2021-09-02 19:02:21,931 - INFO -   |--    Epoch Losses (10): [0.7711, 0.732, 0.7153, 0.671, 0.637, 0.6518, 0.6773, 0.6913, 0.6824, 0.6461]\n"
     ]
    }
   ],
   "source": [
    "rounds = 200\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "wdecay = 0\n",
    "\n",
    "logger.info(f'\\nAlgorithm: FedProx Weighted {standardize}\\nClient Split: 9\\nDataset: {dataset}\\nModel: ResNet9 | Rounds: {rounds} | Epochs: {epochs} | LR: {lr}\\n')\n",
    "\n",
    "train_accs, train_losses, test_accs, test_losses = [], [], [], []\n",
    "match_history, round_forget_history = {}, []\n",
    "forget_cnt_per_client = [0] * len(indices.keys())\n",
    "\n",
    "st = time.time()\n",
    "for r in range(rounds):\n",
    "    \n",
    "    round_forget, round_samples = 0, 0\n",
    "    forget_history, forgettables = {}, {}\n",
    "    local_weights, local_losses = [], []\n",
    "    logger.info(f' | Global Training Round : {r + 1} / {rounds} |')\n",
    "    \n",
    "    for i, k in enumerate(indices.keys()):\n",
    "        match_history, forgettables, global_acc = infer_train(fed_model, inferloaders[k], device, match_history, flag=True)\n",
    "\n",
    "        fed_model.train()\n",
    "        \n",
    "        sampler_idx = indices[k]['index'].copy()\n",
    "        sampler = SubsetRandomSampler(sampler_idx)\n",
    "\n",
    "        trainloader = DataLoader(custom_dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
    "        \n",
    "        local_model = copy.deepcopy(fed_model)\n",
    "        w, ls = train(local_model, trainloader, epochs, lr, wdecay, criterion, device, fed_model, mu=0.01)\n",
    "\n",
    "        local_weights.append(copy.deepcopy(w))\n",
    "        train_losses.append(ls)\n",
    "        \n",
    "        match_history, _, local_acc = infer_train(local_model, inferloaders[k], device, match_history, flag=False)\n",
    "        train_accs.append(local_acc)\n",
    "        \n",
    "        forget_cnt = len(forgettables)\n",
    "        round_forget += forget_cnt\n",
    "        forget_cnt_per_client[int(k)] = forget_cnt\n",
    "        round_samples += len(indices[k]['index'])\n",
    "        \n",
    "        logger.info('  |-- [Party {:>1}] Average Train Loss: {:>8.4f} Train Accuracy: global {:>6.2f}% local {:>6.2f}% ... {:>4} forgettables out of {:>4} ({:>5.2f}%) ... total data used {:>4}'.format(\n",
    "            i + 1, sum(ls) / len(ls), 100 * global_acc, 100 * local_acc, forget_cnt, len(indices[k]['index']), 100 * forget_cnt / len(indices[k]['index']), len(sampler_idx)\n",
    "        ))\n",
    "        logger.info('  |--    Epoch Losses ({:>2}): {}'.format(epochs, formatfloats(ls)))\n",
    "        \n",
    "    standardized_weights = weighting(forget_cnt_per_client, standardize)\n",
    "    fed_weights = average_weights(local_weights, standardized_weights)\n",
    "    fed_model.load_state_dict(fed_weights)\n",
    "    train_acc, _ = inference(fed_model, fed_trainloader, criterion, device)\n",
    "    \n",
    "    if (r + 1) % 50 == 0:\n",
    "        torch.save(fed_model.state_dict(), os.path.join(path_models, filename + f'_round{r+1}.pth'))\n",
    "    \n",
    "    test_acc, test_ls = inference(fed_model, testloader, criterion, device)\n",
    "    test_accs.append(test_acc)\n",
    "    test_losses.append(test_ls)\n",
    "    round_forget_history.append(round_forget)\n",
    "    logger.info('    |---- Number of Forgettables: {} ({:.2f}%)'.format(round_forget, 100 * round_forget / round_samples))\n",
    "    logger.info('    |---- Train Accuracy: {:>.2f}%'.format(100 * train_acc))\n",
    "    logger.info('    |---- Test Accuracy: {:>.2f}%'.format(100 * test_acc))\n",
    "    logger.info('    |---- Test Loss: {:.4f}'.format(test_ls))\n",
    "    logger.info('    |---- Elapsed time: {}'.format(timedelta(seconds=time.time()-st)))\n",
    "    logger.info(f'\\nTest Acc: Max {np.max(test_acc) * 100:.4f}% ({np.argmax(test_accs)+1} round) | Last {test_accs[-1] * 100:.4f}% | Avg {np.mean(test_accs) * 100:.4f}% ({np.argmax(test_accs > np.mean(test_accs))+1} round)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654dbc03-4dd8-4c75-932e-f9f842c9ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = np.asarray(train_losses)\n",
    "train_accs = np.asarray(train_accs)\n",
    "\n",
    "with open(os.path.join(path_results, f'{filename}_tr_ls.npy'), 'wb') as f:\n",
    "    np.save(f, train_losses)\n",
    "with open(os.path.join(path_results, f'{filename}_tr_acc.npy'), 'wb') as f:\n",
    "    np.save(f, train_accs)\n",
    "with open(os.path.join(path_results, f'{filename}_te_ls.npy'), 'wb') as f:\n",
    "    np.save(f, test_losses)\n",
    "with open(os.path.join(path_results, f'{filename}_te_acc.npy'), 'wb') as f:\n",
    "    np.save(f, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e644234-224e-4353-888c-01affc417a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(50, 30))\n",
    "axs = axs.ravel()\n",
    "\n",
    "axs[0].plot(test_accs, c='orange')\n",
    "axs[0].set_title('Test Accuracies')\n",
    "axs[0].set_xlabel('Rounds')\n",
    "axs[0].set_ylabel('Test Accuracy')\n",
    "axs[1].plot(test_losses, c='blue')\n",
    "axs[1].set_title('Test Losses')\n",
    "axs[1].set_xlabel('Rounds')\n",
    "axs[1].set_ylabel('Test Loss')\n",
    "axs[2].plot(train_accs, c='red')\n",
    "axs[2].set_title('Train Average Accuracies by Epochs')\n",
    "axs[2].set_xlabel('Epochs')\n",
    "axs[2].set_ylabel('Train Average Accuracy')\n",
    "axs[3].plot(train_losses.mean(axis=1), c='turquoise')\n",
    "axs[3].set_title('Train Average Losses by Epochs')\n",
    "axs[3].set_xlabel('Epochs')\n",
    "axs[3].set_ylabel('Train Average Loss')\n",
    "axs[4].plot(np.mean(train_accs.reshape(-1, 10), axis=1), c='green')\n",
    "axs[4].set_title('Train Average Accuracies by Rounds')\n",
    "axs[4].set_xlabel('Rounds')\n",
    "axs[4].set_ylabel('Train Average Accuracy')\n",
    "axs[5].plot(train_losses.mean(axis=1).reshape(-1, 10).mean(axis=1), c='lightpink')\n",
    "axs[5].set_title('Train Average Losses by Rounds')\n",
    "axs[5].set_xlabel('Rounds')\n",
    "axs[5].set_ylabel('Train Average Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9a5cc-41d2-41f1-a4cd-114d63d3b2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

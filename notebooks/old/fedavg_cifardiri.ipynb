{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35825f4-b26d-4ee5-8914-d28fdc8bc1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "dataset = 'cifar10'\n",
    "\n",
    "filename = f'{dataset}_diri9a001_42_fedavg'\n",
    "\n",
    "gpu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8447bc5c-393a-4f8d-80fd-50a652a7a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, Subset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "148a5cb1-99a9-4d54-8239-3ae733854155",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    torch.use_deterministic_algorithms(False)\n",
    "except AttributeError:\n",
    "    torch.set_deterministic(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5090116-fe54-40fe-8459-e9ce9aea270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = os.path.dirname(os.getcwd())\n",
    "path_data = os.path.join(path_root, 'data')\n",
    "path_logs = os.path.join(path_root, 'logs')\n",
    "path_models = os.path.join(path_root, 'models', filename)\n",
    "path_results = os.path.join(path_root, 'results', filename)\n",
    "\n",
    "for p in [path_data, path_logs, path_models, path_results]:\n",
    "    os.makedirs(p, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e4f24c-2bab-456f-9897-fb7cf4a72efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(filename)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "streamformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "streamhandler = logging.StreamHandler()\n",
    "streamhandler.setFormatter(streamformatter)\n",
    "logger.addHandler(streamhandler)\n",
    "\n",
    "fileformatter = logging.Formatter('%(message)s')\n",
    "filehandler = logging.FileHandler(os.path.join(path_logs, filename + '.log'), mode='w')\n",
    "filehandler.setFormatter(fileformatter)\n",
    "logger.addHandler(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4543504-728a-4bbf-8ebd-a70f223288d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, target = self.dataset[idx]\n",
    "        return data, target, idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0859e906-dbe3-4c6f-a44d-3ce50555d228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'cifar10':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([x / 255 for x in [125.3, 123, 113.9]], [x / 255 for x in [63, 62.1, 66.7]])\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([x / 255 for x in [125.3, 123, 113.9]], [x / 255 for x in [63, 62.1, 66.7]])\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.CIFAR10(path_data, train=False, transform=test_transform)\n",
    "    \n",
    "elif dataset == 'svhn':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.SVHN(path_data, split='train', transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.SVHN(path_data, split='test', transform=test_transform, download=True)\n",
    "    \n",
    "elif dataset == 'fmnist':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.FashionMNIST(path_data, train=False, transform=test_transform)\n",
    "\n",
    "elif dataset == 'mnist':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.MNIST(path_data, train=False, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6182a3f4-472d-4246-9ae0-e184be956dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_data, f'{dataset}_diri9a001_42.json')) as f:\n",
    "    indices = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02409155-7c8a-4217-8258-ccb1a2c7e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_workers = 128, 0\n",
    "\n",
    "inferloaders, subset_indices = {}, []\n",
    "for k, v in indices.items():\n",
    "    infersubset = Subset(custom_dataset, v['index'])\n",
    "    inferloaders[k] = DataLoader(infersubset, batch_size=batch_size, num_workers=num_workers)\n",
    "    subset_indices.extend(v['index'])\n",
    "\n",
    "train_subset = Subset(train_dataset, indices=subset_indices)\n",
    "fed_trainloader = DataLoader(train_subset, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "try:\n",
    "    train_labels = np.asarray(custom_dataset.dataset.targets)\n",
    "    test_labels = test_dataset.targets\n",
    "except AttributeError:\n",
    "    train_labels = np.asarray(custom_dataset.dataset.labels)\n",
    "    test_labels = test_dataset.labels\n",
    "subset_classes = np.unique(train_labels[subset_indices])\n",
    "boolarr = [True if y in subset_classes else False for y in test_labels]\n",
    "subset_indices = np.arange(len(test_dataset))[boolarr]\n",
    "test_subset = Subset(test_dataset, indices=subset_indices)\n",
    "testloader = DataLoader(test_subset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37423e2e-709a-4a3c-b0de-99acc3e83e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, padding=1, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=kernel_size, stride=1, padding=padding, bias=bias)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if (stride != 1) or (in_channel != self.expansion * out_channel):\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, self.expansion * out_channel, kernel_size=1, stride=stride, bias=bias),\n",
    "                nn.BatchNorm2d(self.expansion * out_channel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dd80881-9cee-4d1c-8348-aaf4eacc8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, padding=1, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=1, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv3 = nn.Conv2d(out_channel, self.expansion * out_channel, kerenel_size=1, bias=bias)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * out_channel)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if (stride != 1) or (in_channel != self.expansion * out_channel):\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, self.expansion * out_channel, kernel_size=1, stride=stride, bias=bias),\n",
    "                nn.BatchNorm2d(self.expansion * out_channel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eede0030-1cdc-4185-a7b3-825076d49102",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, in_channel=3, out_channels=[64, 128, 256, 512], num_blocks=[2, 2, 2, 2], strides=[1, 2, 2, 2], num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_channel = out_channels[0]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channels[0], kernel_size=3, stride=strides[0], padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels[0])\n",
    "        \n",
    "        self.block1 = self._make_layer(block, out_channels[0], num_blocks[0], strides[0])\n",
    "        self.block2 = self._make_layer(block, out_channels[1], num_blocks[1], strides[1])\n",
    "        self.block3 = self._make_layer(block, out_channels[2], num_blocks[2], strides[2])\n",
    "        self.block4 = self._make_layer(block, out_channels[3], num_blocks[3], strides[3])\n",
    "        \n",
    "        self.linear = nn.Linear(out_channels[-1] * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channel, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channel, out_channel, stride=stride))\n",
    "            self.in_channel = out_channel * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae0fe2f3-5ec9-46fe-b29c-8db8b81089ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(w):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "        \n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], float(len(w)))\n",
    "        \n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "611e3c0f-9f0e-419a-aa1f-a497b671aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, epochs, lr, weight_decay, criterion, device):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        batch_losses = []\n",
    "        \n",
    "        for inputs, labels, _ in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss.mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "        epoch_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "        \n",
    "    local_weights = model.state_dict()\n",
    "\n",
    "    return local_weights, epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d5c54db-1ba4-4f65-817d-fb8b2b28fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, loader, criterion, device):\n",
    "    avg_loss, correct, num_samples = 0, 0, 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss.mean()\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += torch.sum(torch.eq(predicted, labels)).item()\n",
    "            num_samples += len(labels)\n",
    "\n",
    "    acc = correct / num_samples\n",
    "    avg_loss /= len(loader)\n",
    "    \n",
    "    return acc, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "629ff8ea-9394-4a48-a5e0-1a90d329a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_train(model, loader, device, match_history, flag=False):\n",
    "    forgettables = []\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, indices in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            match = predicted.eq(labels)\n",
    "            match = match.type(torch.IntTensor)\n",
    "            total += len(indices)\n",
    "            \n",
    "            for j, idx in enumerate(indices):\n",
    "                sample_history = match_history.get(idx.item(), [])\n",
    "                sample_history.append(match[j].item())\n",
    "                match_history[idx.item()] = sample_history\n",
    "                correct += match[j].item()\n",
    "                if flag is True:\n",
    "                    try:\n",
    "                        if match[j].item() - sample_history[-2] == -1:\n",
    "                            forgettables.append(idx.item())\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "            \n",
    "    acc = correct / total\n",
    "            \n",
    "    return match_history, forgettables, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7562c7c7-4368-442b-8bd7-fa1674a6842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatfloats(li):\n",
    "    new = [float(f'{e:>8.4f}') for e in li]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc4a3691-7135-496c-9153-23e7fc4f1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet9 = {'block': ResidualBlock, 'num_blocks': [1, 1, 1, 1]}\n",
    "resnet18 = {'block': ResidualBlock, 'num_blocks': [2, 2, 2, 2]}\n",
    "resnet34 = {'block': ResidualBlock, 'num_blocks': [3, 4, 6, 3]}\n",
    "resnet50 = {'block': Bottleneck, 'num_blocks': [3, 4, 6, 3]}\n",
    "resnet101 = {'block': Bottleneck, 'num_blocks': [3, 4, 23, 3]}\n",
    "resnet152 = {'block': Bottleneck, 'num_blocks': [3, 8, 36, 3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1261d399-3f2a-4f98-b585-81e5037a36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f'cuda:{gpu}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20a73dfb-ec2c-40fd-9893-3eac24fbca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = resnet9.copy()\n",
    "\n",
    "out_channels = [64, 128, 256, 512]\n",
    "strides = [1, 2, 2, 2]\n",
    "in_channel = 3\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "fed_model = ResNet(resnet['block'], in_channel=in_channel, out_channels=out_channels, num_blocks=resnet['num_blocks'], strides=strides, num_classes=num_classes)\n",
    "fed_weights = fed_model.state_dict()\n",
    "\n",
    "fed_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d0203-4dd4-4c5b-9f57-5693890ec7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-02 18:39:11,683 - INFO - \n",
      "Algorithm: FedAvg\n",
      "Client Split: 9\n",
      "Dataset: cifar10\n",
      "Model: ResNet9 | Rounds: 200 | Epochs: 10 | LR: 0.001\n",
      "\n",
      "2021-09-02 18:39:11,688 - INFO -  | Global Training Round : 1 / 200 |\n",
      "2021-09-02 18:39:25,279 - INFO -   |-- [Party 1] Average Train Loss:   1.5463 Train Accuracy: global  10.39% local  55.09% ...    0 forgettables out of 1819 ( 0.00%) ... total data used 1819\n",
      "2021-09-02 18:39:25,283 - INFO -   |--    Epoch Losses (10): [2.1034, 1.7895, 1.6767, 1.5865, 1.5006, 1.4801, 1.4108, 1.3858, 1.2938, 1.2363]\n",
      "2021-09-02 18:39:32,434 - INFO -   |-- [Party 2] Average Train Loss:   1.6066 Train Accuracy: global  10.18% local  48.57% ...    0 forgettables out of 1081 ( 0.00%) ... total data used 1081\n",
      "2021-09-02 18:39:32,439 - INFO -   |--    Epoch Losses (10): [2.1238, 1.8725, 1.744, 1.6233, 1.5814, 1.5274, 1.4677, 1.4534, 1.3734, 1.2994]\n",
      "2021-09-02 18:39:50,817 - INFO -   |-- [Party 3] Average Train Loss:   1.4923 Train Accuracy: global   9.51% local  54.71% ...    0 forgettables out of 2630 ( 0.00%) ... total data used 2630\n",
      "2021-09-02 18:39:50,821 - INFO -   |--    Epoch Losses (10): [2.0148, 1.7504, 1.6223, 1.5413, 1.4722, 1.4182, 1.368, 1.3015, 1.2461, 1.1882]\n",
      "2021-09-02 18:39:56,794 - INFO -   |-- [Party 4] Average Train Loss:   1.6989 Train Accuracy: global  10.49% local  44.04% ...    0 forgettables out of  915 ( 0.00%) ... total data used  915\n",
      "2021-09-02 18:39:56,795 - INFO -   |--    Epoch Losses (10): [2.1564, 1.9064, 1.768, 1.6992, 1.6789, 1.6126, 1.5941, 1.5605, 1.5184, 1.4947]\n",
      "2021-09-02 18:40:08,416 - INFO -   |-- [Party 5] Average Train Loss:   1.7087 Train Accuracy: global   9.71% local  45.20% ...    0 forgettables out of 1668 ( 0.00%) ... total data used 1668\n",
      "2021-09-02 18:40:08,419 - INFO -   |--    Epoch Losses (10): [2.1052, 1.8072, 1.7534, 1.7793, 1.7459, 1.645, 1.5802, 1.6124, 1.5162, 1.542]\n",
      "2021-09-02 18:40:18,598 - INFO -   |-- [Party 6] Average Train Loss:   1.5978 Train Accuracy: global   9.15% local  53.13% ...    0 forgettables out of 1596 ( 0.00%) ... total data used 1596\n",
      "2021-09-02 18:40:18,600 - INFO -   |--    Epoch Losses (10): [2.1223, 1.8317, 1.7252, 1.6344, 1.5747, 1.5484, 1.485, 1.4231, 1.3588, 1.2745]\n",
      "2021-09-02 18:40:32,290 - INFO -   |-- [Party 7] Average Train Loss:   1.4943 Train Accuracy: global  10.18% local  59.74% ...    0 forgettables out of 2131 ( 0.00%) ... total data used 2131\n",
      "2021-09-02 18:40:32,292 - INFO -   |--    Epoch Losses (10): [2.0854, 1.7516, 1.6451, 1.5565, 1.4581, 1.3982, 1.3481, 1.2695, 1.2336, 1.1971]\n",
      "2021-09-02 18:40:39,112 - INFO -   |-- [Party 8] Average Train Loss:   1.6191 Train Accuracy: global   9.41% local  53.97% ...    0 forgettables out of 1084 ( 0.00%) ... total data used 1084\n",
      "2021-09-02 18:40:39,113 - INFO -   |--    Epoch Losses (10): [2.212, 1.8445, 1.727, 1.6142, 1.592, 1.5581, 1.4818, 1.4314, 1.3618, 1.3679]\n",
      "2021-09-02 18:40:54,289 - INFO -   |-- [Party 9] Average Train Loss:   1.4454 Train Accuracy: global   9.00% local  59.64% ...    0 forgettables out of 2034 ( 0.00%) ... total data used 2034\n",
      "2021-09-02 18:40:54,290 - INFO -   |--    Epoch Losses (10): [1.973, 1.6901, 1.6073, 1.5089, 1.434, 1.3772, 1.2902, 1.2377, 1.1831, 1.1521]\n",
      "2021-09-02 18:41:02,573 - INFO -     |---- Number of Forgettables: 0 (0.00%)\n",
      "2021-09-02 18:41:02,577 - INFO -     |---- Train Accuracy: 25.93%\n",
      "2021-09-02 18:41:02,578 - INFO -     |---- Test Accuracy: 26.87%\n",
      "2021-09-02 18:41:02,579 - INFO -     |---- Test Loss: 1.9916\n",
      "2021-09-02 18:41:02,580 - INFO -     |---- Elapsed time: 0:01:50.893237\n",
      "2021-09-02 18:41:02,582 - INFO - \n",
      "Test Acc: Max 26.8700% (1 round) | Last 26.8700% | Avg 26.8700% (1 round)\n",
      "\n",
      "2021-09-02 18:41:02,584 - INFO -  | Global Training Round : 2 / 200 |\n",
      "2021-09-02 18:41:15,371 - INFO -   |-- [Party 1] Average Train Loss:   1.3181 Train Accuracy: global  27.71% local  58.82% ...  700 forgettables out of 1819 (38.48%) ... total data used 1819\n",
      "2021-09-02 18:41:15,373 - INFO -   |--    Epoch Losses (10): [1.7779, 1.511, 1.481, 1.3876, 1.3237, 1.2336, 1.1672, 1.1475, 1.101, 1.0508]\n",
      "2021-09-02 18:41:22,160 - INFO -   |-- [Party 2] Average Train Loss:   1.3476 Train Accuracy: global  27.66% local  57.91% ...  343 forgettables out of 1081 (31.73%) ... total data used 1081\n",
      "2021-09-02 18:41:22,161 - INFO -   |--    Epoch Losses (10): [1.8472, 1.5846, 1.5166, 1.4033, 1.3159, 1.2482, 1.1633, 1.1839, 1.1167, 1.0965]\n",
      "2021-09-02 18:41:38,545 - INFO -   |-- [Party 3] Average Train Loss:   1.2581 Train Accuracy: global  25.29% local  62.28% ... 1050 forgettables out of 2630 (39.92%) ... total data used 2630\n",
      "2021-09-02 18:41:38,549 - INFO -   |--    Epoch Losses (10): [1.7161, 1.5039, 1.4013, 1.2907, 1.2107, 1.1678, 1.1051, 1.1126, 1.0461, 1.0263]\n",
      "2021-09-02 18:41:44,400 - INFO -   |-- [Party 4] Average Train Loss:   1.4352 Train Accuracy: global  26.12% local  39.45% ...  273 forgettables out of  915 (29.84%) ... total data used  915\n",
      "2021-09-02 18:41:44,401 - INFO -   |--    Epoch Losses (10): [1.7714, 1.6828, 1.5168, 1.4947, 1.4324, 1.3986, 1.3065, 1.3402, 1.2331, 1.1757]\n",
      "2021-09-02 18:41:54,974 - INFO -   |-- [Party 5] Average Train Loss:   1.4127 Train Accuracy: global  23.68% local  47.42% ...  512 forgettables out of 1668 (30.70%) ... total data used 1668\n",
      "2021-09-02 18:41:54,975 - INFO -   |--    Epoch Losses (10): [1.6602, 1.6884, 1.5083, 1.414, 1.4788, 1.383, 1.3157, 1.247, 1.2511, 1.1808]\n",
      "2021-09-02 18:42:04,922 - INFO -   |-- [Party 6] Average Train Loss:   1.3171 Train Accuracy: global  24.12% local  57.83% ...  610 forgettables out of 1596 (38.22%) ... total data used 1596\n",
      "2021-09-02 18:42:04,924 - INFO -   |--    Epoch Losses (10): [1.7766, 1.543, 1.4604, 1.3696, 1.3123, 1.246, 1.1679, 1.1402, 1.0792, 1.0755]\n",
      "2021-09-02 18:42:18,226 - INFO -   |-- [Party 7] Average Train Loss:   1.2915 Train Accuracy: global  26.98% local  62.93% ...  884 forgettables out of 2131 (41.48%) ... total data used 2131\n",
      "2021-09-02 18:42:18,227 - INFO -   |--    Epoch Losses (10): [1.8112, 1.5128, 1.413, 1.3395, 1.2783, 1.2241, 1.166, 1.0964, 1.0681, 1.0058]\n",
      "2021-09-02 18:42:25,062 - INFO -   |-- [Party 8] Average Train Loss:   1.2882 Train Accuracy: global  25.74% local  60.61% ...  396 forgettables out of 1084 (36.53%) ... total data used 1084\n",
      "2021-09-02 18:42:25,064 - INFO -   |--    Epoch Losses (10): [1.7412, 1.5356, 1.4103, 1.3679, 1.2983, 1.1938, 1.1653, 1.0753, 1.0487, 1.0458]\n",
      "2021-09-02 18:42:37,619 - INFO -   |-- [Party 9] Average Train Loss:   1.2323 Train Accuracy: global  26.70% local  57.77% ...  840 forgettables out of 2034 (41.30%) ... total data used 2034\n",
      "2021-09-02 18:42:37,620 - INFO -   |--    Epoch Losses (10): [1.7051, 1.4754, 1.3712, 1.2877, 1.2061, 1.1287, 1.0734, 1.0635, 1.0322, 0.9795]\n",
      "2021-09-02 18:42:45,735 - INFO -     |---- Number of Forgettables: 5608 (37.49%)\n",
      "2021-09-02 18:42:45,737 - INFO -     |---- Train Accuracy: 53.82%\n",
      "2021-09-02 18:42:45,737 - INFO -     |---- Test Accuracy: 53.36%\n",
      "2021-09-02 18:42:45,738 - INFO -     |---- Test Loss: 1.2937\n",
      "2021-09-02 18:42:45,739 - INFO -     |---- Elapsed time: 0:03:34.052198\n",
      "2021-09-02 18:42:45,740 - INFO - \n",
      "Test Acc: Max 53.3600% (2 round) | Last 53.3600% | Avg 40.1150% (2 round)\n",
      "\n",
      "2021-09-02 18:42:45,742 - INFO -  | Global Training Round : 3 / 200 |\n",
      "2021-09-02 18:42:57,126 - INFO -   |-- [Party 1] Average Train Loss:   1.0247 Train Accuracy: global  53.11% local  73.01% ...  358 forgettables out of 1819 (19.68%) ... total data used 1819\n",
      "2021-09-02 18:42:57,129 - INFO -   |--    Epoch Losses (10): [1.3733, 1.2393, 1.1414, 1.0714, 1.0178, 0.9371, 0.8997, 0.8885, 0.8526, 0.8262]\n",
      "2021-09-02 18:43:03,896 - INFO -   |-- [Party 2] Average Train Loss:   1.0234 Train Accuracy: global  52.45% local  75.39% ...  198 forgettables out of 1081 (18.32%) ... total data used 1081\n",
      "2021-09-02 18:43:03,897 - INFO -   |--    Epoch Losses (10): [1.3454, 1.3066, 1.1908, 1.1049, 1.0181, 0.9282, 0.8881, 0.8423, 0.8325, 0.7769]\n",
      "2021-09-02 18:43:20,472 - INFO -   |-- [Party 3] Average Train Loss:   1.0096 Train Accuracy: global  53.04% local  70.87% ...  530 forgettables out of 2630 (20.15%) ... total data used 2630\n",
      "2021-09-02 18:43:20,474 - INFO -   |--    Epoch Losses (10): [1.3198, 1.179, 1.1112, 1.065, 1.0043, 1.0026, 0.9051, 0.8719, 0.8163, 0.8205]\n",
      "2021-09-02 18:43:26,322 - INFO -   |-- [Party 4] Average Train Loss:   1.0536 Train Accuracy: global  51.04% local  72.68% ...  155 forgettables out of  915 (16.94%) ... total data used  915\n",
      "2021-09-02 18:43:26,323 - INFO -   |--    Epoch Losses (10): [1.3895, 1.2326, 1.1851, 1.1128, 1.0221, 0.9554, 0.9779, 0.9004, 0.8904, 0.8696]\n",
      "2021-09-02 18:43:36,902 - INFO -   |-- [Party 5] Average Train Loss:   1.1769 Train Accuracy: global  54.08% local  61.87% ...  194 forgettables out of 1668 (11.63%) ... total data used 1668\n",
      "2021-09-02 18:43:36,904 - INFO -   |--    Epoch Losses (10): [1.3658, 1.2984, 1.1714, 1.1529, 1.1708, 1.2058, 1.1124, 1.1087, 1.0802, 1.1024]\n",
      "2021-09-02 18:43:46,914 - INFO -   |-- [Party 6] Average Train Loss:   1.0404 Train Accuracy: global  52.63% local  64.54% ...  277 forgettables out of 1596 (17.36%) ... total data used 1596\n",
      "2021-09-02 18:43:46,918 - INFO -   |--    Epoch Losses (10): [1.3968, 1.2697, 1.1848, 1.0981, 1.0102, 0.978, 0.9125, 0.8636, 0.8334, 0.8565]\n",
      "2021-09-02 18:44:00,342 - INFO -   |-- [Party 7] Average Train Loss:   0.9951 Train Accuracy: global  55.00% local  68.04% ...  373 forgettables out of 2131 (17.50%) ... total data used 2131\n",
      "2021-09-02 18:44:00,343 - INFO -   |--    Epoch Losses (10): [1.3514, 1.1747, 1.0764, 1.039, 1.0246, 0.9437, 0.8741, 0.8476, 0.8229, 0.7968]\n",
      "2021-09-02 18:44:07,178 - INFO -   |-- [Party 8] Average Train Loss:   0.9817 Train Accuracy: global  53.41% local  73.43% ...  214 forgettables out of 1084 (19.74%) ... total data used 1084\n",
      "2021-09-02 18:44:07,180 - INFO -   |--    Epoch Losses (10): [1.4135, 1.2053, 1.0816, 1.0082, 0.975, 0.9179, 0.8717, 0.8452, 0.7937, 0.7051]\n",
      "2021-09-02 18:44:19,848 - INFO -   |-- [Party 9] Average Train Loss:   0.9239 Train Accuracy: global  55.65% local  73.25% ...  364 forgettables out of 2034 (17.90%) ... total data used 2034\n",
      "2021-09-02 18:44:19,850 - INFO -   |--    Epoch Losses (10): [1.2941, 1.0841, 1.037, 0.9798, 0.9395, 0.8425, 0.8185, 0.8152, 0.7263, 0.7024]\n",
      "2021-09-02 18:44:27,973 - INFO -     |---- Number of Forgettables: 2663 (17.80%)\n",
      "2021-09-02 18:44:27,974 - INFO -     |---- Train Accuracy: 66.35%\n",
      "2021-09-02 18:44:27,975 - INFO -     |---- Test Accuracy: 65.10%\n",
      "2021-09-02 18:44:27,976 - INFO -     |---- Test Loss: 1.0174\n",
      "2021-09-02 18:44:27,976 - INFO -     |---- Elapsed time: 0:05:16.289651\n",
      "2021-09-02 18:44:27,977 - INFO - \n",
      "Test Acc: Max 65.1000% (3 round) | Last 65.1000% | Avg 48.4433% (2 round)\n",
      "\n",
      "2021-09-02 18:44:27,979 - INFO -  | Global Training Round : 4 / 200 |\n",
      "2021-09-02 18:44:39,445 - INFO -   |-- [Party 1] Average Train Loss:   0.8327 Train Accuracy: global  65.86% local  65.26% ...  300 forgettables out of 1819 (16.49%) ... total data used 1819\n",
      "2021-09-02 18:44:39,446 - INFO -   |--    Epoch Losses (10): [1.1467, 1.0236, 0.9584, 0.8555, 0.8082, 0.7951, 0.7414, 0.7082, 0.6676, 0.6218]\n",
      "2021-09-02 18:44:46,213 - INFO -   |-- [Party 2] Average Train Loss:   0.7605 Train Accuracy: global  65.96% local  79.28% ...  172 forgettables out of 1081 (15.91%) ... total data used 1081\n",
      "2021-09-02 18:44:46,214 - INFO -   |--    Epoch Losses (10): [1.1506, 0.9657, 0.8782, 0.7603, 0.7927, 0.7059, 0.6437, 0.6237, 0.5639, 0.52]\n"
     ]
    }
   ],
   "source": [
    "rounds = 200\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "wdecay = 0\n",
    "\n",
    "logger.info(f'\\nAlgorithm: FedAvg\\nClient Split: 9\\nDataset: {dataset}\\nModel: ResNet9 | Rounds: {rounds} | Epochs: {epochs} | LR: {lr}\\n')\n",
    "\n",
    "train_accs, train_losses, test_accs, test_losses = [], [], [], []\n",
    "match_history, round_forget_history = {}, []\n",
    "forget_cnt_per_client = [0] * len(indices.keys())\n",
    "\n",
    "st = time.time()\n",
    "for r in range(rounds):\n",
    "    \n",
    "    round_forget, round_samples = 0, 0\n",
    "    forget_history, forgettables = {}, {}\n",
    "    local_weights, local_losses = [], []\n",
    "    logger.info(f' | Global Training Round : {r + 1} / {rounds} |')\n",
    "    \n",
    "    for i, k in enumerate(indices.keys()):\n",
    "        match_history, forgettables, global_acc = infer_train(fed_model, inferloaders[k], device, match_history, flag=True)\n",
    "\n",
    "        fed_model.train()\n",
    "        \n",
    "        sampler_idx = indices[k]['index'].copy()\n",
    "        sampler = SubsetRandomSampler(sampler_idx)\n",
    "\n",
    "        trainloader = DataLoader(custom_dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
    "        \n",
    "        local_model = copy.deepcopy(fed_model)\n",
    "        w, ls = train(local_model, trainloader, epochs, lr, wdecay, criterion, device)\n",
    "\n",
    "        local_weights.append(copy.deepcopy(w))\n",
    "        train_losses.append(ls)\n",
    "        \n",
    "        match_history, _, local_acc = infer_train(local_model, inferloaders[k], device, match_history, flag=False)\n",
    "        train_accs.append(local_acc)\n",
    "        \n",
    "        forget_cnt = len(forgettables)\n",
    "        round_forget += forget_cnt\n",
    "        forget_cnt_per_client[int(k)] = forget_cnt\n",
    "        round_samples += len(indices[k]['index'])\n",
    "        \n",
    "        logger.info('  |-- [Party {:>1}] Average Train Loss: {:>8.4f} Train Accuracy: global {:>6.2f}% local {:>6.2f}% ... {:>4} forgettables out of {:>4} ({:>5.2f}%) ... total data used {:>4}'.format(\n",
    "            i + 1, sum(ls) / len(ls), 100 * global_acc, 100 * local_acc, forget_cnt, len(indices[k]['index']), 100 * forget_cnt / len(indices[k]['index']), len(sampler_idx)\n",
    "        ))\n",
    "        logger.info('  |--    Epoch Losses ({:>2}): {}'.format(epochs, formatfloats(ls)))\n",
    "        \n",
    "    fed_weights = average_weights(local_weights)\n",
    "    fed_model.load_state_dict(fed_weights)\n",
    "    train_acc, _ = inference(fed_model, fed_trainloader, criterion, device)\n",
    "    \n",
    "    if (r + 1) % 50 == 0:\n",
    "        torch.save(fed_model.state_dict(), os.path.join(path_models, filename + f'_round{r+1}.pth'))\n",
    "    \n",
    "    test_acc, test_ls = inference(fed_model, testloader, criterion, device)\n",
    "    test_accs.append(test_acc)\n",
    "    test_losses.append(test_ls)\n",
    "    round_forget_history.append(round_forget)\n",
    "    logger.info('    |---- Number of Forgettables: {} ({:.2f}%)'.format(round_forget, 100 * round_forget / round_samples))\n",
    "    logger.info('    |---- Train Accuracy: {:>.2f}%'.format(100 * train_acc))\n",
    "    logger.info('    |---- Test Accuracy: {:>.2f}%'.format(100 * test_acc))\n",
    "    logger.info('    |---- Test Loss: {:.4f}'.format(test_ls))\n",
    "    logger.info('    |---- Elapsed time: {}'.format(timedelta(seconds=time.time()-st)))\n",
    "    logger.info(f'\\nTest Acc: Max {np.max(test_acc) * 100:.4f}% ({np.argmax(test_accs)+1} round) | Last {test_accs[-1] * 100:.4f}% | Avg {np.mean(test_accs) * 100:.4f}% ({np.argmax(test_accs > np.mean(test_accs))+1} round)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654dbc03-4dd8-4c75-932e-f9f842c9ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = np.asarray(train_losses)\n",
    "train_accs = np.asarray(train_accs)\n",
    "\n",
    "with open(os.path.join(path_results, f'{filename}_tr_ls.npy'), 'wb') as f:\n",
    "    np.save(f, train_losses)\n",
    "with open(os.path.join(path_results, f'{filename}_tr_acc.npy'), 'wb') as f:\n",
    "    np.save(f, train_accs)\n",
    "with open(os.path.join(path_results, f'{filename}_te_ls.npy'), 'wb') as f:\n",
    "    np.save(f, test_losses)\n",
    "with open(os.path.join(path_results, f'{filename}_te_acc.npy'), 'wb') as f:\n",
    "    np.save(f, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e644234-224e-4353-888c-01affc417a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(50, 30))\n",
    "axs = axs.ravel()\n",
    "\n",
    "axs[0].plot(test_accs, c='orange')\n",
    "axs[0].set_title('Test Accuracies')\n",
    "axs[0].set_xlabel('Rounds')\n",
    "axs[0].set_ylabel('Test Accuracy')\n",
    "axs[1].plot(test_losses, c='blue')\n",
    "axs[1].set_title('Test Losses')\n",
    "axs[1].set_xlabel('Rounds')\n",
    "axs[1].set_ylabel('Test Loss')\n",
    "axs[2].plot(train_accs, c='red')\n",
    "axs[2].set_title('Train Average Accuracies by Epochs')\n",
    "axs[2].set_xlabel('Epochs')\n",
    "axs[2].set_ylabel('Train Average Accuracy')\n",
    "axs[3].plot(train_losses.mean(axis=1), c='turquoise')\n",
    "axs[3].set_title('Train Average Losses by Epochs')\n",
    "axs[3].set_xlabel('Epochs')\n",
    "axs[3].set_ylabel('Train Average Loss')\n",
    "axs[4].plot(np.mean(train_accs.reshape(-1, 10), axis=1), c='green')\n",
    "axs[4].set_title('Train Average Accuracies by Rounds')\n",
    "axs[4].set_xlabel('Rounds')\n",
    "axs[4].set_ylabel('Train Average Accuracy')\n",
    "axs[5].plot(train_losses.mean(axis=1).reshape(-1, 10).mean(axis=1), c='lightpink')\n",
    "axs[5].set_title('Train Average Losses by Rounds')\n",
    "axs[5].set_xlabel('Rounds')\n",
    "axs[5].set_ylabel('Train Average Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9a5cc-41d2-41f1-a4cd-114d63d3b2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

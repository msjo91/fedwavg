{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35825f4-b26d-4ee5-8914-d28fdc8bc1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "dataset = 'cifar10'\n",
    "\n",
    "standardize = 0.1\n",
    "\n",
    "filename = '{}_diri9a001_42_fedprox_w{}'.format(dataset, ''.join(e for e in str(standardize) if e.isalnum()))\n",
    "\n",
    "gpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8447bc5c-393a-4f8d-80fd-50a652a7a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, Subset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "148a5cb1-99a9-4d54-8239-3ae733854155",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    torch.use_deterministic_algorithms(False)\n",
    "except AttributeError:\n",
    "    torch.set_deterministic(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5090116-fe54-40fe-8459-e9ce9aea270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = os.path.dirname(os.getcwd())\n",
    "path_data = os.path.join(path_root, 'data')\n",
    "path_logs = os.path.join(path_root, 'logs')\n",
    "path_models = os.path.join(path_root, 'models', filename)\n",
    "path_results = os.path.join(path_root, 'results', filename)\n",
    "\n",
    "for p in [path_data, path_logs, path_models, path_results]:\n",
    "    os.makedirs(p, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e4f24c-2bab-456f-9897-fb7cf4a72efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(filename)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "streamformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "streamhandler = logging.StreamHandler()\n",
    "streamhandler.setFormatter(streamformatter)\n",
    "logger.addHandler(streamhandler)\n",
    "\n",
    "fileformatter = logging.Formatter('%(message)s')\n",
    "filehandler = logging.FileHandler(os.path.join(path_logs, filename + '.log'), mode='w')\n",
    "filehandler.setFormatter(fileformatter)\n",
    "logger.addHandler(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4543504-728a-4bbf-8ebd-a70f223288d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, target = self.dataset[idx]\n",
    "        return data, target, idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0859e906-dbe3-4c6f-a44d-3ce50555d228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'cifar10':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([x / 255 for x in [125.3, 123, 113.9]], [x / 255 for x in [63, 62.1, 66.7]])\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([x / 255 for x in [125.3, 123, 113.9]], [x / 255 for x in [63, 62.1, 66.7]])\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.CIFAR10(path_data, train=False, transform=test_transform)\n",
    "    \n",
    "elif dataset == 'svhn':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.SVHN(path_data, split='train', transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.SVHN(path_data, split='test', transform=test_transform, download=True)\n",
    "    \n",
    "elif dataset == 'fmnist':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.FashionMNIST(path_data, train=False, transform=test_transform)\n",
    "\n",
    "elif dataset == 'mnist':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.MNIST(path_data, train=False, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6182a3f4-472d-4246-9ae0-e184be956dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_data, f'{dataset}_diri9a001_42.json')) as f:\n",
    "    indices = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02409155-7c8a-4217-8258-ccb1a2c7e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_workers = 128, 0\n",
    "\n",
    "inferloaders, subset_indices = {}, []\n",
    "for k, v in indices.items():\n",
    "    infersubset = Subset(custom_dataset, v['index'])\n",
    "    inferloaders[k] = DataLoader(infersubset, batch_size=batch_size, num_workers=num_workers)\n",
    "    subset_indices.extend(v['index'])\n",
    "\n",
    "train_subset = Subset(train_dataset, indices=subset_indices)\n",
    "fed_trainloader = DataLoader(train_subset, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "try:\n",
    "    train_labels = np.asarray(custom_dataset.dataset.targets)\n",
    "    test_labels = test_dataset.targets\n",
    "except AttributeError:\n",
    "    train_labels = np.asarray(custom_dataset.dataset.labels)\n",
    "    test_labels = test_dataset.labels\n",
    "subset_classes = np.unique(train_labels[subset_indices])\n",
    "boolarr = [True if y in subset_classes else False for y in test_labels]\n",
    "subset_indices = np.arange(len(test_dataset))[boolarr]\n",
    "test_subset = Subset(test_dataset, indices=subset_indices)\n",
    "testloader = DataLoader(test_subset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37423e2e-709a-4a3c-b0de-99acc3e83e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, padding=1, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=kernel_size, stride=1, padding=padding, bias=bias)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if (stride != 1) or (in_channel != self.expansion * out_channel):\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, self.expansion * out_channel, kernel_size=1, stride=stride, bias=bias),\n",
    "                nn.BatchNorm2d(self.expansion * out_channel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dd80881-9cee-4d1c-8348-aaf4eacc8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, padding=1, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=1, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv3 = nn.Conv2d(out_channel, self.expansion * out_channel, kerenel_size=1, bias=bias)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * out_channel)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if (stride != 1) or (in_channel != self.expansion * out_channel):\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, self.expansion * out_channel, kernel_size=1, stride=stride, bias=bias),\n",
    "                nn.BatchNorm2d(self.expansion * out_channel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eede0030-1cdc-4185-a7b3-825076d49102",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, in_channel=3, out_channels=[64, 128, 256, 512], num_blocks=[2, 2, 2, 2], strides=[1, 2, 2, 2], num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_channel = out_channels[0]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channels[0], kernel_size=3, stride=strides[0], padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels[0])\n",
    "        \n",
    "        self.block1 = self._make_layer(block, out_channels[0], num_blocks[0], strides[0])\n",
    "        self.block2 = self._make_layer(block, out_channels[1], num_blocks[1], strides[1])\n",
    "        self.block3 = self._make_layer(block, out_channels[2], num_blocks[2], strides[2])\n",
    "        self.block4 = self._make_layer(block, out_channels[3], num_blocks[3], strides[3])\n",
    "        \n",
    "        self.linear = nn.Linear(out_channels[-1] * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channel, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channel, out_channel, stride=stride))\n",
    "            self.in_channel = out_channel * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9209704a-22fa-4e92-b5e2-b666bda96ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplenorm(x):\n",
    "    return x / np.sum(x)\n",
    "\n",
    "def weighting(forget_cnt_per_client, standardize=0.1):\n",
    "    if (standardize == 0.0) or (np.sum(forget_cnt_per_client) == 0):\n",
    "        weights = np.ones(len(forget_cnt_per_client))\n",
    "    else:\n",
    "        weights = 1 - standardize + len(forget_cnt_per_client) * standardize * simplenorm(forget_cnt_per_client)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae0fe2f3-5ec9-46fe-b29c-8db8b81089ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(w, standardized_weights):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    \n",
    "    w_avg.update((k, v * standardized_weights[0]) for k, v in w_avg.items())\n",
    "    \n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += (w[i][key] * standardized_weights[i])\n",
    "        w_avg[key] = torch.div(w_avg[key], float(len(w)))\n",
    "        \n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "611e3c0f-9f0e-419a-aa1f-a497b671aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, epochs, lr, weight_decay, criterion, device, global_model, mu=0.01):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    global_params = list(global_model.parameters())\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        batch_losses = []\n",
    "        \n",
    "        for inputs, labels, _ in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss.mean()\n",
    "            \n",
    "            prox_reg = 0.0\n",
    "            for i, param in enumerate(model.parameters()):\n",
    "                prox_reg += ((mu / 2) * torch.norm((param - global_params[i])) ** 2)\n",
    "            loss += prox_reg\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "        epoch_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "        \n",
    "    local_weights = model.state_dict()\n",
    "\n",
    "    return local_weights, epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d5c54db-1ba4-4f65-817d-fb8b2b28fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, loader, criterion, device):\n",
    "    avg_loss, correct, num_samples = 0, 0, 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss.mean()\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += torch.sum(torch.eq(predicted, labels)).item()\n",
    "            num_samples += len(labels)\n",
    "\n",
    "    acc = correct / num_samples\n",
    "    avg_loss /= len(loader)\n",
    "    \n",
    "    return acc, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "629ff8ea-9394-4a48-a5e0-1a90d329a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_train(model, loader, device, match_history, flag=False):\n",
    "    forgettables = []\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, indices in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            match = predicted.eq(labels)\n",
    "            match = match.type(torch.IntTensor)\n",
    "            total += len(indices)\n",
    "            \n",
    "            for j, idx in enumerate(indices):\n",
    "                sample_history = match_history.get(idx.item(), [])\n",
    "                sample_history.append(match[j].item())\n",
    "                match_history[idx.item()] = sample_history\n",
    "                correct += match[j].item()\n",
    "                if flag is True:\n",
    "                    try:\n",
    "                        if match[j].item() - sample_history[-2] == -1:\n",
    "                            forgettables.append(idx.item())\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "            \n",
    "    acc = correct / total\n",
    "            \n",
    "    return match_history, forgettables, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7562c7c7-4368-442b-8bd7-fa1674a6842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatfloats(li):\n",
    "    new = [float(f'{e:>8.4f}') for e in li]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc4a3691-7135-496c-9153-23e7fc4f1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet9 = {'block': ResidualBlock, 'num_blocks': [1, 1, 1, 1]}\n",
    "resnet18 = {'block': ResidualBlock, 'num_blocks': [2, 2, 2, 2]}\n",
    "resnet34 = {'block': ResidualBlock, 'num_blocks': [3, 4, 6, 3]}\n",
    "resnet50 = {'block': Bottleneck, 'num_blocks': [3, 4, 6, 3]}\n",
    "resnet101 = {'block': Bottleneck, 'num_blocks': [3, 4, 23, 3]}\n",
    "resnet152 = {'block': Bottleneck, 'num_blocks': [3, 8, 36, 3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1261d399-3f2a-4f98-b585-81e5037a36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f'cuda:{gpu}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20a73dfb-ec2c-40fd-9893-3eac24fbca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = resnet9.copy()\n",
    "\n",
    "out_channels = [64, 128, 256, 512]\n",
    "strides = [1, 2, 2, 2]\n",
    "in_channel = 3\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "fed_model = ResNet(resnet['block'], in_channel=in_channel, out_channels=out_channels, num_blocks=resnet['num_blocks'], strides=strides, num_classes=num_classes)\n",
    "fed_weights = fed_model.state_dict()\n",
    "\n",
    "fed_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d0203-4dd4-4c5b-9f57-5693890ec7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-02 18:52:39,068 - INFO - \n",
      "Algorithm: FedProx Weighted 0.1\n",
      "Client Split: 9\n",
      "Dataset: cifar10\n",
      "Model: ResNet9 | Rounds: 200 | Epochs: 10 | LR: 0.001\n",
      "\n",
      "2021-09-02 18:52:39,071 - INFO -  | Global Training Round : 1 / 200 |\n",
      "2021-09-02 18:52:55,100 - INFO -   |-- [Party 1] Average Train Loss:   1.8864 Train Accuracy: global  10.39% local  43.49% ...    0 forgettables out of 1819 ( 0.00%) ... total data used 1819\n",
      "2021-09-02 18:52:55,101 - INFO -   |--    Epoch Losses (10): [2.308, 2.2157, 2.0582, 1.8829, 1.8047, 1.7953, 1.7576, 1.7247, 1.678, 1.639]\n",
      "2021-09-02 18:53:03,396 - INFO -   |-- [Party 2] Average Train Loss:   1.9336 Train Accuracy: global  10.18% local  31.54% ...    0 forgettables out of 1081 ( 0.00%) ... total data used 1081\n",
      "2021-09-02 18:53:03,399 - INFO -   |--    Epoch Losses (10): [2.2486, 2.2382, 2.1743, 2.0089, 1.901, 1.8124, 1.7813, 1.7832, 1.6914, 1.6965]\n",
      "2021-09-02 18:53:23,309 - INFO -   |-- [Party 3] Average Train Loss:   1.8234 Train Accuracy: global   9.51% local  52.51% ...    0 forgettables out of 2630 ( 0.00%) ... total data used 2630\n",
      "2021-09-02 18:53:23,313 - INFO -   |--    Epoch Losses (10): [2.2817, 2.1247, 1.9204, 1.804, 1.7468, 1.7185, 1.7015, 1.6699, 1.646, 1.6204]\n",
      "2021-09-02 18:53:29,940 - INFO -   |-- [Party 4] Average Train Loss:   2.0240 Train Accuracy: global  10.49% local  37.38% ...    0 forgettables out of  915 ( 0.00%) ... total data used  915\n",
      "2021-09-02 18:53:29,942 - INFO -   |--    Epoch Losses (10): [2.2632, 2.2396, 2.2065, 2.1318, 2.0752, 1.956, 1.8919, 1.8613, 1.787, 1.8278]\n",
      "2021-09-02 18:53:41,676 - INFO -   |-- [Party 5] Average Train Loss:   2.1071 Train Accuracy: global   9.71% local  29.56% ...    0 forgettables out of 1668 ( 0.00%) ... total data used 1668\n",
      "2021-09-02 18:53:41,680 - INFO -   |--    Epoch Losses (10): [2.3089, 2.2367, 2.1505, 2.1545, 2.1242, 2.0644, 2.0214, 2.0369, 1.9544, 2.019]\n",
      "2021-09-02 18:53:54,816 - INFO -   |-- [Party 6] Average Train Loss:   1.9239 Train Accuracy: global   9.15% local  37.41% ...    0 forgettables out of 1596 ( 0.00%) ... total data used 1596\n",
      "2021-09-02 18:53:54,819 - INFO -   |--    Epoch Losses (10): [2.3071, 2.2498, 2.1049, 1.9351, 1.8656, 1.8466, 1.8248, 1.7718, 1.6917, 1.6419]\n",
      "2021-09-02 18:54:10,615 - INFO -   |-- [Party 7] Average Train Loss:   1.8382 Train Accuracy: global  10.18% local  51.43% ...    0 forgettables out of 2131 ( 0.00%) ... total data used 2131\n",
      "2021-09-02 18:54:10,617 - INFO -   |--    Epoch Losses (10): [2.3177, 2.1762, 1.9799, 1.846, 1.7667, 1.7269, 1.7172, 1.6347, 1.6301, 1.5863]\n",
      "2021-09-02 18:54:18,282 - INFO -   |-- [Party 8] Average Train Loss:   1.9499 Train Accuracy: global   9.41% local  50.09% ...    0 forgettables out of 1084 ( 0.00%) ... total data used 1084\n",
      "2021-09-02 18:54:18,284 - INFO -   |--    Epoch Losses (10): [2.3342, 2.2127, 2.1764, 2.0185, 1.9413, 1.9047, 1.7785, 1.7409, 1.6828, 1.7091]\n",
      "2021-09-02 18:54:34,314 - INFO -   |-- [Party 9] Average Train Loss:   1.7852 Train Accuracy: global   9.00% local  49.31% ...    0 forgettables out of 2034 ( 0.00%) ... total data used 2034\n",
      "2021-09-02 18:54:34,316 - INFO -   |--    Epoch Losses (10): [2.1935, 2.0935, 1.9306, 1.79, 1.7467, 1.6678, 1.6322, 1.6478, 1.5881, 1.5624]\n",
      "2021-09-02 18:54:43,066 - INFO -     |---- Number of Forgettables: 0 (0.00%)\n",
      "2021-09-02 18:54:43,067 - INFO -     |---- Train Accuracy: 17.41%\n",
      "2021-09-02 18:54:43,068 - INFO -     |---- Test Accuracy: 18.82%\n",
      "2021-09-02 18:54:43,069 - INFO -     |---- Test Loss: 2.1426\n",
      "2021-09-02 18:54:43,070 - INFO -     |---- Elapsed time: 0:02:03.999297\n",
      "2021-09-02 18:54:43,070 - INFO - \n",
      "Test Acc: Max 18.8200% (1 round) | Last 18.8200% | Avg 18.8200% (1 round)\n",
      "\n",
      "2021-09-02 18:54:43,073 - INFO -  | Global Training Round : 2 / 200 |\n",
      "2021-09-02 18:54:55,649 - INFO -   |-- [Party 1] Average Train Loss:   1.6519 Train Accuracy: global  18.91% local  43.10% ...  670 forgettables out of 1819 (36.83%) ... total data used 1819\n",
      "2021-09-02 18:54:55,651 - INFO -   |--    Epoch Losses (10): [1.9893, 1.7943, 1.6985, 1.6342, 1.6202, 1.5494, 1.587, 1.6042, 1.4985, 1.543]\n",
      "2021-09-02 18:55:03,262 - INFO -   |-- [Party 2] Average Train Loss:   1.6260 Train Accuracy: global  18.50% local  53.65% ...  259 forgettables out of 1081 (23.96%) ... total data used 1081\n",
      "2021-09-02 18:55:03,263 - INFO -   |--    Epoch Losses (10): [2.0188, 1.8697, 1.8079, 1.6817, 1.5808, 1.5463, 1.43, 1.4415, 1.4615, 1.4222]\n",
      "2021-09-02 18:55:23,865 - INFO -   |-- [Party 3] Average Train Loss:   1.6079 Train Accuracy: global  17.49% local  48.86% ... 1132 forgettables out of 2630 (43.04%) ... total data used 2630\n",
      "2021-09-02 18:55:23,867 - INFO -   |--    Epoch Losses (10): [1.9649, 1.7801, 1.6646, 1.5708, 1.5196, 1.5177, 1.5083, 1.5294, 1.4914, 1.532]\n",
      "2021-09-02 18:55:30,397 - INFO -   |-- [Party 4] Average Train Loss:   1.7220 Train Accuracy: global  19.02% local  38.91% ...  281 forgettables out of  915 (30.71%) ... total data used  915\n",
      "2021-09-02 18:55:30,400 - INFO -   |--    Epoch Losses (10): [1.9748, 1.9665, 1.8313, 1.8091, 1.7161, 1.645, 1.5393, 1.6616, 1.5501, 1.5266]\n",
      "2021-09-02 18:55:43,598 - INFO -   |-- [Party 5] Average Train Loss:   1.7692 Train Accuracy: global  15.65% local  41.19% ...  398 forgettables out of 1668 (23.86%) ... total data used 1668\n",
      "2021-09-02 18:55:43,599 - INFO -   |--    Epoch Losses (10): [1.8758, 1.9906, 1.8632, 1.7571, 1.8122, 1.7675, 1.7128, 1.6811, 1.6557, 1.5764]\n",
      "2021-09-02 18:55:55,205 - INFO -   |-- [Party 6] Average Train Loss:   1.5926 Train Accuracy: global  16.54% local  48.25% ...  518 forgettables out of 1596 (32.46%) ... total data used 1596\n",
      "2021-09-02 18:55:55,207 - INFO -   |--    Epoch Losses (10): [1.9722, 1.8232, 1.6774, 1.6028, 1.5501, 1.4606, 1.4785, 1.4344, 1.42, 1.5065]\n",
      "2021-09-02 18:56:10,955 - INFO -   |-- [Party 7] Average Train Loss:   1.5723 Train Accuracy: global  19.19% local  55.98% ...  858 forgettables out of 2131 (40.26%) ... total data used 2131\n",
      "2021-09-02 18:56:10,959 - INFO -   |--    Epoch Losses (10): [2.0014, 1.7667, 1.6008, 1.5561, 1.5353, 1.5196, 1.4785, 1.4086, 1.427, 1.4294]\n",
      "2021-09-02 18:56:18,402 - INFO -   |-- [Party 8] Average Train Loss:   1.5612 Train Accuracy: global  16.70% local  52.58% ...  442 forgettables out of 1084 (40.77%) ... total data used 1084\n",
      "2021-09-02 18:56:18,404 - INFO -   |--    Epoch Losses (10): [1.8811, 1.8203, 1.7059, 1.6174, 1.5253, 1.4469, 1.4234, 1.3745, 1.392, 1.4254]\n",
      "2021-09-02 18:56:32,758 - INFO -   |-- [Party 9] Average Train Loss:   1.5340 Train Accuracy: global  17.70% local  47.84% ...  803 forgettables out of 2034 (39.48%) ... total data used 2034\n",
      "2021-09-02 18:56:32,760 - INFO -   |--    Epoch Losses (10): [1.9408, 1.7807, 1.5824, 1.5245, 1.4908, 1.4043, 1.409, 1.4472, 1.3923, 1.3678]\n",
      "2021-09-02 18:56:41,278 - INFO -     |---- Number of Forgettables: 5361 (35.84%)\n",
      "2021-09-02 18:56:41,280 - INFO -     |---- Train Accuracy: 52.90%\n",
      "2021-09-02 18:56:41,281 - INFO -     |---- Test Accuracy: 52.13%\n",
      "2021-09-02 18:56:41,282 - INFO -     |---- Test Loss: 1.2920\n",
      "2021-09-02 18:56:41,285 - INFO -     |---- Elapsed time: 0:04:02.214406\n",
      "2021-09-02 18:56:41,286 - INFO - \n",
      "Test Acc: Max 52.1300% (2 round) | Last 52.1300% | Avg 35.4750% (2 round)\n",
      "\n",
      "2021-09-02 18:56:41,290 - INFO -  | Global Training Round : 3 / 200 |\n",
      "2021-09-02 18:56:54,111 - INFO -   |-- [Party 1] Average Train Loss:   1.3533 Train Accuracy: global  52.39% local  60.53% ...  255 forgettables out of 1819 (14.02%) ... total data used 1819\n",
      "2021-09-02 18:56:54,113 - INFO -   |--    Epoch Losses (10): [1.5891, 1.434, 1.3666, 1.3687, 1.3577, 1.2989, 1.2689, 1.2942, 1.2992, 1.2555]\n",
      "2021-09-02 18:57:01,533 - INFO -   |-- [Party 2] Average Train Loss:   1.3221 Train Accuracy: global  51.62% local  62.72% ...  179 forgettables out of 1081 (16.56%) ... total data used 1081\n",
      "2021-09-02 18:57:01,535 - INFO -   |--    Epoch Losses (10): [1.528, 1.5234, 1.4455, 1.3764, 1.2915, 1.2428, 1.2026, 1.2296, 1.2049, 1.1767]\n",
      "2021-09-02 18:57:19,605 - INFO -   |-- [Party 3] Average Train Loss:   1.3356 Train Accuracy: global  52.17% local  47.49% ...  413 forgettables out of 2630 (15.70%) ... total data used 2630\n",
      "2021-09-02 18:57:19,606 - INFO -   |--    Epoch Losses (10): [1.5587, 1.4107, 1.3622, 1.3433, 1.3466, 1.3263, 1.2519, 1.263, 1.2465, 1.2464]\n",
      "2021-09-02 18:57:26,031 - INFO -   |-- [Party 4] Average Train Loss:   1.3580 Train Accuracy: global  50.82% local  59.78% ...  122 forgettables out of  915 (13.33%) ... total data used  915\n",
      "2021-09-02 18:57:26,032 - INFO -   |--    Epoch Losses (10): [1.5911, 1.5051, 1.4537, 1.3839, 1.345, 1.3244, 1.2841, 1.2437, 1.2425, 1.207]\n",
      "2021-09-02 18:57:37,623 - INFO -   |-- [Party 5] Average Train Loss:   1.5481 Train Accuracy: global  52.22% local  50.90% ...  175 forgettables out of 1668 (10.49%) ... total data used 1668\n",
      "2021-09-02 18:57:37,625 - INFO -   |--    Epoch Losses (10): [1.5916, 1.5721, 1.5056, 1.4867, 1.4569, 1.595, 1.526, 1.5485, 1.5842, 1.6142]\n",
      "2021-09-02 18:57:49,040 - INFO -   |-- [Party 6] Average Train Loss:   1.3535 Train Accuracy: global  52.19% local  55.51% ...  227 forgettables out of 1596 (14.22%) ... total data used 1596\n",
      "2021-09-02 18:57:49,041 - INFO -   |--    Epoch Losses (10): [1.64, 1.5224, 1.4214, 1.3988, 1.3164, 1.3101, 1.2419, 1.2522, 1.1711, 1.2606]\n",
      "2021-09-02 18:58:03,422 - INFO -   |-- [Party 7] Average Train Loss:   1.3197 Train Accuracy: global  53.26% local  61.43% ...  346 forgettables out of 2131 (16.24%) ... total data used 2131\n",
      "2021-09-02 18:58:03,423 - INFO -   |--    Epoch Losses (10): [1.6098, 1.4246, 1.3187, 1.2933, 1.2875, 1.2543, 1.2586, 1.2543, 1.2652, 1.231]\n",
      "2021-09-02 18:58:10,651 - INFO -   |-- [Party 8] Average Train Loss:   1.2837 Train Accuracy: global  54.15% local  63.84% ...  166 forgettables out of 1084 (15.31%) ... total data used 1084\n",
      "2021-09-02 18:58:10,652 - INFO -   |--    Epoch Losses (10): [1.6345, 1.466, 1.327, 1.2571, 1.2419, 1.1925, 1.1644, 1.2178, 1.1922, 1.1438]\n",
      "2021-09-02 18:58:26,923 - INFO -   |-- [Party 9] Average Train Loss:   1.2399 Train Accuracy: global  54.42% local  56.64% ...  270 forgettables out of 2034 (13.27%) ... total data used 2034\n",
      "2021-09-02 18:58:26,927 - INFO -   |--    Epoch Losses (10): [1.5194, 1.2941, 1.2915, 1.244, 1.254, 1.148, 1.1392, 1.2218, 1.1577, 1.1296]\n",
      "2021-09-02 18:58:35,632 - INFO -     |---- Number of Forgettables: 2153 (14.39%)\n",
      "2021-09-02 18:58:35,635 - INFO -     |---- Train Accuracy: 62.64%\n",
      "2021-09-02 18:58:35,636 - INFO -     |---- Test Accuracy: 60.97%\n",
      "2021-09-02 18:58:35,637 - INFO -     |---- Test Loss: 1.0935\n",
      "2021-09-02 18:58:35,637 - INFO -     |---- Elapsed time: 0:05:56.567245\n",
      "2021-09-02 18:58:35,639 - INFO - \n",
      "Test Acc: Max 60.9700% (3 round) | Last 60.9700% | Avg 43.9733% (2 round)\n",
      "\n",
      "2021-09-02 18:58:35,641 - INFO -  | Global Training Round : 4 / 200 |\n",
      "2021-09-02 18:58:49,186 - INFO -   |-- [Party 1] Average Train Loss:   1.1790 Train Accuracy: global  61.24% local  58.33% ...  254 forgettables out of 1819 (13.96%) ... total data used 1819\n",
      "2021-09-02 18:58:49,190 - INFO -   |--    Epoch Losses (10): [1.37, 1.2716, 1.2391, 1.1715, 1.1239, 1.1212, 1.132, 1.0995, 1.1398, 1.121]\n",
      "2021-09-02 18:58:57,047 - INFO -   |-- [Party 2] Average Train Loss:   1.1066 Train Accuracy: global  63.46% local  60.96% ...  143 forgettables out of 1081 (13.23%) ... total data used 1081\n",
      "2021-09-02 18:58:57,050 - INFO -   |--    Epoch Losses (10): [1.4134, 1.2806, 1.1795, 1.0672, 1.118, 1.0526, 1.0198, 1.0171, 0.9852, 0.9321]\n",
      "2021-09-02 18:59:17,065 - INFO -   |-- [Party 3] Average Train Loss:   1.1927 Train Accuracy: global  61.25% local  59.85% ...  246 forgettables out of 2630 ( 9.35%) ... total data used 2630\n",
      "2021-09-02 18:59:17,067 - INFO -   |--    Epoch Losses (10): [1.3763, 1.2337, 1.1956, 1.1633, 1.2053, 1.1568, 1.1142, 1.1703, 1.1577, 1.1542]\n",
      "2021-09-02 18:59:23,828 - INFO -   |-- [Party 4] Average Train Loss:   1.1866 Train Accuracy: global  62.84% local  65.25% ...  107 forgettables out of  915 (11.69%) ... total data used  915\n",
      "2021-09-02 18:59:23,831 - INFO -   |--    Epoch Losses (10): [1.3364, 1.439, 1.3355, 1.2358, 1.2047, 1.121, 1.0658, 1.0351, 1.0634, 1.0289]\n",
      "2021-09-02 18:59:36,518 - INFO -   |-- [Party 5] Average Train Loss:   1.4471 Train Accuracy: global  62.89% local  51.32% ...  152 forgettables out of 1668 ( 9.11%) ... total data used 1668\n",
      "2021-09-02 18:59:36,520 - INFO -   |--    Epoch Losses (10): [1.4836, 1.5196, 1.4768, 1.3873, 1.3479, 1.3085, 1.4353, 1.5281, 1.4871, 1.4968]\n",
      "2021-09-02 18:59:47,958 - INFO -   |-- [Party 6] Average Train Loss:   1.1564 Train Accuracy: global  62.47% local  64.60% ...  171 forgettables out of 1596 (10.71%) ... total data used 1596\n",
      "2021-09-02 18:59:47,959 - INFO -   |--    Epoch Losses (10): [1.4735, 1.2953, 1.1666, 1.1673, 1.1341, 1.0619, 1.0673, 1.0919, 1.0443, 1.0614]\n",
      "2021-09-02 19:00:04,189 - INFO -   |-- [Party 7] Average Train Loss:   1.1570 Train Accuracy: global  62.93% local  62.13% ...  253 forgettables out of 2131 (11.87%) ... total data used 2131\n",
      "2021-09-02 19:00:04,190 - INFO -   |--    Epoch Losses (10): [1.3966, 1.2414, 1.1669, 1.1361, 1.1041, 1.1325, 1.0916, 1.1167, 1.0839, 1.1002]\n",
      "2021-09-02 19:00:11,594 - INFO -   |-- [Party 8] Average Train Loss:   1.0836 Train Accuracy: global  62.82% local  66.79% ...  143 forgettables out of 1084 (13.19%) ... total data used 1084\n",
      "2021-09-02 19:00:11,595 - INFO -   |--    Epoch Losses (10): [1.3808, 1.2974, 1.1564, 1.065, 1.0335, 0.9931, 0.9876, 0.9608, 1.0016, 0.9597]\n",
      "2021-09-02 19:00:25,509 - INFO -   |-- [Party 9] Average Train Loss:   1.1228 Train Accuracy: global  64.16% local  59.64% ...  236 forgettables out of 2034 (11.60%) ... total data used 2034\n",
      "2021-09-02 19:00:25,513 - INFO -   |--    Epoch Losses (10): [1.3319, 1.208, 1.1811, 1.1161, 1.0761, 1.0841, 1.1027, 1.0291, 1.0428, 1.0563]\n",
      "2021-09-02 19:00:34,060 - INFO -     |---- Number of Forgettables: 1705 (11.40%)\n",
      "2021-09-02 19:00:34,062 - INFO -     |---- Train Accuracy: 67.06%\n",
      "2021-09-02 19:00:34,063 - INFO -     |---- Test Accuracy: 65.30%\n",
      "2021-09-02 19:00:34,064 - INFO -     |---- Test Loss: 0.9817\n",
      "2021-09-02 19:00:34,065 - INFO -     |---- Elapsed time: 0:07:54.994384\n",
      "2021-09-02 19:00:34,066 - INFO - \n",
      "Test Acc: Max 65.3000% (4 round) | Last 65.3000% | Avg 49.3050% (2 round)\n",
      "\n",
      "2021-09-02 19:00:34,069 - INFO -  | Global Training Round : 5 / 200 |\n",
      "2021-09-02 19:00:46,655 - INFO -   |-- [Party 1] Average Train Loss:   1.0849 Train Accuracy: global  65.64% local  65.53% ...  190 forgettables out of 1819 (10.45%) ... total data used 1819\n",
      "2021-09-02 19:00:46,657 - INFO -   |--    Epoch Losses (10): [1.2557, 1.1803, 1.1206, 1.1785, 1.059, 1.0366, 1.0322, 1.01, 0.9962, 0.9801]\n",
      "2021-09-02 19:00:55,180 - INFO -   |-- [Party 2] Average Train Loss:   0.9669 Train Accuracy: global  66.42% local  75.12% ...  152 forgettables out of 1081 (14.06%) ... total data used 1081\n",
      "2021-09-02 19:00:55,182 - INFO -   |--    Epoch Losses (10): [1.2385, 1.1784, 1.0004, 0.9715, 0.9227, 0.9444, 0.8739, 0.8226, 0.8708, 0.8455]\n",
      "2021-09-02 19:01:14,045 - INFO -   |-- [Party 3] Average Train Loss:   1.1055 Train Accuracy: global  66.73% local  69.89% ...  289 forgettables out of 2630 (10.99%) ... total data used 2630\n",
      "2021-09-02 19:01:14,048 - INFO -   |--    Epoch Losses (10): [1.2892, 1.1816, 1.0953, 1.0557, 1.106, 1.0577, 1.0774, 1.0624, 1.0946, 1.0352]\n",
      "2021-09-02 19:01:21,265 - INFO -   |-- [Party 4] Average Train Loss:   1.0588 Train Accuracy: global  66.01% local  63.06% ...  115 forgettables out of  915 (12.57%) ... total data used  915\n",
      "2021-09-02 19:01:21,267 - INFO -   |--    Epoch Losses (10): [1.1791, 1.1996, 1.1459, 1.0205, 1.0618, 1.0447, 1.0632, 1.0122, 0.9619, 0.8987]\n",
      "2021-09-02 19:01:34,374 - INFO -   |-- [Party 5] Average Train Loss:   1.2991 Train Accuracy: global  67.39% local  62.41% ...  104 forgettables out of 1668 ( 6.24%) ... total data used 1668\n",
      "2021-09-02 19:01:34,375 - INFO -   |--    Epoch Losses (10): [1.261, 1.2208, 1.2954, 1.3252, 1.2811, 1.3713, 1.3748, 1.2774, 1.3182, 1.2659]\n",
      "2021-09-02 19:01:45,892 - INFO -   |-- [Party 6] Average Train Loss:   1.0544 Train Accuracy: global  66.35% local  61.53% ...  167 forgettables out of 1596 (10.46%) ... total data used 1596\n",
      "2021-09-02 19:01:45,894 - INFO -   |--    Epoch Losses (10): [1.2764, 1.1882, 1.0961, 1.027, 1.0138, 1.0118, 0.9688, 0.9917, 0.9761, 0.9942]\n",
      "2021-09-02 19:02:00,494 - INFO -   |-- [Party 7] Average Train Loss:   1.0474 Train Accuracy: global  68.28% local  73.86% ...  193 forgettables out of 2131 ( 9.06%) ... total data used 2131\n",
      "2021-09-02 19:02:00,496 - INFO -   |--    Epoch Losses (10): [1.2586, 1.1567, 1.0254, 1.0411, 0.997, 1.0207, 1.0085, 0.987, 0.9943, 0.9843]\n"
     ]
    }
   ],
   "source": [
    "rounds = 200\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "wdecay = 0\n",
    "\n",
    "logger.info(f'\\nAlgorithm: FedProx Weighted {standardize}\\nClient Split: 9\\nDataset: {dataset}\\nModel: ResNet9 | Rounds: {rounds} | Epochs: {epochs} | LR: {lr}\\n')\n",
    "\n",
    "train_accs, train_losses, test_accs, test_losses = [], [], [], []\n",
    "match_history, round_forget_history = {}, []\n",
    "forget_cnt_per_client = [0] * len(indices.keys())\n",
    "\n",
    "st = time.time()\n",
    "for r in range(rounds):\n",
    "    \n",
    "    round_forget, round_samples = 0, 0\n",
    "    forget_history, forgettables = {}, {}\n",
    "    local_weights, local_losses = [], []\n",
    "    logger.info(f' | Global Training Round : {r + 1} / {rounds} |')\n",
    "    \n",
    "    for i, k in enumerate(indices.keys()):\n",
    "        match_history, forgettables, global_acc = infer_train(fed_model, inferloaders[k], device, match_history, flag=True)\n",
    "\n",
    "        fed_model.train()\n",
    "        \n",
    "        sampler_idx = indices[k]['index'].copy()\n",
    "        sampler = SubsetRandomSampler(sampler_idx)\n",
    "\n",
    "        trainloader = DataLoader(custom_dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
    "        \n",
    "        local_model = copy.deepcopy(fed_model)\n",
    "        w, ls = train(local_model, trainloader, epochs, lr, wdecay, criterion, device, fed_model, mu=0.01)\n",
    "\n",
    "        local_weights.append(copy.deepcopy(w))\n",
    "        train_losses.append(ls)\n",
    "        \n",
    "        match_history, _, local_acc = infer_train(local_model, inferloaders[k], device, match_history, flag=False)\n",
    "        train_accs.append(local_acc)\n",
    "        \n",
    "        forget_cnt = len(forgettables)\n",
    "        round_forget += forget_cnt\n",
    "        forget_cnt_per_client[int(k)] = forget_cnt\n",
    "        round_samples += len(indices[k]['index'])\n",
    "        \n",
    "        logger.info('  |-- [Party {:>1}] Average Train Loss: {:>8.4f} Train Accuracy: global {:>6.2f}% local {:>6.2f}% ... {:>4} forgettables out of {:>4} ({:>5.2f}%) ... total data used {:>4}'.format(\n",
    "            i + 1, sum(ls) / len(ls), 100 * global_acc, 100 * local_acc, forget_cnt, len(indices[k]['index']), 100 * forget_cnt / len(indices[k]['index']), len(sampler_idx)\n",
    "        ))\n",
    "        logger.info('  |--    Epoch Losses ({:>2}): {}'.format(epochs, formatfloats(ls)))\n",
    "        \n",
    "    standardized_weights = weighting(forget_cnt_per_client, standardize)\n",
    "    fed_weights = average_weights(local_weights, standardized_weights)\n",
    "    fed_model.load_state_dict(fed_weights)\n",
    "    train_acc, _ = inference(fed_model, fed_trainloader, criterion, device)\n",
    "    \n",
    "    if (r + 1) % 50 == 0:\n",
    "        torch.save(fed_model.state_dict(), os.path.join(path_models, filename + f'_round{r+1}.pth'))\n",
    "    \n",
    "    test_acc, test_ls = inference(fed_model, testloader, criterion, device)\n",
    "    test_accs.append(test_acc)\n",
    "    test_losses.append(test_ls)\n",
    "    round_forget_history.append(round_forget)\n",
    "    logger.info('    |---- Number of Forgettables: {} ({:.2f}%)'.format(round_forget, 100 * round_forget / round_samples))\n",
    "    logger.info('    |---- Train Accuracy: {:>.2f}%'.format(100 * train_acc))\n",
    "    logger.info('    |---- Test Accuracy: {:>.2f}%'.format(100 * test_acc))\n",
    "    logger.info('    |---- Test Loss: {:.4f}'.format(test_ls))\n",
    "    logger.info('    |---- Elapsed time: {}'.format(timedelta(seconds=time.time()-st)))\n",
    "    logger.info(f'\\nTest Acc: Max {np.max(test_acc) * 100:.4f}% ({np.argmax(test_accs)+1} round) | Last {test_accs[-1] * 100:.4f}% | Avg {np.mean(test_accs) * 100:.4f}% ({np.argmax(test_accs > np.mean(test_accs))+1} round)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654dbc03-4dd8-4c75-932e-f9f842c9ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = np.asarray(train_losses)\n",
    "train_accs = np.asarray(train_accs)\n",
    "\n",
    "with open(os.path.join(path_results, f'{filename}_tr_ls.npy'), 'wb') as f:\n",
    "    np.save(f, train_losses)\n",
    "with open(os.path.join(path_results, f'{filename}_tr_acc.npy'), 'wb') as f:\n",
    "    np.save(f, train_accs)\n",
    "with open(os.path.join(path_results, f'{filename}_te_ls.npy'), 'wb') as f:\n",
    "    np.save(f, test_losses)\n",
    "with open(os.path.join(path_results, f'{filename}_te_acc.npy'), 'wb') as f:\n",
    "    np.save(f, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e644234-224e-4353-888c-01affc417a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(50, 30))\n",
    "axs = axs.ravel()\n",
    "\n",
    "axs[0].plot(test_accs, c='orange')\n",
    "axs[0].set_title('Test Accuracies')\n",
    "axs[0].set_xlabel('Rounds')\n",
    "axs[0].set_ylabel('Test Accuracy')\n",
    "axs[1].plot(test_losses, c='blue')\n",
    "axs[1].set_title('Test Losses')\n",
    "axs[1].set_xlabel('Rounds')\n",
    "axs[1].set_ylabel('Test Loss')\n",
    "axs[2].plot(train_accs, c='red')\n",
    "axs[2].set_title('Train Average Accuracies by Epochs')\n",
    "axs[2].set_xlabel('Epochs')\n",
    "axs[2].set_ylabel('Train Average Accuracy')\n",
    "axs[3].plot(train_losses.mean(axis=1), c='turquoise')\n",
    "axs[3].set_title('Train Average Losses by Epochs')\n",
    "axs[3].set_xlabel('Epochs')\n",
    "axs[3].set_ylabel('Train Average Loss')\n",
    "axs[4].plot(np.mean(train_accs.reshape(-1, 10), axis=1), c='green')\n",
    "axs[4].set_title('Train Average Accuracies by Rounds')\n",
    "axs[4].set_xlabel('Rounds')\n",
    "axs[4].set_ylabel('Train Average Accuracy')\n",
    "axs[5].plot(train_losses.mean(axis=1).reshape(-1, 10).mean(axis=1), c='lightpink')\n",
    "axs[5].set_title('Train Average Losses by Rounds')\n",
    "axs[5].set_xlabel('Rounds')\n",
    "axs[5].set_ylabel('Train Average Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9a5cc-41d2-41f1-a4cd-114d63d3b2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

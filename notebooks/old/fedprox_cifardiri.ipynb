{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35825f4-b26d-4ee5-8914-d28fdc8bc1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "dataset = 'cifar10'\n",
    "\n",
    "filename = f'{dataset}_diri9a001_42_fedprox'\n",
    "\n",
    "gpu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8447bc5c-393a-4f8d-80fd-50a652a7a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, Subset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "148a5cb1-99a9-4d54-8239-3ae733854155",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    torch.use_deterministic_algorithms(False)\n",
    "except AttributeError:\n",
    "    torch.set_deterministic(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5090116-fe54-40fe-8459-e9ce9aea270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = os.path.dirname(os.getcwd())\n",
    "path_data = os.path.join(path_root, 'data')\n",
    "path_logs = os.path.join(path_root, 'logs')\n",
    "path_models = os.path.join(path_root, 'models', filename)\n",
    "path_results = os.path.join(path_root, 'results', filename)\n",
    "\n",
    "for p in [path_data, path_logs, path_models, path_results]:\n",
    "    os.makedirs(p, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e4f24c-2bab-456f-9897-fb7cf4a72efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(filename)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "streamformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "streamhandler = logging.StreamHandler()\n",
    "streamhandler.setFormatter(streamformatter)\n",
    "logger.addHandler(streamhandler)\n",
    "\n",
    "fileformatter = logging.Formatter('%(message)s')\n",
    "filehandler = logging.FileHandler(os.path.join(path_logs, filename + '.log'), mode='w')\n",
    "filehandler.setFormatter(fileformatter)\n",
    "logger.addHandler(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4543504-728a-4bbf-8ebd-a70f223288d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, target = self.dataset[idx]\n",
    "        return data, target, idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0859e906-dbe3-4c6f-a44d-3ce50555d228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'cifar10':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([x / 255 for x in [125.3, 123, 113.9]], [x / 255 for x in [63, 62.1, 66.7]])\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([x / 255 for x in [125.3, 123, 113.9]], [x / 255 for x in [63, 62.1, 66.7]])\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.CIFAR10(path_data, train=False, transform=test_transform)\n",
    "    \n",
    "elif dataset == 'svhn':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.SVHN(path_data, split='train', transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.SVHN(path_data, split='test', transform=test_transform, download=True)\n",
    "    \n",
    "elif dataset == 'fmnist':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.FashionMNIST(path_data, train=False, transform=test_transform)\n",
    "\n",
    "elif dataset == 'mnist':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.MNIST(path_data, train=False, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6182a3f4-472d-4246-9ae0-e184be956dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_data, f'{dataset}_diri9a001_42.json')) as f:\n",
    "    indices = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02409155-7c8a-4217-8258-ccb1a2c7e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_workers = 128, 0\n",
    "\n",
    "inferloaders, subset_indices = {}, []\n",
    "for k, v in indices.items():\n",
    "    infersubset = Subset(custom_dataset, v['index'])\n",
    "    inferloaders[k] = DataLoader(infersubset, batch_size=batch_size, num_workers=num_workers)\n",
    "    subset_indices.extend(v['index'])\n",
    "\n",
    "train_subset = Subset(train_dataset, indices=subset_indices)\n",
    "fed_trainloader = DataLoader(train_subset, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "try:\n",
    "    train_labels = np.asarray(custom_dataset.dataset.targets)\n",
    "    test_labels = test_dataset.targets\n",
    "except AttributeError:\n",
    "    train_labels = np.asarray(custom_dataset.dataset.labels)\n",
    "    test_labels = test_dataset.labels\n",
    "subset_classes = np.unique(train_labels[subset_indices])\n",
    "boolarr = [True if y in subset_classes else False for y in test_labels]\n",
    "subset_indices = np.arange(len(test_dataset))[boolarr]\n",
    "test_subset = Subset(test_dataset, indices=subset_indices)\n",
    "testloader = DataLoader(test_subset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37423e2e-709a-4a3c-b0de-99acc3e83e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, padding=1, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=kernel_size, stride=1, padding=padding, bias=bias)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if (stride != 1) or (in_channel != self.expansion * out_channel):\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, self.expansion * out_channel, kernel_size=1, stride=stride, bias=bias),\n",
    "                nn.BatchNorm2d(self.expansion * out_channel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dd80881-9cee-4d1c-8348-aaf4eacc8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, padding=1, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=1, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv3 = nn.Conv2d(out_channel, self.expansion * out_channel, kerenel_size=1, bias=bias)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * out_channel)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if (stride != 1) or (in_channel != self.expansion * out_channel):\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, self.expansion * out_channel, kernel_size=1, stride=stride, bias=bias),\n",
    "                nn.BatchNorm2d(self.expansion * out_channel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eede0030-1cdc-4185-a7b3-825076d49102",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, in_channel=3, out_channels=[64, 128, 256, 512], num_blocks=[2, 2, 2, 2], strides=[1, 2, 2, 2], num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_channel = out_channels[0]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channels[0], kernel_size=3, stride=strides[0], padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels[0])\n",
    "        \n",
    "        self.block1 = self._make_layer(block, out_channels[0], num_blocks[0], strides[0])\n",
    "        self.block2 = self._make_layer(block, out_channels[1], num_blocks[1], strides[1])\n",
    "        self.block3 = self._make_layer(block, out_channels[2], num_blocks[2], strides[2])\n",
    "        self.block4 = self._make_layer(block, out_channels[3], num_blocks[3], strides[3])\n",
    "        \n",
    "        self.linear = nn.Linear(out_channels[-1] * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channel, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channel, out_channel, stride=stride))\n",
    "            self.in_channel = out_channel * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae0fe2f3-5ec9-46fe-b29c-8db8b81089ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(w):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "        \n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], float(len(w)))\n",
    "        \n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "611e3c0f-9f0e-419a-aa1f-a497b671aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, epochs, lr, weight_decay, criterion, device, global_model, mu=0.01):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    global_params = list(global_model.parameters())\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        batch_losses = []\n",
    "        \n",
    "        for inputs, labels, _ in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss.mean()\n",
    "            \n",
    "            prox_reg = 0.0\n",
    "            for i, param in enumerate(model.parameters()):\n",
    "                prox_reg += ((mu / 2) * torch.norm((param - global_params[i])) ** 2)\n",
    "            loss += prox_reg\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "        epoch_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "        \n",
    "    local_weights = model.state_dict()\n",
    "\n",
    "    return local_weights, epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d5c54db-1ba4-4f65-817d-fb8b2b28fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, loader, criterion, device):\n",
    "    avg_loss, correct, num_samples = 0, 0, 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss.mean()\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += torch.sum(torch.eq(predicted, labels)).item()\n",
    "            num_samples += len(labels)\n",
    "\n",
    "    acc = correct / num_samples\n",
    "    avg_loss /= len(loader)\n",
    "    \n",
    "    return acc, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "629ff8ea-9394-4a48-a5e0-1a90d329a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_train(model, loader, device, match_history, flag=False):\n",
    "    forgettables = []\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, indices in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            match = predicted.eq(labels)\n",
    "            match = match.type(torch.IntTensor)\n",
    "            total += len(indices)\n",
    "            \n",
    "            for j, idx in enumerate(indices):\n",
    "                sample_history = match_history.get(idx.item(), [])\n",
    "                sample_history.append(match[j].item())\n",
    "                match_history[idx.item()] = sample_history\n",
    "                correct += match[j].item()\n",
    "                if flag is True:\n",
    "                    try:\n",
    "                        if match[j].item() - sample_history[-2] == -1:\n",
    "                            forgettables.append(idx.item())\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "            \n",
    "    acc = correct / total\n",
    "            \n",
    "    return match_history, forgettables, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7562c7c7-4368-442b-8bd7-fa1674a6842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatfloats(li):\n",
    "    new = [float(f'{e:>8.4f}') for e in li]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc4a3691-7135-496c-9153-23e7fc4f1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet9 = {'block': ResidualBlock, 'num_blocks': [1, 1, 1, 1]}\n",
    "resnet18 = {'block': ResidualBlock, 'num_blocks': [2, 2, 2, 2]}\n",
    "resnet34 = {'block': ResidualBlock, 'num_blocks': [3, 4, 6, 3]}\n",
    "resnet50 = {'block': Bottleneck, 'num_blocks': [3, 4, 6, 3]}\n",
    "resnet101 = {'block': Bottleneck, 'num_blocks': [3, 4, 23, 3]}\n",
    "resnet152 = {'block': Bottleneck, 'num_blocks': [3, 8, 36, 3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1261d399-3f2a-4f98-b585-81e5037a36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f'cuda:{gpu}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20a73dfb-ec2c-40fd-9893-3eac24fbca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = resnet9.copy()\n",
    "\n",
    "out_channels = [64, 128, 256, 512]\n",
    "strides = [1, 2, 2, 2]\n",
    "in_channel = 3\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "fed_model = ResNet(resnet['block'], in_channel=in_channel, out_channels=out_channels, num_blocks=resnet['num_blocks'], strides=strides, num_classes=num_classes)\n",
    "fed_weights = fed_model.state_dict()\n",
    "\n",
    "fed_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d0203-4dd4-4c5b-9f57-5693890ec7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-02 18:52:38,725 - INFO - \n",
      "Algorithm: FedProx\n",
      "Client Split: 9\n",
      "Dataset: cifar10\n",
      "Model: ResNet9 | Rounds: 200 | Epochs: 10 | LR: 0.001\n",
      "\n",
      "2021-09-02 18:52:38,729 - INFO -  | Global Training Round : 1 / 200 |\n",
      "2021-09-02 18:52:53,184 - INFO -   |-- [Party 1] Average Train Loss:   1.8883 Train Accuracy: global  10.39% local  43.38% ...    0 forgettables out of 1819 ( 0.00%) ... total data used 1819\n",
      "2021-09-02 18:52:53,189 - INFO -   |--    Epoch Losses (10): [2.3076, 2.2129, 2.0562, 1.8849, 1.8048, 1.8018, 1.7561, 1.7106, 1.6864, 1.6618]\n",
      "2021-09-02 18:53:02,180 - INFO -   |-- [Party 2] Average Train Loss:   1.9335 Train Accuracy: global  10.18% local  33.95% ...    0 forgettables out of 1081 ( 0.00%) ... total data used 1081\n",
      "2021-09-02 18:53:02,186 - INFO -   |--    Epoch Losses (10): [2.2484, 2.2364, 2.1764, 2.0129, 1.9022, 1.8124, 1.7756, 1.7796, 1.6969, 1.694]\n",
      "2021-09-02 18:53:23,175 - INFO -   |-- [Party 3] Average Train Loss:   1.8240 Train Accuracy: global   9.51% local  47.98% ...    0 forgettables out of 2630 ( 0.00%) ... total data used 2630\n",
      "2021-09-02 18:53:23,179 - INFO -   |--    Epoch Losses (10): [2.283, 2.1246, 1.9195, 1.8083, 1.7637, 1.7077, 1.682, 1.6593, 1.6682, 1.624]\n",
      "2021-09-02 18:53:30,907 - INFO -   |-- [Party 4] Average Train Loss:   2.0240 Train Accuracy: global  10.49% local  36.17% ...    0 forgettables out of  915 ( 0.00%) ... total data used  915\n",
      "2021-09-02 18:53:30,909 - INFO -   |--    Epoch Losses (10): [2.2625, 2.238, 2.2078, 2.1333, 2.0794, 1.9529, 1.892, 1.8659, 1.7838, 1.8248]\n",
      "2021-09-02 18:53:46,023 - INFO -   |-- [Party 5] Average Train Loss:   2.1073 Train Accuracy: global   9.71% local  35.43% ...    0 forgettables out of 1668 ( 0.00%) ... total data used 1668\n",
      "2021-09-02 18:53:46,026 - INFO -   |--    Epoch Losses (10): [2.3088, 2.2402, 2.1492, 2.1462, 2.1268, 2.0695, 2.0371, 2.0432, 1.9378, 2.0142]\n",
      "2021-09-02 18:53:59,268 - INFO -   |-- [Party 6] Average Train Loss:   1.9256 Train Accuracy: global   9.15% local  44.74% ...    0 forgettables out of 1596 ( 0.00%) ... total data used 1596\n",
      "2021-09-02 18:53:59,270 - INFO -   |--    Epoch Losses (10): [2.3076, 2.2526, 2.1028, 1.9323, 1.8722, 1.8483, 1.8326, 1.7763, 1.6965, 1.6348]\n",
      "2021-09-02 18:54:19,413 - INFO -   |-- [Party 7] Average Train Loss:   1.8380 Train Accuracy: global  10.18% local  46.69% ...    0 forgettables out of 2131 ( 0.00%) ... total data used 2131\n",
      "2021-09-02 18:54:19,417 - INFO -   |--    Epoch Losses (10): [2.3175, 2.1763, 1.9798, 1.8455, 1.7646, 1.7341, 1.7127, 1.6296, 1.6276, 1.5922]\n",
      "2021-09-02 18:54:27,770 - INFO -   |-- [Party 8] Average Train Loss:   1.9503 Train Accuracy: global   9.41% local  48.52% ...    0 forgettables out of 1084 ( 0.00%) ... total data used 1084\n",
      "2021-09-02 18:54:27,772 - INFO -   |--    Epoch Losses (10): [2.3349, 2.2133, 2.1755, 2.0208, 1.9447, 1.9019, 1.7882, 1.7422, 1.6785, 1.7025]\n",
      "2021-09-02 18:54:44,262 - INFO -   |-- [Party 9] Average Train Loss:   1.7903 Train Accuracy: global   9.00% local  49.85% ...    0 forgettables out of 2034 ( 0.00%) ... total data used 2034\n",
      "2021-09-02 18:54:44,264 - INFO -   |--    Epoch Losses (10): [2.1932, 2.0937, 1.9294, 1.7888, 1.7461, 1.6614, 1.6437, 1.6744, 1.6127, 1.5592]\n",
      "2021-09-02 18:54:53,793 - INFO -     |---- Number of Forgettables: 0 (0.00%)\n",
      "2021-09-02 18:54:53,795 - INFO -     |---- Train Accuracy: 17.92%\n",
      "2021-09-02 18:54:53,796 - INFO -     |---- Test Accuracy: 19.33%\n",
      "2021-09-02 18:54:53,797 - INFO -     |---- Test Loss: 2.1112\n",
      "2021-09-02 18:54:53,798 - INFO -     |---- Elapsed time: 0:02:15.069281\n",
      "2021-09-02 18:54:53,799 - INFO - \n",
      "Test Acc: Max 19.3300% (1 round) | Last 19.3300% | Avg 19.3300% (1 round)\n",
      "\n",
      "2021-09-02 18:54:53,802 - INFO -  | Global Training Round : 2 / 200 |\n",
      "2021-09-02 18:55:08,702 - INFO -   |-- [Party 1] Average Train Loss:   1.6448 Train Accuracy: global  19.57% local  46.56% ...  668 forgettables out of 1819 (36.72%) ... total data used 1819\n",
      "2021-09-02 18:55:08,706 - INFO -   |--    Epoch Losses (10): [2.0165, 1.7935, 1.7027, 1.6268, 1.6228, 1.5393, 1.5374, 1.5768, 1.4753, 1.5572]\n",
      "2021-09-02 18:55:17,760 - INFO -   |-- [Party 2] Average Train Loss:   1.6288 Train Accuracy: global  18.96% local  50.79% ...  281 forgettables out of 1081 (25.99%) ... total data used 1081\n",
      "2021-09-02 18:55:17,765 - INFO -   |--    Epoch Losses (10): [2.0281, 1.8845, 1.8083, 1.6875, 1.5789, 1.5446, 1.4274, 1.4534, 1.4383, 1.4372]\n",
      "2021-09-02 18:55:40,836 - INFO -   |-- [Party 3] Average Train Loss:   1.6056 Train Accuracy: global  17.87% local  48.56% ... 1022 forgettables out of 2630 (38.86%) ... total data used 2630\n",
      "2021-09-02 18:55:40,838 - INFO -   |--    Epoch Losses (10): [1.9463, 1.7692, 1.6429, 1.5603, 1.5321, 1.5326, 1.5221, 1.5211, 1.5127, 1.5169]\n",
      "2021-09-02 18:55:48,250 - INFO -   |-- [Party 4] Average Train Loss:   1.7212 Train Accuracy: global  18.25% local  36.17% ...  281 forgettables out of  915 (30.71%) ... total data used  915\n",
      "2021-09-02 18:55:48,251 - INFO -   |--    Epoch Losses (10): [1.9776, 1.9856, 1.8451, 1.7882, 1.7177, 1.6437, 1.5277, 1.6512, 1.5745, 1.5011]\n",
      "2021-09-02 18:56:01,541 - INFO -   |-- [Party 5] Average Train Loss:   1.7579 Train Accuracy: global  17.39% local  45.02% ...  453 forgettables out of 1668 (27.16%) ... total data used 1668\n",
      "2021-09-02 18:56:01,542 - INFO -   |--    Epoch Losses (10): [1.8866, 1.973, 1.8719, 1.7968, 1.822, 1.739, 1.6659, 1.6266, 1.6245, 1.5728]\n",
      "2021-09-02 18:56:14,190 - INFO -   |-- [Party 6] Average Train Loss:   1.6056 Train Accuracy: global  17.29% local  47.99% ...  600 forgettables out of 1596 (37.59%) ... total data used 1596\n",
      "2021-09-02 18:56:14,191 - INFO -   |--    Epoch Losses (10): [1.9899, 1.8505, 1.7025, 1.6218, 1.5626, 1.513, 1.473, 1.4345, 1.4272, 1.4807]\n",
      "2021-09-02 18:56:31,810 - INFO -   |-- [Party 7] Average Train Loss:   1.5755 Train Accuracy: global  18.96% local  50.02% ...  834 forgettables out of 2131 (39.14%) ... total data used 2131\n",
      "2021-09-02 18:56:31,812 - INFO -   |--    Epoch Losses (10): [2.0024, 1.7722, 1.5952, 1.5531, 1.5257, 1.5343, 1.4869, 1.4285, 1.4286, 1.4282]\n",
      "2021-09-02 18:56:40,660 - INFO -   |-- [Party 8] Average Train Loss:   1.5631 Train Accuracy: global  17.34% local  51.38% ...  433 forgettables out of 1084 (39.94%) ... total data used 1084\n",
      "2021-09-02 18:56:40,661 - INFO -   |--    Epoch Losses (10): [1.8846, 1.8166, 1.7141, 1.6284, 1.5301, 1.4673, 1.4303, 1.3864, 1.3725, 1.4004]\n",
      "2021-09-02 18:56:56,952 - INFO -   |-- [Party 9] Average Train Loss:   1.5329 Train Accuracy: global  17.94% local  56.15% ...  811 forgettables out of 2034 (39.87%) ... total data used 2034\n",
      "2021-09-02 18:56:56,953 - INFO -   |--    Epoch Losses (10): [1.9498, 1.7925, 1.5794, 1.516, 1.4919, 1.4129, 1.4276, 1.4397, 1.3753, 1.3439]\n",
      "2021-09-02 18:57:06,212 - INFO -     |---- Number of Forgettables: 5383 (35.99%)\n",
      "2021-09-02 18:57:06,215 - INFO -     |---- Train Accuracy: 54.10%\n",
      "2021-09-02 18:57:06,216 - INFO -     |---- Test Accuracy: 53.34%\n",
      "2021-09-02 18:57:06,217 - INFO -     |---- Test Loss: 1.2820\n",
      "2021-09-02 18:57:06,217 - INFO -     |---- Elapsed time: 0:04:27.488684\n",
      "2021-09-02 18:57:06,218 - INFO - \n",
      "Test Acc: Max 53.3400% (2 round) | Last 53.3400% | Avg 36.3350% (2 round)\n",
      "\n",
      "2021-09-02 18:57:06,220 - INFO -  | Global Training Round : 3 / 200 |\n",
      "2021-09-02 18:57:22,486 - INFO -   |-- [Party 1] Average Train Loss:   1.3604 Train Accuracy: global  53.22% local  59.76% ...  279 forgettables out of 1819 (15.34%) ... total data used 1819\n",
      "2021-09-02 18:57:22,488 - INFO -   |--    Epoch Losses (10): [1.6033, 1.4642, 1.3982, 1.3834, 1.3579, 1.3029, 1.269, 1.2703, 1.2735, 1.2808]\n",
      "2021-09-02 18:57:30,819 - INFO -   |-- [Party 2] Average Train Loss:   1.3331 Train Accuracy: global  53.28% local  64.20% ...  159 forgettables out of 1081 (14.71%) ... total data used 1081\n",
      "2021-09-02 18:57:30,820 - INFO -   |--    Epoch Losses (10): [1.5351, 1.5504, 1.4634, 1.3934, 1.3147, 1.2453, 1.2013, 1.2122, 1.2087, 1.2065]\n",
      "2021-09-02 18:57:51,083 - INFO -   |-- [Party 3] Average Train Loss:   1.3414 Train Accuracy: global  53.54% local  51.48% ...  381 forgettables out of 2630 (14.49%) ... total data used 2630\n",
      "2021-09-02 18:57:51,087 - INFO -   |--    Epoch Losses (10): [1.5698, 1.4209, 1.3727, 1.3404, 1.3471, 1.3228, 1.269, 1.2705, 1.2492, 1.2519]\n",
      "2021-09-02 18:57:58,343 - INFO -   |-- [Party 4] Average Train Loss:   1.3753 Train Accuracy: global  51.58% local  61.09% ...  113 forgettables out of  915 (12.35%) ... total data used  915\n",
      "2021-09-02 18:57:58,344 - INFO -   |--    Epoch Losses (10): [1.5971, 1.5319, 1.4848, 1.4008, 1.3507, 1.3134, 1.2988, 1.2768, 1.2624, 1.2362]\n",
      "2021-09-02 18:58:11,671 - INFO -   |-- [Party 5] Average Train Loss:   1.5495 Train Accuracy: global  52.82% local  50.00% ...  196 forgettables out of 1668 (11.75%) ... total data used 1668\n",
      "2021-09-02 18:58:11,672 - INFO -   |--    Epoch Losses (10): [1.6004, 1.6041, 1.5111, 1.481, 1.4507, 1.6261, 1.5332, 1.5397, 1.5599, 1.5891]\n",
      "2021-09-02 18:58:25,219 - INFO -   |-- [Party 6] Average Train Loss:   1.3513 Train Accuracy: global  53.07% local  51.69% ...  238 forgettables out of 1596 (14.91%) ... total data used 1596\n",
      "2021-09-02 18:58:25,223 - INFO -   |--    Epoch Losses (10): [1.6393, 1.5126, 1.4154, 1.3961, 1.3168, 1.3099, 1.2316, 1.2364, 1.1845, 1.2701]\n",
      "2021-09-02 18:58:44,221 - INFO -   |-- [Party 7] Average Train Loss:   1.3198 Train Accuracy: global  55.23% local  55.84% ...  256 forgettables out of 2131 (12.01%) ... total data used 2131\n",
      "2021-09-02 18:58:44,224 - INFO -   |--    Epoch Losses (10): [1.6098, 1.4158, 1.3228, 1.2979, 1.2812, 1.2544, 1.2721, 1.2536, 1.2687, 1.2213]\n",
      "2021-09-02 18:58:52,727 - INFO -   |-- [Party 8] Average Train Loss:   1.2834 Train Accuracy: global  54.15% local  58.67% ...  170 forgettables out of 1084 (15.68%) ... total data used 1084\n",
      "2021-09-02 18:58:52,729 - INFO -   |--    Epoch Losses (10): [1.6417, 1.4679, 1.3243, 1.2642, 1.2636, 1.1788, 1.1726, 1.1996, 1.1764, 1.1449]\n",
      "2021-09-02 18:59:08,662 - INFO -   |-- [Party 9] Average Train Loss:   1.2434 Train Accuracy: global  55.80% local  63.03% ...  273 forgettables out of 2034 (13.42%) ... total data used 2034\n",
      "2021-09-02 18:59:08,663 - INFO -   |--    Epoch Losses (10): [1.5276, 1.3174, 1.2723, 1.2474, 1.248, 1.1442, 1.1499, 1.2174, 1.1661, 1.1439]\n",
      "2021-09-02 18:59:17,396 - INFO -     |---- Number of Forgettables: 2065 (13.81%)\n",
      "2021-09-02 18:59:17,399 - INFO -     |---- Train Accuracy: 62.49%\n",
      "2021-09-02 18:59:17,401 - INFO -     |---- Test Accuracy: 61.02%\n",
      "2021-09-02 18:59:17,402 - INFO -     |---- Test Loss: 1.0960\n",
      "2021-09-02 18:59:17,403 - INFO -     |---- Elapsed time: 0:06:38.674333\n",
      "2021-09-02 18:59:17,405 - INFO - \n",
      "Test Acc: Max 61.0200% (3 round) | Last 61.0200% | Avg 44.5633% (2 round)\n",
      "\n",
      "2021-09-02 18:59:17,409 - INFO -  | Global Training Round : 4 / 200 |\n",
      "2021-09-02 18:59:32,596 - INFO -   |-- [Party 1] Average Train Loss:   1.1772 Train Accuracy: global  61.68% local  62.45% ...  245 forgettables out of 1819 (13.47%) ... total data used 1819\n",
      "2021-09-02 18:59:32,597 - INFO -   |--    Epoch Losses (10): [1.3731, 1.2769, 1.23, 1.1602, 1.1244, 1.1119, 1.1292, 1.1032, 1.1435, 1.1201]\n",
      "2021-09-02 18:59:41,345 - INFO -   |-- [Party 2] Average Train Loss:   1.1070 Train Accuracy: global  62.90% local  64.66% ...  169 forgettables out of 1081 (15.63%) ... total data used 1081\n",
      "2021-09-02 18:59:41,346 - INFO -   |--    Epoch Losses (10): [1.405, 1.2716, 1.1706, 1.0598, 1.1304, 1.0639, 1.0375, 1.0155, 0.9801, 0.9361]\n",
      "2021-09-02 19:00:02,099 - INFO -   |-- [Party 3] Average Train Loss:   1.1980 Train Accuracy: global  61.10% local  64.71% ...  285 forgettables out of 2630 (10.84%) ... total data used 2630\n",
      "2021-09-02 19:00:02,101 - INFO -   |--    Epoch Losses (10): [1.3918, 1.241, 1.2093, 1.1438, 1.2191, 1.1714, 1.1074, 1.1595, 1.1693, 1.1679]\n",
      "2021-09-02 19:00:09,862 - INFO -   |-- [Party 4] Average Train Loss:   1.1965 Train Accuracy: global  62.95% local  70.38% ...  114 forgettables out of  915 (12.46%) ... total data used  915\n",
      "2021-09-02 19:00:09,863 - INFO -   |--    Epoch Losses (10): [1.3439, 1.4529, 1.3432, 1.2506, 1.2099, 1.1496, 1.0709, 1.0463, 1.0605, 1.0375]\n",
      "2021-09-02 19:00:23,497 - INFO -   |-- [Party 5] Average Train Loss:   1.4447 Train Accuracy: global  64.21% local  51.32% ...  131 forgettables out of 1668 ( 7.85%) ... total data used 1668\n",
      "2021-09-02 19:00:23,498 - INFO -   |--    Epoch Losses (10): [1.4832, 1.4988, 1.4729, 1.4187, 1.3365, 1.2709, 1.4596, 1.5676, 1.4651, 1.4738]\n",
      "2021-09-02 19:00:37,682 - INFO -   |-- [Party 6] Average Train Loss:   1.1601 Train Accuracy: global  62.59% local  65.48% ...  163 forgettables out of 1596 (10.21%) ... total data used 1596\n",
      "2021-09-02 19:00:37,683 - INFO -   |--    Epoch Losses (10): [1.4687, 1.2984, 1.1741, 1.1739, 1.1352, 1.051, 1.0604, 1.1007, 1.0544, 1.0839]\n"
     ]
    }
   ],
   "source": [
    "rounds = 200\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "wdecay = 0\n",
    "\n",
    "logger.info(f'\\nAlgorithm: FedProx\\nClient Split: 9\\nDataset: {dataset}\\nModel: ResNet9 | Rounds: {rounds} | Epochs: {epochs} | LR: {lr}\\n')\n",
    "\n",
    "train_accs, train_losses, test_accs, test_losses = [], [], [], []\n",
    "match_history, round_forget_history = {}, []\n",
    "forget_cnt_per_client = [0] * len(indices.keys())\n",
    "\n",
    "st = time.time()\n",
    "for r in range(rounds):\n",
    "    \n",
    "    round_forget, round_samples = 0, 0\n",
    "    forget_history, forgettables = {}, {}\n",
    "    local_weights, local_losses = [], []\n",
    "    logger.info(f' | Global Training Round : {r + 1} / {rounds} |')\n",
    "    \n",
    "    for i, k in enumerate(indices.keys()):\n",
    "        match_history, forgettables, global_acc = infer_train(fed_model, inferloaders[k], device, match_history, flag=True)\n",
    "\n",
    "        fed_model.train()\n",
    "        \n",
    "        sampler_idx = indices[k]['index'].copy()\n",
    "        sampler = SubsetRandomSampler(sampler_idx)\n",
    "\n",
    "        trainloader = DataLoader(custom_dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
    "        \n",
    "        local_model = copy.deepcopy(fed_model)\n",
    "        w, ls = train(local_model, trainloader, epochs, lr, wdecay, criterion, device, fed_model, mu=0.01)\n",
    "\n",
    "        local_weights.append(copy.deepcopy(w))\n",
    "        train_losses.append(ls)\n",
    "        \n",
    "        match_history, _, local_acc = infer_train(local_model, inferloaders[k], device, match_history, flag=False)\n",
    "        train_accs.append(local_acc)\n",
    "        \n",
    "        forget_cnt = len(forgettables)\n",
    "        round_forget += forget_cnt\n",
    "        forget_cnt_per_client[int(k)] = forget_cnt\n",
    "        round_samples += len(indices[k]['index'])\n",
    "        \n",
    "        logger.info('  |-- [Party {:>1}] Average Train Loss: {:>8.4f} Train Accuracy: global {:>6.2f}% local {:>6.2f}% ... {:>4} forgettables out of {:>4} ({:>5.2f}%) ... total data used {:>4}'.format(\n",
    "            i + 1, sum(ls) / len(ls), 100 * global_acc, 100 * local_acc, forget_cnt, len(indices[k]['index']), 100 * forget_cnt / len(indices[k]['index']), len(sampler_idx)\n",
    "        ))\n",
    "        logger.info('  |--    Epoch Losses ({:>2}): {}'.format(epochs, formatfloats(ls)))\n",
    "        \n",
    "    fed_weights = average_weights(local_weights)\n",
    "    fed_model.load_state_dict(fed_weights)\n",
    "    train_acc, _ = inference(fed_model, fed_trainloader, criterion, device)\n",
    "    \n",
    "    if (r + 1) % 50 == 0:\n",
    "        torch.save(fed_model.state_dict(), os.path.join(path_models, filename + f'_round{r+1}.pth'))\n",
    "    \n",
    "    test_acc, test_ls = inference(fed_model, testloader, criterion, device)\n",
    "    test_accs.append(test_acc)\n",
    "    test_losses.append(test_ls)\n",
    "    round_forget_history.append(round_forget)\n",
    "    logger.info('    |---- Number of Forgettables: {} ({:.2f}%)'.format(round_forget, 100 * round_forget / round_samples))\n",
    "    logger.info('    |---- Train Accuracy: {:>.2f}%'.format(100 * train_acc))\n",
    "    logger.info('    |---- Test Accuracy: {:>.2f}%'.format(100 * test_acc))\n",
    "    logger.info('    |---- Test Loss: {:.4f}'.format(test_ls))\n",
    "    logger.info('    |---- Elapsed time: {}'.format(timedelta(seconds=time.time()-st)))\n",
    "    logger.info(f'\\nTest Acc: Max {np.max(test_acc) * 100:.4f}% ({np.argmax(test_accs)+1} round) | Last {test_accs[-1] * 100:.4f}% | Avg {np.mean(test_accs) * 100:.4f}% ({np.argmax(test_accs > np.mean(test_accs))+1} round)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654dbc03-4dd8-4c75-932e-f9f842c9ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = np.asarray(train_losses)\n",
    "train_accs = np.asarray(train_accs)\n",
    "\n",
    "with open(os.path.join(path_results, f'{filename}_tr_ls.npy'), 'wb') as f:\n",
    "    np.save(f, train_losses)\n",
    "with open(os.path.join(path_results, f'{filename}_tr_acc.npy'), 'wb') as f:\n",
    "    np.save(f, train_accs)\n",
    "with open(os.path.join(path_results, f'{filename}_te_ls.npy'), 'wb') as f:\n",
    "    np.save(f, test_losses)\n",
    "with open(os.path.join(path_results, f'{filename}_te_acc.npy'), 'wb') as f:\n",
    "    np.save(f, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e644234-224e-4353-888c-01affc417a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(50, 30))\n",
    "axs = axs.ravel()\n",
    "\n",
    "axs[0].plot(test_accs, c='orange')\n",
    "axs[0].set_title('Test Accuracies')\n",
    "axs[0].set_xlabel('Rounds')\n",
    "axs[0].set_ylabel('Test Accuracy')\n",
    "axs[1].plot(test_losses, c='blue')\n",
    "axs[1].set_title('Test Losses')\n",
    "axs[1].set_xlabel('Rounds')\n",
    "axs[1].set_ylabel('Test Loss')\n",
    "axs[2].plot(train_accs, c='red')\n",
    "axs[2].set_title('Train Average Accuracies by Epochs')\n",
    "axs[2].set_xlabel('Epochs')\n",
    "axs[2].set_ylabel('Train Average Accuracy')\n",
    "axs[3].plot(train_losses.mean(axis=1), c='turquoise')\n",
    "axs[3].set_title('Train Average Losses by Epochs')\n",
    "axs[3].set_xlabel('Epochs')\n",
    "axs[3].set_ylabel('Train Average Loss')\n",
    "axs[4].plot(np.mean(train_accs.reshape(-1, 10), axis=1), c='green')\n",
    "axs[4].set_title('Train Average Accuracies by Rounds')\n",
    "axs[4].set_xlabel('Rounds')\n",
    "axs[4].set_ylabel('Train Average Accuracy')\n",
    "axs[5].plot(train_losses.mean(axis=1).reshape(-1, 10).mean(axis=1), c='lightpink')\n",
    "axs[5].set_title('Train Average Losses by Rounds')\n",
    "axs[5].set_xlabel('Rounds')\n",
    "axs[5].set_ylabel('Train Average Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9a5cc-41d2-41f1-a4cd-114d63d3b2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35825f4-b26d-4ee5-8914-d28fdc8bc1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "dataset = 'mnist'\n",
    "num_clients = 100\n",
    "\n",
    "filename = '{}_diri{}a01_42_fedavg_ep'.format(dataset, num_clients)\n",
    "\n",
    "gpu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8447bc5c-393a-4f8d-80fd-50a652a7a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, Subset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "148a5cb1-99a9-4d54-8239-3ae733854155",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    torch.use_deterministic_algorithms(False)\n",
    "except AttributeError:\n",
    "    torch.set_deterministic(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5090116-fe54-40fe-8459-e9ce9aea270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = os.path.dirname(os.getcwd())\n",
    "path_data = os.path.join(path_root, 'data')\n",
    "path_logs = os.path.join(path_root, 'logs')\n",
    "path_models = os.path.join(path_root, 'models', filename)\n",
    "path_results = os.path.join(path_root, 'results', filename)\n",
    "\n",
    "for p in [path_data, path_logs, path_models, path_results]:\n",
    "    os.makedirs(p, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e4f24c-2bab-456f-9897-fb7cf4a72efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(filename)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "streamformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "streamhandler = logging.StreamHandler()\n",
    "streamhandler.setFormatter(streamformatter)\n",
    "logger.addHandler(streamhandler)\n",
    "\n",
    "fileformatter = logging.Formatter('%(message)s')\n",
    "filehandler = logging.FileHandler(os.path.join(path_logs, filename + '.log'), mode='w')\n",
    "filehandler.setFormatter(fileformatter)\n",
    "logger.addHandler(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4543504-728a-4bbf-8ebd-a70f223288d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, target = self.dataset[idx]\n",
    "        return data, target, idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0859e906-dbe3-4c6f-a44d-3ce50555d228",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'cifar10':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([x / 255 for x in [125.3, 123, 113.9]], [x / 255 for x in [63, 62.1, 66.7]])\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([x / 255 for x in [125.3, 123, 113.9]], [x / 255 for x in [63, 62.1, 66.7]])\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.CIFAR10(path_data, train=False, transform=test_transform)\n",
    "    \n",
    "elif dataset == 'svhn':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.SVHN(path_data, split='train', transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.SVHN(path_data, split='test', transform=test_transform, download=True)\n",
    "    \n",
    "elif dataset == 'fmnist':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.FashionMNIST(path_data, train=False, transform=test_transform)\n",
    "\n",
    "elif dataset == 'mnist':\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST(path_data, train=True, transform=train_transform, download=True)\n",
    "    custom_dataset = CustomDataset(train_dataset)\n",
    "    test_dataset = datasets.MNIST(path_data, train=False, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6182a3f4-472d-4246-9ae0-e184be956dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_data, f'{dataset}_diri{num_clients}a01_42.json')) as f:\n",
    "    indices = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02409155-7c8a-4217-8258-ccb1a2c7e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_workers = 128, 0\n",
    "\n",
    "inferloaders, subset_indices = {}, []\n",
    "for k, v in indices.items():\n",
    "    infersubset = Subset(custom_dataset, v['index'])\n",
    "    inferloaders[k] = DataLoader(infersubset, batch_size=batch_size, num_workers=num_workers)\n",
    "    subset_indices.extend(v['index'])\n",
    "\n",
    "train_subset = Subset(train_dataset, indices=subset_indices)\n",
    "fed_trainloader = DataLoader(train_subset, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "try:\n",
    "    train_labels = np.asarray(custom_dataset.dataset.targets)\n",
    "    test_labels = np.asarray(test_dataset.targets)\n",
    "except AttributeError:\n",
    "    train_labels = np.asarray(custom_dataset.dataset.labels)\n",
    "    test_labels = np.asarray(test_dataset.labels)\n",
    "subset_classes = np.unique(train_labels[subset_indices])\n",
    "boolarr = [True if y in subset_classes else False for y in test_labels]\n",
    "subset_indices = np.arange(len(test_dataset))[boolarr]\n",
    "test_subset = Subset(test_dataset, indices=subset_indices)\n",
    "testloader = DataLoader(test_subset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dd80881-9cee-4d1c-8348-aaf4eacc8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResidualBlock(nn.Module):\n",
    "#     expansion = 1\n",
    "\n",
    "#     def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, padding=1, bias=False):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "#         self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "#         self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=kernel_size, stride=1, padding=padding, bias=bias)\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "#         self.shortcut = nn.Sequential()\n",
    "#         if (stride != 1) or (in_channel != self.expansion * out_channel):\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 nn.Conv2d(in_channel, self.expansion * out_channel, kernel_size=1, stride=stride, bias=bias),\n",
    "#                 nn.BatchNorm2d(self.expansion * out_channel)\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.bn1(self.conv1(x)))\n",
    "#         out = self.bn2(self.conv2(out))\n",
    "#         out += self.shortcut(x)\n",
    "#         out = F.relu(out)\n",
    "#         return out\n",
    "\n",
    "# class Bottleneck(nn.Module):\n",
    "#     expansion = 4\n",
    "    \n",
    "#     def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, padding=1, bias=False):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=1, bias=bias)\n",
    "#         self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "#         self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "#         self.conv3 = nn.Conv2d(out_channel, self.expansion * out_channel, kerenel_size=1, bias=bias)\n",
    "#         self.bn3 = nn.BatchNorm2d(self.expansion * out_channel)\n",
    "\n",
    "#         self.shortcut = nn.Sequential()\n",
    "#         if (stride != 1) or (in_channel != self.expansion * out_channel):\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 nn.Conv2d(in_channel, self.expansion * out_channel, kernel_size=1, stride=stride, bias=bias),\n",
    "#                 nn.BatchNorm2d(self.expansion * out_channel)\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.bn1(self.conv1(x)))\n",
    "#         out = F.relu(self.bn2(self.conv2(out)))\n",
    "#         out = self.bn3(self.conv2(out))\n",
    "#         out += self.shortcut(x)\n",
    "#         out = F.relu(out)\n",
    "#         return out\n",
    "    \n",
    "# class ResNet(nn.Module):\n",
    "#     def __init__(self, block, in_channel=3, out_channels=[64, 128, 256, 512], num_blocks=[2, 2, 2, 2], strides=[1, 2, 2, 2], num_classes=10):\n",
    "#         super().__init__()\n",
    "#         self.in_channel = out_channels[0]\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(in_channel, out_channels[0], kernel_size=3, stride=strides[0], padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(out_channels[0])\n",
    "        \n",
    "#         self.block1 = self._make_layer(block, out_channels[0], num_blocks[0], strides[0])\n",
    "#         self.block2 = self._make_layer(block, out_channels[1], num_blocks[1], strides[1])\n",
    "#         self.block3 = self._make_layer(block, out_channels[2], num_blocks[2], strides[2])\n",
    "#         self.block4 = self._make_layer(block, out_channels[3], num_blocks[3], strides[3])\n",
    "        \n",
    "#         self.linear = nn.Linear(out_channels[-1] * block.expansion, num_classes)\n",
    "\n",
    "#     def _make_layer(self, block, out_channel, num_blocks, stride):\n",
    "#         strides = [stride] + [1] * (num_blocks - 1)\n",
    "#         layers = []\n",
    "#         for stride in strides:\n",
    "#             layers.append(block(self.in_channel, out_channel, stride=stride))\n",
    "#             self.in_channel = out_channel * block.expansion\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.bn1(self.conv1(x)))\n",
    "#         out = self.block1(out)\n",
    "#         out = self.block2(out)\n",
    "#         out = self.block3(out)\n",
    "#         out = self.block4(out)\n",
    "#         out = F.avg_pool2d(out, 4)\n",
    "#         out = out.view(out.size(0), -1)\n",
    "#         out = self.linear(out)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8998a15-f414-43ff-8a37-4276a13f07ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_feature=784, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_feature, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4093b5ac-d61b-4172-8544-053d53f4b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, in_channel=3, num_classes=10):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channel, 64, 5)\n",
    "#         self.conv2 = nn.Conv2d(64, 64, 5)\n",
    "#         self.fc1 = nn.Linear(64 * 5 * 5, 384)\n",
    "#         self.fc2 = nn.Linear(384, 192)\n",
    "#         self.fc3 = nn.Linear(192, num_classes)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         out = F.max_pool2d(F.relu(self.conv1(x)), 2, 2)\n",
    "#         out = F.max_pool2d(F.relu(self.conv2(out)), 2, 2)\n",
    "#         out = out.view(-1, 64 * 5 * 5)\n",
    "#         out = F.relu(self.fc1(out))\n",
    "#         out = F.relu(self.fc2(out))\n",
    "#         out = self.fc3(out)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1666ec7e-4866-47c4-9827-905d1bfb9ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_avg(w):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    \n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], float(len(w)))\n",
    "        \n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f232cb6d-451b-4973-a9bd-92afb2ac1451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simplenorm(x):\n",
    "#     return x / np.sum(x)\n",
    "\n",
    "# def weighting(forget_cnt_per_client, standardize=0.1):\n",
    "#     if (standardize == 0.0) or (np.sum(forget_cnt_per_client) == 0):\n",
    "#         weights = np.ones(len(forget_cnt_per_client))\n",
    "#     else:\n",
    "#         weights = 1 - standardize + len(forget_cnt_per_client) * standardize * simplenorm(forget_cnt_per_client)\n",
    "#     return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae0fe2f3-5ec9-46fe-b29c-8db8b81089ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_wavg(w, standardized_weights):\n",
    "#     w_avg = copy.deepcopy(w[0])\n",
    "    \n",
    "#     w_avg.update((k, v * standardized_weights[0]) for k, v in w_avg.items())\n",
    "    \n",
    "#     for key in w_avg.keys():\n",
    "#         for i in range(1, len(w)):\n",
    "#             w_avg[key] += (w[i][key] * standardized_weights[i])\n",
    "#         w_avg[key] = torch.div(w_avg[key], float(len(w)))\n",
    "        \n",
    "#     return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "611e3c0f-9f0e-419a-aa1f-a497b671aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, epochs, lr, weight_decay, criterion, device):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        batch_losses = []\n",
    "        \n",
    "        for inputs, labels, _ in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss.mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "        epoch_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "        \n",
    "    local_weights = model.state_dict()\n",
    "\n",
    "    return local_weights, epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d5c54db-1ba4-4f65-817d-fb8b2b28fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, loader, criterion, device):\n",
    "    avg_loss, correct, num_samples = 0, 0, 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss.mean()\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += torch.sum(torch.eq(predicted, labels)).item()\n",
    "            num_samples += len(labels)\n",
    "\n",
    "    acc = correct / num_samples\n",
    "    avg_loss /= len(loader)\n",
    "    \n",
    "    return acc, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "629ff8ea-9394-4a48-a5e0-1a90d329a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_train(model, loader, device, match_history, flag=False):\n",
    "    forgettables = []\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, indices in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            match = predicted.eq(labels)\n",
    "            match = match.type(torch.IntTensor)\n",
    "            total += len(indices)\n",
    "            \n",
    "            for j, idx in enumerate(indices):\n",
    "                sample_history = match_history.get(idx.item(), [])\n",
    "                sample_history.append(match[j].item())\n",
    "                match_history[idx.item()] = sample_history\n",
    "                correct += match[j].item()\n",
    "                if flag is True:\n",
    "                    try:\n",
    "                        if match[j].item() - sample_history[-2] == -1:\n",
    "                            forgettables.append(idx.item())\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "            \n",
    "    acc = correct / total\n",
    "            \n",
    "    return match_history, forgettables, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7562c7c7-4368-442b-8bd7-fa1674a6842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatfloats(li):\n",
    "    new = [float(f'{e:>8.4f}') for e in li]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc4a3691-7135-496c-9153-23e7fc4f1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet9 = {'block': ResidualBlock, 'num_blocks': [1, 1, 1, 1]}\n",
    "# resnet18 = {'block': ResidualBlock, 'num_blocks': [2, 2, 2, 2]}\n",
    "# resnet34 = {'block': ResidualBlock, 'num_blocks': [3, 4, 6, 3]}\n",
    "# resnet50 = {'block': Bottleneck, 'num_blocks': [3, 4, 6, 3]}\n",
    "# resnet101 = {'block': Bottleneck, 'num_blocks': [3, 4, 23, 3]}\n",
    "# resnet152 = {'block': Bottleneck, 'num_blocks': [3, 8, 36, 3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1261d399-3f2a-4f98-b585-81e5037a36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f'cuda:{gpu}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20a73dfb-ec2c-40fd-9893-3eac24fbca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "fed_model = MLP()\n",
    "fed_weights = fed_model.state_dict()\n",
    "\n",
    "fed_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c65d0203-4dd4-4c5b-9f57-5693890ec7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:05:07,404 - INFO - \n",
      "Algorithm: FedAvg Dynamic Epoch\n",
      "Clients: 100\n",
      "Dataset: mnist\n",
      "Model: MLP | Rounds: 1000 | Epochs: 10 | LR: 0.001\n",
      "\n",
      "2021-09-23 01:05:07,410 - INFO -  | Global Training Round : 1 / 1000 |\n",
      "2021-09-23 01:05:07,411 - INFO -  | Current Participants : [14, 20, 51, 60, 71, 74, 74, 82, 86, 92] |\n",
      "2021-09-23 01:05:07,412 - INFO -  |    Next Participants : [1, 2, 21, 23, 29, 37, 52, 87, 87, 99] |\n",
      "2021-09-23 01:05:10,598 - INFO -   |-- [Party  1] Training for forgettable counting in the next round\n",
      "2021-09-23 01:05:13,288 - INFO -   |-- [Party  2] Training for forgettable counting in the next round\n",
      "2021-09-23 01:05:15,971 - INFO -   |-- [Party 14] Average Train Loss:   0.7118 Train Accuracy: global  14.83% local  78.50% ...    0 forgettables out of  600 ( 0.00%) ... total data used  600\n",
      "2021-09-23 01:05:15,976 - INFO -   |--    Epoch Losses (10): [1.465, 0.7606, 0.7266, 0.6833, 0.639, 0.592, 0.5845, 0.5644, 0.5597, 0.5426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:05:18,528 - INFO -   |-- [Party 20] Average Train Loss:   0.9734 Train Accuracy: global   7.67% local  62.33% ...    0 forgettables out of  600 ( 0.00%) ... total data used  600\n",
      "2021-09-23 01:05:18,529 - INFO -   |--    Epoch Losses (10): [1.6949, 1.0947, 0.9721, 0.9122, 0.8864, 0.8673, 0.8558, 0.8161, 0.8366, 0.7983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:05:21,086 - INFO -   |-- [Party 21] Training for forgettable counting in the next round\n",
      "2021-09-23 01:05:23,632 - INFO -   |-- [Party 23] Training for forgettable counting in the next round\n",
      "2021-09-23 01:05:26,295 - INFO -   |-- [Party 29] Training for forgettable counting in the next round\n",
      "2021-09-23 01:05:28,957 - INFO -   |-- [Party 37] Training for forgettable counting in the next round\n",
      "2021-09-23 01:05:31,218 - INFO -   |-- [Party 51] Average Train Loss:   1.3379 Train Accuracy: global   0.00% local  63.33% ...    0 forgettables out of  600 ( 0.00%) ... total data used  600\n",
      "2021-09-23 01:05:31,220 - INFO -   |--    Epoch Losses (10): [2.0357, 1.554, 1.4501, 1.3679, 1.3132, 1.2396, 1.1694, 1.1429, 1.053, 1.0535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:05:33,441 - INFO -   |-- [Party 52] Training for forgettable counting in the next round\n",
      "2021-09-23 01:05:35,738 - INFO -   |-- [Party 60] Average Train Loss:   1.1755 Train Accuracy: global  17.50% local  65.33% ...    0 forgettables out of  600 ( 0.00%) ... total data used  600\n",
      "2021-09-23 01:05:35,741 - INFO -   |--    Epoch Losses (10): [1.6845, 1.3057, 1.2061, 1.1637, 1.1228, 1.1152, 1.0566, 1.0542, 1.0338, 1.0129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:05:37,971 - INFO -   |-- [Party 71] Average Train Loss:   0.6470 Train Accuracy: global  11.67% local  87.00% ...    0 forgettables out of  600 ( 0.00%) ... total data used  600\n",
      "2021-09-23 01:05:37,973 - INFO -   |--    Epoch Losses (10): [1.484, 0.7927, 0.7059, 0.6565, 0.5421, 0.5363, 0.4898, 0.4471, 0.425, 0.3901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:05:40,224 - INFO -   |-- [Party 74] Average Train Loss:   0.6190 Train Accuracy: global  37.50% local  85.83% ...    0 forgettables out of  600 ( 0.00%) ... total data used  600\n",
      "2021-09-23 01:05:40,225 - INFO -   |--    Epoch Losses (10): [1.2638, 0.7031, 0.5763, 0.5601, 0.5218, 0.5252, 0.5151, 0.5123, 0.5051, 0.5069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:05:42,386 - INFO -   |-- [Party 82] Average Train Loss:   0.9861 Train Accuracy: global  25.67% local  77.00% ...    0 forgettables out of  600 ( 0.00%) ... total data used  600\n",
      "2021-09-23 01:05:42,387 - INFO -   |--    Epoch Losses (10): [1.4475, 1.1044, 1.0293, 0.9623, 0.9474, 0.9285, 0.8895, 0.8752, 0.8531, 0.8239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:05:44,891 - INFO -   |-- [Party 86] Average Train Loss:   1.0572 Train Accuracy: global   2.17% local  69.67% ...    0 forgettables out of  600 ( 0.00%) ... total data used  600\n",
      "2021-09-23 01:05:44,893 - INFO -   |--    Epoch Losses (10): [1.8943, 1.1681, 1.1294, 1.0321, 0.9677, 0.9301, 0.897, 0.8576, 0.8628, 0.8332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:05:47,437 - INFO -   |-- [Party 87] Training for forgettable counting in the next round\n",
      "2021-09-23 01:05:49,987 - INFO -   |-- [Party 92] Average Train Loss:   0.3427 Train Accuracy: global   0.00% local  95.17% ...    0 forgettables out of  600 ( 0.00%) ... total data used  600\n",
      "2021-09-23 01:05:49,990 - INFO -   |--    Epoch Losses (10): [1.1386, 0.4112, 0.343, 0.2485, 0.2231, 0.2335, 0.2128, 0.2129, 0.2039, 0.1993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:05:52,523 - INFO -   |-- [Party 99] Training for forgettable counting in the next round\n",
      "2021-09-23 01:05:54,116 - INFO -     |---- Number of Forgettables: 0 (0.00%)\n",
      "2021-09-23 01:05:54,117 - INFO -     |---- Test Accuracy: 15.46%\n",
      "2021-09-23 01:05:54,119 - INFO -     |---- Test Loss: 2.3968\n",
      "2021-09-23 01:05:54,120 - INFO -     |---- Elapsed time: 0:00:46.711908\n",
      "2021-09-23 01:05:54,123 - INFO - \n",
      "Test Acc: Highest 15.4600% (1 round) | Avg 15.4600% (1 round) | Curr 15.4600%\n",
      "2021-09-23 01:05:54,124 - INFO -  | Global Training Round : 2 / 1000 |\n",
      "2021-09-23 01:05:54,125 - INFO -  | Current Participants : [1, 2, 21, 23, 29, 37, 52, 87, 87, 99] |\n",
      "2021-09-23 01:05:54,127 - INFO -  |    Next Participants : [1, 20, 21, 32, 48, 57, 59, 63, 75, 88] |\n",
      "2021-09-23 01:05:56,136 - INFO -   |-- [Party  1] Average Train Loss:   0.8204 Train Accuracy: global   0.17% local  80.17% ...  481 forgettables out of  600 (80.17%) ... total data used  600\n",
      "2021-09-23 01:05:56,138 - INFO -   |--    Epoch Losses ( 8): [1.7553, 0.7741, 0.7506, 0.7272, 0.6538, 0.6466, 0.6351, 0.6206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:05:58,268 - INFO -   |-- [Party  2] Average Train Loss:   0.4621 Train Accuracy: global   0.33% local  91.67% ...  548 forgettables out of  600 (91.33%) ... total data used  600\n",
      "2021-09-23 01:05:58,269 - INFO -   |--    Epoch Losses ( 9): [1.2624, 0.5375, 0.4409, 0.3847, 0.3593, 0.3387, 0.2978, 0.2934, 0.2438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:06:00,595 - INFO -   |-- [Party 20] Training for forgettable counting in the next round\n",
      "2021-09-23 01:06:02,072 - INFO -   |-- [Party 21] Average Train Loss:   1.3387 Train Accuracy: global   7.67% local  58.50% ...  353 forgettables out of  600 (58.83%) ... total data used  600\n",
      "2021-09-23 01:06:02,073 - INFO -   |--    Epoch Losses ( 5): [1.8624, 1.2826, 1.2437, 1.1725, 1.1325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:06:03,985 - INFO -   |-- [Party 23] Average Train Loss:   0.8866 Train Accuracy: global   0.50% local  78.17% ...  469 forgettables out of  600 (78.17%) ... total data used  600\n",
      "2021-09-23 01:06:03,987 - INFO -   |--    Epoch Losses ( 7): [1.6278, 1.0021, 0.8177, 0.7819, 0.7137, 0.647, 0.6159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:06:06,107 - INFO -   |-- [Party 29] Average Train Loss:   0.7478 Train Accuracy: global   0.00% local  82.67% ...  487 forgettables out of  600 (81.17%) ... total data used  600\n",
      "2021-09-23 01:06:06,109 - INFO -   |--    Epoch Losses ( 8): [1.7712, 0.7178, 0.689, 0.6412, 0.5851, 0.5475, 0.5209, 0.5096]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:06:08,642 - INFO -   |-- [Party 32] Training for forgettable counting in the next round\n",
      "2021-09-23 01:06:10,207 - INFO -   |-- [Party 37] Average Train Loss:   0.5370 Train Accuracy: global  52.33% local  92.50% ...  249 forgettables out of  600 (41.50%) ... total data used  600\n",
      "2021-09-23 01:06:10,209 - INFO -   |--    Epoch Losses ( 4): [0.8707, 0.4683, 0.4349, 0.374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:06:11,298 - INFO -   |-- [Party 48] Training for forgettable counting in the next round\n",
      "2021-09-23 01:06:13,256 - INFO -   |-- [Party 52] Average Train Loss:   0.6537 Train Accuracy: global   0.17% local  84.17% ...  493 forgettables out of  600 (82.17%) ... total data used  600\n",
      "2021-09-23 01:06:13,257 - INFO -   |--    Epoch Losses ( 8): [1.3311, 0.7384, 0.5975, 0.567, 0.5222, 0.5096, 0.482, 0.4822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:06:15,273 - INFO -   |-- [Party 57] Training for forgettable counting in the next round\n",
      "2021-09-23 01:06:17,500 - INFO -   |-- [Party 59] Training for forgettable counting in the next round\n",
      "2021-09-23 01:06:19,728 - INFO -   |-- [Party 63] Training for forgettable counting in the next round\n",
      "2021-09-23 01:06:21,911 - INFO -   |-- [Party 75] Training for forgettable counting in the next round\n",
      "2021-09-23 01:06:23,488 - INFO -   |-- [Party 87] Average Train Loss:   0.8408 Train Accuracy: global  15.50% local  79.17% ...  400 forgettables out of  600 (66.67%) ... total data used  600\n",
      "2021-09-23 01:06:23,489 - INFO -   |--    Epoch Losses ( 6): [1.3311, 0.8745, 0.7934, 0.7247, 0.6972, 0.6238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:06:24,926 - INFO -   |-- [Party 88] Training for forgettable counting in the next round\n",
      "2021-09-23 01:06:26,078 - INFO -   |-- [Party 99] Average Train Loss:   0.1815 Train Accuracy: global  50.83% local 100.00% ...  295 forgettables out of  600 (49.17%) ... total data used  600\n",
      "2021-09-23 01:06:26,079 - INFO -   |--    Epoch Losses ( 4): [0.7217, 0.0042, 0.0, 0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:06:27,737 - INFO -     |---- Number of Forgettables: 3775 (69.91%)\n",
      "2021-09-23 01:06:27,739 - INFO -     |---- Test Accuracy: 10.17%\n",
      "2021-09-23 01:06:27,740 - INFO -     |---- Test Loss: 2.8718\n",
      "2021-09-23 01:06:27,740 - INFO -     |---- Elapsed time: 0:01:20.331970\n",
      "2021-09-23 01:06:27,742 - INFO - \n",
      "Test Acc: Highest 15.4600% (1 round) | Avg 12.8150% (1 round) | Curr 10.1700%\n",
      "2021-09-23 01:06:27,743 - INFO -  | Global Training Round : 3 / 1000 |\n",
      "2021-09-23 01:06:27,744 - INFO -  | Current Participants : [1, 20, 21, 32, 48, 57, 59, 63, 75, 88] |\n",
      "2021-09-23 01:06:27,745 - INFO -  |    Next Participants : [14, 41, 46, 58, 59, 61, 61, 79, 90, 91] |\n",
      "2021-09-23 01:06:29,248 - INFO -   |-- [Party  1] Average Train Loss:   0.7238 Train Accuracy: global   9.83% local  80.17% ...  423 forgettables out of  600 (70.50%) ... total data used  600\n",
      "2021-09-23 01:06:29,250 - INFO -   |--    Epoch Losses ( 7): [1.1389, 0.7134, 0.6733, 0.6497, 0.6322, 0.6416, 0.6178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:06:30,821 - INFO -   |-- [Party 14] Training for forgettable counting in the next round\n",
      "2021-09-23 01:06:32,170 - INFO -   |-- [Party 20] Average Train Loss:   1.1301 Train Accuracy: global   1.50% local  62.17% ...  384 forgettables out of  600 (64.00%) ... total data used  600\n",
      "2021-09-23 01:06:32,172 - INFO -   |--    Epoch Losses ( 6): [2.0571, 1.1206, 0.9426, 0.9291, 0.8723, 0.859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:06:33,477 - INFO -   |-- [Party 21] Average Train Loss:   1.2511 Train Accuracy: global   2.00% local  58.67% ...  351 forgettables out of  600 (58.50%) ... total data used  600\n",
      "2021-09-23 01:06:33,479 - INFO -   |--    Epoch Losses ( 5): [1.6573, 1.2238, 1.174, 1.1194, 1.0808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:06:35,168 - INFO -   |-- [Party 32] Average Train Loss:   0.8408 Train Accuracy: global   2.67% local  79.00% ...  473 forgettables out of  600 (78.83%) ... total data used  600\n",
      "2021-09-23 01:06:35,170 - INFO -   |--    Epoch Losses ( 7): [1.5818, 0.826, 0.7837, 0.728, 0.684, 0.646, 0.6361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:06:36,814 - INFO -   |-- [Party 41] Training for forgettable counting in the next round\n",
      "2021-09-23 01:06:38,495 - INFO -   |-- [Party 46] Training for forgettable counting in the next round\n",
      "2021-09-23 01:06:40,151 - INFO -   |-- [Party 48] Average Train Loss:   0.8618 Train Accuracy: global   9.67% local  79.50% ...  427 forgettables out of  600 (71.17%) ... total data used  600\n",
      "2021-09-23 01:06:40,153 - INFO -   |--    Epoch Losses ( 7): [1.2383, 0.8947, 0.8303, 0.8013, 0.7686, 0.7565, 0.743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:06:41,799 - INFO -   |-- [Party 57] Average Train Loss:   1.1627 Train Accuracy: global   0.00% local  71.33% ...  435 forgettables out of  600 (72.50%) ... total data used  600\n",
      "2021-09-23 01:06:41,800 - INFO -   |--    Epoch Losses ( 7): [2.5621, 1.2554, 0.977, 0.8983, 0.8631, 0.8301, 0.7529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:06:43,474 - INFO -   |-- [Party 58] Training for forgettable counting in the next round\n",
      "2021-09-23 01:06:45,211 - INFO -   |-- [Party 59] Average Train Loss:   0.7509 Train Accuracy: global   8.67% local  83.00% ...  447 forgettables out of  600 (74.50%) ... total data used  600\n",
      "2021-09-23 01:06:45,212 - INFO -   |--    Epoch Losses ( 7): [1.1002, 0.7738, 0.7039, 0.6853, 0.6602, 0.6552, 0.6773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:06:47,128 - INFO -   |-- [Party 61] Training for forgettable counting in the next round\n",
      "2021-09-23 01:06:49,466 - INFO -   |-- [Party 63] Average Train Loss:   0.3140 Train Accuracy: global   0.00% local  98.67% ...  592 forgettables out of  600 (98.67%) ... total data used  600\n",
      "2021-09-23 01:06:49,468 - INFO -   |--    Epoch Losses ( 9): [1.9066, 0.1875, 0.1171, 0.1163, 0.1273, 0.1234, 0.0978, 0.0764, 0.0737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:06:51,389 - INFO -   |-- [Party 75] Average Train Loss:   1.1263 Train Accuracy: global   5.17% local  70.67% ...  463 forgettables out of  600 (77.17%) ... total data used  600\n",
      "2021-09-23 01:06:51,390 - INFO -   |--    Epoch Losses ( 7): [2.3419, 1.2356, 0.9714, 0.9029, 0.8619, 0.8062, 0.7642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:06:53,295 - INFO -   |-- [Party 79] Training for forgettable counting in the next round\n",
      "2021-09-23 01:06:55,217 - INFO -   |-- [Party 88] Average Train Loss:   0.8063 Train Accuracy: global   0.00% local  74.67% ...  426 forgettables out of  600 (71.00%) ... total data used  600\n",
      "2021-09-23 01:06:55,219 - INFO -   |--    Epoch Losses ( 7): [1.3941, 0.7834, 0.7634, 0.716, 0.681, 0.6814, 0.6246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:06:57,137 - INFO -   |-- [Party 90] Training for forgettable counting in the next round\n",
      "2021-09-23 01:06:59,053 - INFO -   |-- [Party 91] Training for forgettable counting in the next round\n",
      "2021-09-23 01:07:00,639 - INFO -     |---- Number of Forgettables: 4421 (73.68%)\n",
      "2021-09-23 01:07:00,642 - INFO -     |---- Test Accuracy: 11.66%\n",
      "2021-09-23 01:07:00,643 - INFO -     |---- Test Loss: 2.6040\n",
      "2021-09-23 01:07:00,644 - INFO -     |---- Elapsed time: 0:01:53.235548\n",
      "2021-09-23 01:07:00,645 - INFO - \n",
      "Test Acc: Highest 15.4600% (1 round) | Avg 12.4300% (1 round) | Curr 11.6600%\n",
      "2021-09-23 01:07:00,646 - INFO -  | Global Training Round : 4 / 1000 |\n",
      "2021-09-23 01:07:00,647 - INFO -  | Current Participants : [14, 41, 46, 58, 59, 61, 61, 79, 90, 91] |\n",
      "2021-09-23 01:07:00,648 - INFO -  |    Next Participants : [2, 6, 20, 38, 50, 50, 54, 61, 63, 72] |\n",
      "2021-09-23 01:07:02,310 - INFO -   |-- [Party  2] Training for forgettable counting in the next round\n",
      "2021-09-23 01:07:04,151 - INFO -   |-- [Party  6] Training for forgettable counting in the next round\n",
      "2021-09-23 01:07:06,115 - INFO -   |-- [Party 14] Average Train Loss:   0.7569 Train Accuracy: global   0.50% local  79.83% ...  485 forgettables out of  600 (80.83%) ... total data used  600\n",
      "2021-09-23 01:07:06,117 - INFO -   |--    Epoch Losses ( 8): [1.9457, 0.7373, 0.6392, 0.6209, 0.5657, 0.524, 0.5267, 0.4957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:07:07,949 - INFO -   |-- [Party 20] Training for forgettable counting in the next round\n",
      "2021-09-23 01:07:09,803 - INFO -   |-- [Party 38] Training for forgettable counting in the next round\n",
      "2021-09-23 01:07:10,702 - INFO -   |-- [Party 41] Average Train Loss:   1.8329 Train Accuracy: global  24.33% local  38.50% ...  220 forgettables out of  600 (36.67%) ... total data used  600\n",
      "2021-09-23 01:07:10,704 - INFO -   |--    Epoch Losses ( 3): [2.2623, 1.7284, 1.5079]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:07:12,554 - INFO -   |-- [Party 46] Average Train Loss:   0.5707 Train Accuracy: global   1.83% local  86.00% ...  501 forgettables out of  600 (83.50%) ... total data used  600\n",
      "2021-09-23 01:07:12,555 - INFO -   |--    Epoch Losses ( 8): [0.956, 0.5999, 0.5353, 0.5183, 0.4932, 0.5058, 0.4783, 0.4785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 01:07:14,695 - INFO -   |-- [Party 50] Training for forgettable counting in the next round\n",
      "2021-09-23 01:07:16,820 - INFO -   |-- [Party 54] Training for forgettable counting in the next round\n",
      "2021-09-23 01:07:18,731 - INFO -   |-- [Party 58] Average Train Loss:   1.0063 Train Accuracy: global   1.00% local  78.67% ...  450 forgettables out of  600 (75.00%) ... total data used  600\n",
      "2021-09-23 01:07:18,733 - INFO -   |--    Epoch Losses ( 7): [1.559, 1.0859, 1.0071, 0.9273, 0.8583, 0.8098, 0.7966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "600\n",
      "0\n",
      "600\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-642b18d6a9bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             logger.info('  |-- [Party {:>2}] Average Train Loss: {:>8.4f} Train Accuracy: global {:>6.2f}% local {:>6.2f}% ... {:>4} forgettables out of {:>4} ({:>5.2f}%) ... total data used {:>4}'.format(\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mglobal_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlocal_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforget_cnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mforget_cnt\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             ))\n\u001b[1;32m     67\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  |--    Epoch Losses ({:>2}): {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdyn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatfloats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "rounds = 1000\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "wdecay = 0.0001\n",
    "\n",
    "logger.info(f'\\nAlgorithm: FedAvg Dynamic Epoch\\nClients: {num_clients}\\nDataset: {dataset}\\nModel: MLP | Rounds: {rounds} | Epochs: {epochs} | LR: {lr}\\n')\n",
    "\n",
    "train_accs, train_losses, test_accs, test_losses = [], [], [], []\n",
    "match_history, round_forget_history = {}, []\n",
    "# forget_cnt_per_client = [0] * len(indices.keys())\n",
    "\n",
    "st = time.time()\n",
    "curr_participants = np.random.choice(num_clients, 10)\n",
    "\n",
    "for r in range(rounds):\n",
    "    \n",
    "    forget_cnt_per_client = [0] * 10\n",
    "    \n",
    "    round_forget, round_samples = 0, 0\n",
    "    forget_history, forgettables = {}, {}\n",
    "    local_weights, local_losses = [], []\n",
    "    logger.info(f' | Global Training Round : {r + 1} / {rounds} |')\n",
    "    logger.info(f' | Current Participants : {sorted(curr_participants.tolist())} |')\n",
    "\n",
    "    next_participants = np.random.choice(num_clients, 10)\n",
    "    logger.info(f' |    Next Participants : {sorted(next_participants.tolist())} |')\n",
    "    \n",
    "    tmp_cnt = 0\n",
    "    for i, k in enumerate(indices.keys()):\n",
    "        if int(k) in curr_participants:\n",
    "            match_history, forgettables, global_acc = infer_train(fed_model, inferloaders[k], device, match_history, flag=True)\n",
    "            \n",
    "            forget_cnt = len(forgettables)\n",
    "            round_forget += forget_cnt\n",
    "            forget_cnt_per_client[tmp_cnt] = forget_cnt\n",
    "            tmp_cnt += 1\n",
    "            round_samples += len(indices[k]['index'])\n",
    "\n",
    "            fed_model.train()\n",
    "\n",
    "            sampler_idx = indices[k]['index'].copy()\n",
    "            sampler = SubsetRandomSampler(sampler_idx)\n",
    "\n",
    "            trainloader = DataLoader(custom_dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
    "\n",
    "            local_model = copy.deepcopy(fed_model)\n",
    "            if r < 1:\n",
    "                dyn_epochs = epochs\n",
    "            elif forget_cnt > 0:\n",
    "                dyn_epochs = int(epochs * forget_cnt // len(sampler_idx))\n",
    "            else:\n",
    "                dyn_epochs = 1\n",
    "            w, ls = train(local_model, trainloader, dyn_epochs, lr, wdecay, criterion, device)\n",
    "\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            train_losses.append(ls)\n",
    "\n",
    "            match_history, _, local_acc = infer_train(local_model, inferloaders[k], device, match_history, flag=False)\n",
    "            train_accs.append(local_acc)\n",
    "            \n",
    "            logger.info('  |-- [Party {:>2}] Average Train Loss: {:>8.4f} Train Accuracy: global {:>6.2f}% local {:>6.2f}% ... {:>4} forgettables out of {:>4} ({:>5.2f}%) ... total data used {:>4}'.format(\n",
    "                k, sum(ls) / len(ls), 100 * global_acc, 100 * local_acc, forget_cnt, len(indices[k]['index']), 100 * forget_cnt / len(indices[k]['index']), len(sampler_idx)\n",
    "            ))\n",
    "            logger.info('  |--    Epoch Losses ({:>2}): {}'.format(dyn_epochs, formatfloats(ls)))\n",
    "            \n",
    "        elif int(k) in next_participants:\n",
    "            match_history, forgettables, global_acc = infer_train(fed_model, inferloaders[k], device, match_history, flag=True)\n",
    "            \n",
    "            fed_model.train()\n",
    "\n",
    "            sampler_idx = indices[k]['index'].copy()\n",
    "            sampler = SubsetRandomSampler(sampler_idx)\n",
    "\n",
    "            trainloader = DataLoader(custom_dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
    "\n",
    "            local_model = copy.deepcopy(fed_model)\n",
    "            if r < 1:\n",
    "                dyn_epochs = epochs\n",
    "            elif forget_cnt > 0:\n",
    "                dyn_epochs = epochs * forget_cnt // len(indices[k]['index'])\n",
    "            else:\n",
    "                dyn_epochs = 1\n",
    "            _, _ = train(local_model, trainloader, dyn_epochs, lr, wdecay, criterion, device)\n",
    "            \n",
    "            match_history, _, local_acc = infer_train(local_model, inferloaders[k], device, match_history, flag=False)\n",
    "\n",
    "            logger.info('  |-- [Party {:>2}] Training for forgettable counting in the next round'.format(k))\n",
    "        \n",
    "#     standardized_weights = weighting(forget_cnt_per_client, standardize)\n",
    "    fed_weights = update_avg(local_weights)\n",
    "    fed_model.load_state_dict(fed_weights)\n",
    "#     train_acc, _ = inference(fed_model, fed_trainloader, criterion, device)\n",
    "    \n",
    "    curr_participants = next_participants.copy()\n",
    "    \n",
    "    if (r + 1) % 100 == 0:\n",
    "        torch.save(fed_model.state_dict(), os.path.join(path_models, filename + f'_round{r+1}.pth'))\n",
    "    \n",
    "    test_acc, test_ls = inference(fed_model, testloader, criterion, device)\n",
    "    test_accs.append(test_acc)\n",
    "    test_losses.append(test_ls)\n",
    "    round_forget_history.append(round_forget)\n",
    "    logger.info('    |---- Number of Forgettables: {} ({:.2f}%)'.format(round_forget, 100 * round_forget / round_samples))\n",
    "#     logger.info('    |---- Train Accuracy: {:>.2f}%'.format(100 * train_acc))\n",
    "    logger.info('    |---- Test Accuracy: {:>.2f}%'.format(100 * test_acc))\n",
    "    logger.info('    |---- Test Loss: {:.4f}'.format(test_ls))\n",
    "    logger.info('    |---- Elapsed time: {}'.format(timedelta(seconds=time.time()-st)))\n",
    "    logger.info(f'\\nTest Acc: Highest {np.max(test_accs) * 100:.4f}% ({np.argmax(test_accs)+1} round) | Avg {np.mean(test_accs) * 100:.4f}% ({np.argmax(test_accs > np.mean(test_accs))+1} round) | Curr {test_accs[-1] * 100:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654dbc03-4dd8-4c75-932e-f9f842c9ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = np.asarray(train_losses)\n",
    "train_accs = np.asarray(train_accs)\n",
    "\n",
    "with open(os.path.join(path_results, f'{filename}_tr_ls.npy'), 'wb') as f:\n",
    "    np.save(f, train_losses)\n",
    "with open(os.path.join(path_results, f'{filename}_tr_acc.npy'), 'wb') as f:\n",
    "    np.save(f, train_accs)\n",
    "with open(os.path.join(path_results, f'{filename}_te_ls.npy'), 'wb') as f:\n",
    "    np.save(f, test_losses)\n",
    "with open(os.path.join(path_results, f'{filename}_te_acc.npy'), 'wb') as f:\n",
    "    np.save(f, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e644234-224e-4353-888c-01affc417a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(50, 30))\n",
    "axs = axs.ravel()\n",
    "\n",
    "axs[0].plot(test_accs, c='orange')\n",
    "axs[0].set_title('Test Accuracies')\n",
    "axs[0].set_xlabel('Rounds')\n",
    "axs[0].set_ylabel('Test Accuracy')\n",
    "axs[1].plot(test_losses, c='blue')\n",
    "axs[1].set_title('Test Losses')\n",
    "axs[1].set_xlabel('Rounds')\n",
    "axs[1].set_ylabel('Test Loss')\n",
    "axs[2].plot(train_accs, c='red')\n",
    "axs[2].set_title('Train Average Accuracies by Epochs')\n",
    "axs[2].set_xlabel('Epochs')\n",
    "axs[2].set_ylabel('Train Average Accuracy')\n",
    "axs[3].plot(train_losses.mean(axis=1), c='turquoise')\n",
    "axs[3].set_title('Train Average Losses by Epochs')\n",
    "axs[3].set_xlabel('Epochs')\n",
    "axs[3].set_ylabel('Train Average Loss')\n",
    "axs[4].plot(np.mean(train_accs.reshape(-1, 10), axis=1), c='green')\n",
    "axs[4].set_title('Train Average Accuracies by Rounds')\n",
    "axs[4].set_xlabel('Rounds')\n",
    "axs[4].set_ylabel('Train Average Accuracy')\n",
    "axs[5].plot(train_losses.mean(axis=1).reshape(-1, 10).mean(axis=1), c='lightpink')\n",
    "axs[5].set_title('Train Average Losses by Rounds')\n",
    "axs[5].set_xlabel('Rounds')\n",
    "axs[5].set_ylabel('Train Average Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9a5cc-41d2-41f1-a4cd-114d63d3b2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
